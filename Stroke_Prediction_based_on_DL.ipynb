{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Stroke Prediction based on DL.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0g2RQRSltgja"
      },
      "source": [
        "\n",
        "###**Case Study:** Stroke Prediction\n",
        "\n",
        "**Objective:** The goal of this challenge is to apply the deep learning  model to predict if a person will have a stroke or not.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SSCD88NxJjFo"
      },
      "source": [
        "**Dataset Explanation:** We will be using the stroke dataset. Its features are:\n",
        "\n",
        "\n",
        "* **id:** unique identifier\n",
        "* **gender:** \"Male\", \"Female\" or \"Other\"\n",
        "* **age:** age of the patient\n",
        "* **hypertension:** 0 if the patient doesn't have hypertension, 1 if the patient has hypertension\n",
        "* **heart_disease:** 0 if the patient doesn't have any heart diseases, 1 if the patient has a heart disease\n",
        "* **ever_married:** \"No\" or \"Yes\"\n",
        "* **work_type:** \"children\", \"Govt_jov\", \"Never_worked\", \"Private\" or \"Self-employed\"\n",
        "* **Residence_type:** \"Rural\" or \"Urban\"\n",
        "* **avg_glucose_level:** average glucose level in blood\n",
        "* **bmi:** body mass index\n",
        "* **smoking_status:** \"formerly smoked\", \"never smoked\", \"smokes\" or \"Unknown\"*\n",
        "* **stroke:** 1 if the patient had a stroke or 0 if not"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gBvX5nCYt8cT"
      },
      "source": [
        "#Importing Libraries"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JaPfsfvXK2_0"
      },
      "source": [
        "We start by importing the libraries: numpy and pandas"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mMQuUG7OtfrG"
      },
      "source": [
        "#Test Your Zaka\n",
        "import numpy as np\n",
        "import pandas as pd"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0-RxAH5auFFy"
      },
      "source": [
        "#Loading the Dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z_jj2t6zK6zy"
      },
      "source": [
        "We load the dataset from a csv file, and see its first rows"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3OQQmRYS7Yni",
        "outputId": "43e5ea60-5d74-4763-e8de-565ff7676376"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# dataset link: https://drive.google.com/file/d/1Wpm7jOL2nLlf18MWaZLUN4is5cKgVKyL/view?usp=sharing\n",
        "data=pd.read_csv('/content/drive/MyDrive/ZAka/Week 6/healthcare-dataset-stroke-data.csv') # change to your saved dataset directory\n",
        "data.head()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "id": "r2CNDIv87j4Y",
        "outputId": "0d279d93-37f0-49ed-a24b-fddd87737936"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "      id  gender   age  hypertension  heart_disease ever_married  \\\n",
              "0   9046    Male  67.0             0              1          Yes   \n",
              "1  51676  Female  61.0             0              0          Yes   \n",
              "2  31112    Male  80.0             0              1          Yes   \n",
              "3  60182  Female  49.0             0              0          Yes   \n",
              "4   1665  Female  79.0             1              0          Yes   \n",
              "\n",
              "       work_type Residence_type  avg_glucose_level   bmi   smoking_status  \\\n",
              "0        Private          Urban             228.69  36.6  formerly smoked   \n",
              "1  Self-employed          Rural             202.21   NaN     never smoked   \n",
              "2        Private          Rural             105.92  32.5     never smoked   \n",
              "3        Private          Urban             171.23  34.4           smokes   \n",
              "4  Self-employed          Rural             174.12  24.0     never smoked   \n",
              "\n",
              "   stroke  \n",
              "0       1  \n",
              "1       1  \n",
              "2       1  \n",
              "3       1  \n",
              "4       1  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-5ce7810f-c893-43fb-845f-5642b1fde4bc\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>id</th>\n",
              "      <th>gender</th>\n",
              "      <th>age</th>\n",
              "      <th>hypertension</th>\n",
              "      <th>heart_disease</th>\n",
              "      <th>ever_married</th>\n",
              "      <th>work_type</th>\n",
              "      <th>Residence_type</th>\n",
              "      <th>avg_glucose_level</th>\n",
              "      <th>bmi</th>\n",
              "      <th>smoking_status</th>\n",
              "      <th>stroke</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>9046</td>\n",
              "      <td>Male</td>\n",
              "      <td>67.0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>Yes</td>\n",
              "      <td>Private</td>\n",
              "      <td>Urban</td>\n",
              "      <td>228.69</td>\n",
              "      <td>36.6</td>\n",
              "      <td>formerly smoked</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>51676</td>\n",
              "      <td>Female</td>\n",
              "      <td>61.0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>Yes</td>\n",
              "      <td>Self-employed</td>\n",
              "      <td>Rural</td>\n",
              "      <td>202.21</td>\n",
              "      <td>NaN</td>\n",
              "      <td>never smoked</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>31112</td>\n",
              "      <td>Male</td>\n",
              "      <td>80.0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>Yes</td>\n",
              "      <td>Private</td>\n",
              "      <td>Rural</td>\n",
              "      <td>105.92</td>\n",
              "      <td>32.5</td>\n",
              "      <td>never smoked</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>60182</td>\n",
              "      <td>Female</td>\n",
              "      <td>49.0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>Yes</td>\n",
              "      <td>Private</td>\n",
              "      <td>Urban</td>\n",
              "      <td>171.23</td>\n",
              "      <td>34.4</td>\n",
              "      <td>smokes</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>1665</td>\n",
              "      <td>Female</td>\n",
              "      <td>79.0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>Yes</td>\n",
              "      <td>Self-employed</td>\n",
              "      <td>Rural</td>\n",
              "      <td>174.12</td>\n",
              "      <td>24.0</td>\n",
              "      <td>never smoked</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-5ce7810f-c893-43fb-845f-5642b1fde4bc')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-5ce7810f-c893-43fb-845f-5642b1fde4bc button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-5ce7810f-c893-43fb-845f-5642b1fde4bc');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r6gAyBGtubI7"
      },
      "source": [
        "#Exploratory Data Analysis"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XZ_93CvuLF3j"
      },
      "source": [
        "Now we start the exploratory data analysis."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2925yVCdud0a"
      },
      "source": [
        "###Shape of the data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yZ0hWvVALJy4"
      },
      "source": [
        "First, you need to know the shape of our data (How many examples and features do we have)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8pvWR3PKuQEy",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4521a443-884c-45b6-b30a-b5e077027109"
      },
      "source": [
        "#Test Your Zaka\n",
        "print(f\"We have {data.shape[0]} examples, {(data.shape[1])-1} features and 1 output\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "We have 5110 examples, 11 features and 1 output\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yUy4oI5xukRr"
      },
      "source": [
        "###Types of different Columns"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G1q10ievLTTs"
      },
      "source": [
        "See the type of each of your features and see if you have any nulls"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_8snoohouhUP",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d571e1f6-57b2-4e4c-b878-d16a682717df"
      },
      "source": [
        "#Test Your Zaka\n",
        "data.info()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 5110 entries, 0 to 5109\n",
            "Data columns (total 12 columns):\n",
            " #   Column             Non-Null Count  Dtype  \n",
            "---  ------             --------------  -----  \n",
            " 0   id                 5110 non-null   int64  \n",
            " 1   gender             5110 non-null   object \n",
            " 2   age                5110 non-null   float64\n",
            " 3   hypertension       5110 non-null   int64  \n",
            " 4   heart_disease      5110 non-null   int64  \n",
            " 5   ever_married       5110 non-null   object \n",
            " 6   work_type          5110 non-null   object \n",
            " 7   Residence_type     5110 non-null   object \n",
            " 8   avg_glucose_level  5110 non-null   float64\n",
            " 9   bmi                4909 non-null   float64\n",
            " 10  smoking_status     5110 non-null   object \n",
            " 11  stroke             5110 non-null   int64  \n",
            "dtypes: float64(3), int64(4), object(5)\n",
            "memory usage: 479.2+ KB\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nkrEh6RYygms"
      },
      "source": [
        "###Dealing with categorical variables"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z7rtFrHVLg5U"
      },
      "source": [
        "Now we will walk through the categorical variables that we have to see the categories and the counts of each of them."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c5uIMEU-gYEJ"
      },
      "source": [
        "'smoking_status'"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DlSfSQ35ykgc",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 385
        },
        "outputId": "152de6dd-4f42-4151-d76e-b6d6455c56f5"
      },
      "source": [
        "#Test Your Zaka\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "print(data.smoking_status.value_counts())\n",
        "data.smoking_status.value_counts().plot.bar(rot=0)\n",
        "plt.title(\"Distribution of smoking status\")\n",
        "plt.legend()\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "never smoked       1892\n",
            "Unknown            1544\n",
            "formerly smoked     885\n",
            "smokes              789\n",
            "Name: smoking_status, dtype: int64\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.legend.Legend at 0x7f305a4661d0>"
            ]
          },
          "metadata": {},
          "execution_count": 6
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z1yf6OU5geTr"
      },
      "source": [
        "'Residence_type'"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Yg0xlLNUy7AF",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 350
        },
        "outputId": "4fcd998d-7450-4d39-c347-bd55ad85ab48"
      },
      "source": [
        "#Test Your Zaka\n",
        "\n",
        "print(data.Residence_type.value_counts())\n",
        "data.Residence_type.value_counts().plot.bar(rot=0)\n",
        "plt.title(\"Distribution of Residence_type\")\n",
        "plt.legend()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Urban    2596\n",
            "Rural    2514\n",
            "Name: Residence_type, dtype: int64\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.legend.Legend at 0x7f305a3a12d0>"
            ]
          },
          "metadata": {},
          "execution_count": 7
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "82ovosaVgjC8"
      },
      "source": [
        "'work_type'"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T72r_mkrzF9V",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 404
        },
        "outputId": "212f6de7-823b-4b41-ca0e-be30d9848617"
      },
      "source": [
        "#Test Your Zaka\n",
        "print(data.work_type.value_counts())\n",
        "data.work_type.value_counts().plot.bar(rot=0)\n",
        "plt.title(\"Distribution of work_type\")\n",
        "plt.legend()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Private          2925\n",
            "Self-employed     819\n",
            "children          687\n",
            "Govt_job          657\n",
            "Never_worked       22\n",
            "Name: work_type, dtype: int64\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.legend.Legend at 0x7f3059e911d0>"
            ]
          },
          "metadata": {},
          "execution_count": 8
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0oa7__UPgnu8"
      },
      "source": [
        "'ever_married'"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z0kpR57LzQE9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 350
        },
        "outputId": "9ba2e927-097a-430c-d27f-e2d2a6d43387"
      },
      "source": [
        "#Test Your Zaka\n",
        "print(data.ever_married.value_counts())\n",
        "data.ever_married.value_counts().plot.bar(rot=0)\n",
        "plt.title(\"Distribution of ever_married\")\n",
        "plt.legend()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Yes    3353\n",
            "No     1757\n",
            "Name: ever_married, dtype: int64\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.legend.Legend at 0x7f3059e8be10>"
            ]
          },
          "metadata": {},
          "execution_count": 9
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LqUD-N4ygrxB"
      },
      "source": [
        "'hypertension'"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fQIoAs8y3igG",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 350
        },
        "outputId": "2b97a294-9fdb-433a-8dd5-b981f58175b9"
      },
      "source": [
        "#Test Your Zaka\n",
        "print(data.hypertension.value_counts())\n",
        "data.hypertension.value_counts().plot.bar(rot=0)\n",
        "plt.title(\"Distribution of hypertension\")\n",
        "plt.legend()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0    4612\n",
            "1     498\n",
            "Name: hypertension, dtype: int64\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.legend.Legend at 0x7f3059de1d50>"
            ]
          },
          "metadata": {},
          "execution_count": 10
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NmyP9Nfsg29X"
      },
      "source": [
        "'heart_disease'"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xEhNH46j3p3Y",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 350
        },
        "outputId": "c2a1cbea-d84c-454b-c103-6de36ca34bce"
      },
      "source": [
        "#Test Your Zaka\n",
        "print(data.heart_disease.value_counts())\n",
        "data.heart_disease.value_counts().plot.bar(rot=0)\n",
        "plt.title(\"Distribution of heart_disease\")\n",
        "plt.legend()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0    4834\n",
            "1     276\n",
            "Name: heart_disease, dtype: int64\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.legend.Legend at 0x7f3059d69410>"
            ]
          },
          "metadata": {},
          "execution_count": 11
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x8Oql1Xag94S"
      },
      "source": [
        "'stroke'"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8cI7uJvA3Njv",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 350
        },
        "outputId": "f49ab5a3-af8a-47bd-f573-6c9de53f9019"
      },
      "source": [
        "#Test Your Zaka\n",
        "print(data.stroke.value_counts())\n",
        "data.stroke.value_counts().plot.bar(rot=0 )\n",
        "plt.title(\"Distribution of stroke\")\n",
        "plt.legend()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0    4861\n",
            "1     249\n",
            "Name: stroke, dtype: int64\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.legend.Legend at 0x7f3059de9890>"
            ]
          },
          "metadata": {},
          "execution_count": 12
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAX0AAAEICAYAAACzliQjAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjIsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+WH4yJAAAVEklEQVR4nO3dcbSV1Xnn8e8jYO4oKEaoIyBe2pCOlCyiYcBZLcsYEsXIiIkxpasmaMliJbGddspYtbFjTHTGzKwYmplE61QUTYwhmRitcUypkhVdMSgajRriiATjhUSvoFQxOKLP/HH2tUe813vu5XAvuL+fte4677vf/e53v2fB77xnn/fsE5mJJKkO+w13ByRJQ8fQl6SKGPqSVBFDX5IqYuhLUkUMfUmqiKGvPS4iroiIv2lTW5Mj4oWIGFHWfxARn2hH26W9/xMRi9rV3gCOe3FEPBMRvx7i43ZGREbEyKE8roaPoa/dEhEbI+I3EfF8RDwXET+KiE9GxGv/tjLzk5n5+Rbbev+b1cnMX2bm6Mx8pQ19/2xEfG2X9k/KzBW72/YA+zEZWApMy8x/PcB9+33OpGaGvtrh32fmGOBI4FLgXOCqdh/kLXw1OhnYkplPt7vht/BzpkEy9NU2mbktM28G/hBYFBHTASLimoi4uCyPi4hbyruCrRFxZ0TsFxHX0Qi/fyjDN3/VNPSwOCJ+CdzRx3DE70TEPRHxzxFxU0S8vRzrvRHR1dzHnivjiJgH/DXwh+V4D5btrw0XlX5dEBFPRMTTEXFtRBxctvX0Y1FE/LIMzXymr+cmIg4u+3eX9i4o7b8fWAVMKP24ppd9d/c56/M8ejnWaeU5ml72Oy8iHo+ILRGxsue51b7L0FfbZeY9QBcwp5fNS8u28cBhNII3M/NjwC9pvGsYnZn/rWmf44CjgBP7OOTHgT8BDgd2Al9uoY+3Af8F+GY53oxeqp1Z/o4HfhsYDfzPXer8AfC7wFzgP0fEUX0c8n8AB5d2jit9Pisz/wk4Cdhc+nFmL/vu7nPWynkQEWcBXwDen5kPA38GnFramgA8C3ylj/PTPsLQ156yGejtqvBlGuF8ZGa+nJl3Zv8TQH02M7dn5m/62H5dZj6cmduBvwE+2vNB7276Y+CyzNyQmS8A5wMLd3mXcVFm/iYzHwQeBN7w4lH6shA4PzOfz8yNwBeBj7XYj919zlo5j78AzgHem5nrS9kngc9kZldmvgR8FviIQ0b7NkNfe8pEYGsv5f8dWA/8Y0RsiIjzWmjryQFsfwIYBYxrqZdvbkJpr7ntkTSutns0323zIo2r6F2NK33ata2JLfZjd5+zVs7jHOArmdk8HHYkcGMZVnoOWAe8sst+2scY+mq7iPi3NALtrl23lSvdpZn528ApwF9GxNyezX002d9V7RFNy5NpXBk/A2wHDmjq1wgaQySttruZRvA1t70TeKqf/Xb1TOnTrm1tamXnNjxnrZzHCcAFEXFaU9mTwEmZObbpryMzW+q39k6GvtomIg6KiPnADcDXMvOhXurMj4h3REQA22hcOb5aNj9FY8x5oM6IiGkRcQDwOeDb5ZbO/wt0RMTJETEKuAB4W9N+TwGdzbeX7uIbwH+MiCkRMZp/+Qxg50A6V/qyErgkIsZExJHAXwJfe/M9G9rwnLVyHo8A84CvRMQppeyK0ucjSz/GR8SCVvqsvZehr3b4h4h4nsaV4WeAy4Cz+qg7Ffgn4AXgbuCrmbm6bPuvNK42n4uI/zSA418HXENjqKUD+A/QuJsI+DTw9zSuqrfT+EC0x7fK45aIuL+XdpeXtn8I/ALYQePDzcH4s3L8DTTeAV1f2m/F7j5nLZ1H+VxiPvC/IuIk4G+Bm2kMKz0P/BiY3WKftZcKf0RFkurhlb4kVcTQl6SKGPqSVBFDX5Iqsld/s27cuHHZ2dk53N2QpH3Kfffd90xmju9t214d+p2dnaxdu3a4uyFJ+5SIeKKvbS0N75RZ9x6KiAciYm0pe3tErIqIx8rjIaU8IuLLEbE+In4aEcc0tbOo1H8shuGHKiSpdgMZ0z8+M9+dmTPL+nnA7Zk5Fbi9rENjxsCp5W8JcDk0XiSAC2l8uWMWcGHPC4UkaWjszge5C4CeXxhaQWMK1p7ya7Phx8DYiDicxhSvqzJza2Y+S2MO8Xm7cXxJ0gC1OqafNL6KncDfZeaVwGGZ+auy/df8y8x7E3n9DH9dpayv8teJiCU03iEwefLkFrsnqTYvv/wyXV1d7NixY7i7Mmw6OjqYNGkSo0aNanmfVkP/DzJzU0T8FrAqIn7evDEzs7wg7LbygnIlwMyZM50jQlKvurq6GDNmDJ2dnTTmoqtLZrJlyxa6urqYMmVKy/u1NLzTM5Vq+Q3PG2mMyT9Vhm0ojz2/77mJ1091O6mU9VUuSQO2Y8cODj300CoDHyAiOPTQQwf8Tqff0I+IAyNiTM8yjXm3H6Yx+17PHTiLgJvK8s3Ax8tdPMcC28ow0PeBEyLikPIB7gmlTJIGpdbA7zGY829leOcwGr+e01P/+sy8LSLuBVZGxGIav8Tz0VL/VuCDNH7p50XKFLuZuTUiPg/cW+p9LjN7+2UlSdIe0m/oZ+YGevndz8zcQuPHoHctT+DsPtpaTutziEtSyzrP+15b29t46ckD3mfZsmUsWbKEAw44oP/KPcfZuJH58+fz8MMPD/h4g7FXfyN3X9Huf2y1G8x/NmlvsGzZMs4444xeQ/+VV15hxIgRw9Cr13PCNUkahO3bt3PyySczY8YMpk+fzkUXXcTmzZs5/vjjOf744wEYPXo0S5cuZcaMGdx9991cdtllTJ8+nenTp7Ns2bI3tLlhwwaOPvpo7r33Xh5//HHmzZvHe97zHubMmcPPf/7zN9QfDK/0JWkQbrvtNiZMmMD3vtd4p79t2zauvvpqVq9ezbhx44DGC8Ps2bP54he/yH333cfVV1/NmjVryExmz57NcccdxyGHNCYmePTRR1m4cCHXXHMNM2bMYO7cuVxxxRVMnTqVNWvW8OlPf5o77rhjt/tt6EvSILzrXe9i6dKlnHvuucyfP585c+a8oc6IESM47bTTALjrrrv40Ic+xIEHHgjAhz/8Ye68805OOeUUuru7WbBgAd/5zneYNm0aL7zwAj/60Y84/fTTX2vrpZdeaku/DX1JGoR3vvOd3H///dx6661ccMEFzJ37hvta6OjoaGkc/+CDD2by5MncddddTJs2jVdffZWxY8fywAMPtL3fjulL0iBs3ryZAw44gDPOOINzzjmH+++/nzFjxvD888/3Wn/OnDl897vf5cUXX2T79u3ceOONr7072H///bnxxhu59tpruf766znooIOYMmUK3/rWt4DGt28ffPDBtvTbK31JbwlDfdfXQw89xDnnnMN+++3HqFGjuPzyy7n77ruZN28eEyZMYPXq1a+rf8wxx3DmmWcya9YsAD7xiU9w9NFHs3HjRgAOPPBAbrnlFj7wgQ8wevRovv71r/OpT32Kiy++mJdffpmFCxcyY8Yb7p4fsGjcVr93mjlzZu4LP6LiLZvt5S2basW6des46qijhrsbw6635yEi7muaBv91HN6RpIoY+pJUEUNf0j5rbx6eHgqDOX9DX9I+qaOjgy1btlQb/D3z6Xd0dAxoP+/ekbRPmjRpEl1dXXR3dw93V4ZNzy9nDYShL2mfNGrUqAH9YpQaHN6RpIoY+pJUEUNfkipi6EtSRQx9SaqIoS9JFTH0Jakihr4kVcTQl6SKGPqSVBFDX5IqYuhLUkUMfUmqiKEvSRUx9CWpIoa+JFXE0Jekihj6klQRQ1+SKtJy6EfEiIj4SUTcUtanRMSaiFgfEd+MiP1L+dvK+vqyvbOpjfNL+aMRcWK7T0aS9OYGcqX/58C6pvUvAF/KzHcAzwKLS/li4NlS/qVSj4iYBiwEfg+YB3w1IkbsXvclSQPRUuhHxCTgZODvy3oA7wO+XaqsAE4tywvKOmX73FJ/AXBDZr6Umb8A1gOz2nESkqTWtHqlvwz4K+DVsn4o8Fxm7izrXcDEsjwReBKgbN9W6r9W3ss+r4mIJRGxNiLWdnd3D+BUJEn96Tf0I2I+8HRm3jcE/SEzr8zMmZk5c/z48UNxSEmqxsgW6vw+cEpEfBDoAA4C/hYYGxEjy9X8JGBTqb8JOALoioiRwMHAlqbyHs37SJKGQL9X+pl5fmZOysxOGh/E3pGZfwysBj5Sqi0CbirLN5d1yvY7MjNL+cJyd88UYCpwT9vORJLUr1au9PtyLnBDRFwM/AS4qpRfBVwXEeuBrTReKMjMRyJiJfAzYCdwdma+shvHlyQN0IBCPzN/APygLG+gl7tvMnMHcHof+18CXDLQTkqS2sNv5EpSRQx9SaqIoS9JFTH0Jakihr4kVcTQl6SKGPqSVBFDX5IqYuhLUkUMfUmqiKEvSRUx9CWpIoa+JFXE0Jekihj6klQRQ1+SKmLoS1JFDH1JqoihL0kVMfQlqSKGviRVxNCXpIoY+pJUEUNfkipi6EtSRQx9SaqIoS9JFTH0Jakihr4kVcTQl6SKGPqSVBFDX5IqYuhLUkUMfUmqSL+hHxEdEXFPRDwYEY9ExEWlfEpErImI9RHxzYjYv5S/rayvL9s7m9o6v5Q/GhEn7qmTkiT1rpUr/ZeA92XmDODdwLyIOBb4AvClzHwH8CywuNRfDDxbyr9U6hER04CFwO8B84CvRsSIdp6MJOnN9Rv62fBCWR1V/hJ4H/DtUr4COLUsLyjrlO1zIyJK+Q2Z+VJm/gJYD8xqy1lIklrS0ph+RIyIiAeAp4FVwOPAc5m5s1TpAiaW5YnAkwBl+zbg0ObyXvZpPtaSiFgbEWu7u7sHfkaSpD61FPqZ+UpmvhuYROPq/N/sqQ5l5pWZOTMzZ44fP35PHUaSqjSgu3cy8zlgNfDvgLERMbJsmgRsKsubgCMAyvaDgS3N5b3sI0kaAq3cvTM+IsaW5X8FfABYRyP8P1KqLQJuKss3l3XK9jsyM0v5wnJ3zxRgKnBPu05EktS/kf1X4XBgRbnTZj9gZWbeEhE/A26IiIuBnwBXlfpXAddFxHpgK407dsjMRyJiJfAzYCdwdma+0t7TkSS9mX5DPzN/ChzdS/kGern7JjN3AKf30dYlwCUD76YkqR38Rq4kVcTQl6SKGPqSVBFDX5IqYuhLUkUMfUmqiKEvSRUx9CWpIoa+JFXE0Jekihj6klQRQ1+SKmLoS1JFDH1JqoihL0kVMfQlqSKGviRVxNCXpIoY+pJUEUNfkipi6EtSRQx9SaqIoS9JFTH0Jakihr4kVcTQl6SKGPqSVBFDX5IqYuhLUkUMfUmqiKEvSRUx9CWpIoa+JFWk39CPiCMiYnVE/CwiHomIPy/lb4+IVRHxWHk8pJRHRHw5ItZHxE8j4pimthaV+o9FxKI9d1qSpN60cqW/E1iamdOAY4GzI2IacB5we2ZOBW4v6wAnAVPL3xLgcmi8SAAXArOBWcCFPS8UkqSh0W/oZ+avMvP+svw8sA6YCCwAVpRqK4BTy/IC4Nps+DEwNiIOB04EVmXm1sx8FlgFzGvr2UiS3tSAxvQjohM4GlgDHJaZvyqbfg0cVpYnAk827dZVyvoqlyQNkZZDPyJGA/8b+IvM/OfmbZmZQLajQxGxJCLWRsTa7u7udjQpSSpaCv2IGEUj8L+emd8pxU+VYRvK49OlfBNwRNPuk0pZX+Wvk5lXZubMzJw5fvz4gZyLJKkfrdy9E8BVwLrMvKxp081Azx04i4Cbmso/Xu7iORbYVoaBvg+cEBGHlA9wTyhlkqQhMrKFOr8PfAx4KCIeKGV/DVwKrIyIxcATwEfLtluBDwLrgReBswAyc2tEfB64t9T7XGZubctZSJJa0m/oZ+ZdQPSxeW4v9RM4u4+2lgPLB9JBSVL7+I1cSaqIoS9JFTH0Jakihr4kVcTQl6SKGPqSVBFDX5IqYuhLUkUMfUmqiKEvSRUx9CWpIoa+JFXE0Jekihj6klQRQ1+SKmLoS1JFDH1JqoihL0kVMfQlqSKGviRVxNCXpIoY+pJUEUNfkipi6EtSRQx9SaqIoS9JFTH0Jakihr4kVcTQl6SKGPqSVBFDX5IqYuhLUkUMfUmqiKEvSRXpN/QjYnlEPB0RDzeVvT0iVkXEY+XxkFIeEfHliFgfET+NiGOa9llU6j8WEYv2zOlIkt5MK1f61wDzdik7D7g9M6cCt5d1gJOAqeVvCXA5NF4kgAuB2cAs4MKeFwpJ0tDpN/Qz84fA1l2KFwAryvIK4NSm8muz4cfA2Ig4HDgRWJWZWzPzWWAVb3whkSTtYYMd0z8sM39Vln8NHFaWJwJPNtXrKmV9lb9BRCyJiLURsba7u3uQ3ZMk9Wa3P8jNzASyDX3pae/KzJyZmTPHjx/frmYlSQw+9J8qwzaUx6dL+SbgiKZ6k0pZX+WSpCE02NC/Gei5A2cRcFNT+cfLXTzHAtvKMND3gRMi4pDyAe4JpUySNIRG9lchIr4BvBcYFxFdNO7CuRRYGRGLgSeAj5bqtwIfBNYDLwJnAWTm1oj4PHBvqfe5zNz1w2FJ0h7Wb+hn5h/1sWluL3UTOLuPdpYDywfUO0lSW/mNXEmqiKEvSRUx9CWpIoa+JFXE0Jekihj6klQRQ1+SKmLoS1JFDH1JqoihL0kVMfQlqSKGviRVxNCXpIoY+pJUEUNfkipi6EtSRQx9SaqIoS9JFTH0Jakihr4kVcTQl6SKGPqSVBFDX5IqYuhLUkUMfUmqiKEvSRUZOdwdkLRndZ73veHuwlvGxktPHu4u7Dav9CWpIoa+JFXE0Jekihj6klQRQ1+SKmLoS1JFDH1JqsiQh35EzIuIRyNifUScN9THl6SaDWnoR8QI4CvAScA04I8iYtpQ9kGSajbUV/qzgPWZuSEz/x9wA7BgiPsgSdUa6mkYJgJPNq13AbObK0TEEmBJWX0hIh4dor7VYBzwzHB3oj/xheHugYaB/zbb68i+Nux1c+9k5pXAlcPdj7eiiFibmTOHux/Srvy3OXSGenhnE3BE0/qkUiZJGgJDHfr3AlMjYkpE7A8sBG4e4j5IUrWGdHgnM3dGxJ8C3wdGAMsz85Gh7EPlHDbT3sp/m0MkMnO4+yBJGiJ+I1eSKmLoS1JFDP1KOP2F9kYRsTwino6Ih4e7L7Uw9Cvg9Bfai10DzBvuTtTE0K+D019or5SZPwS2Dnc/amLo16G36S8mDlNfJA0jQ1+SKmLo18HpLyQBhn4tnP5CEmDoVyEzdwI901+sA1Y6/YX2BhHxDeBu4HcjoisiFg93n97qnIZBkirilb4kVcTQl6SKGPqSVBFDX5IqYuhLUkUMfUmqiKEvSRX5/w6WPFpiE3R0AAAAAElFTkSuQmCC\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D6jD-fR_k49a"
      },
      "source": [
        "#Preprocessing"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Qs4Tnvhbk-Ew"
      },
      "source": [
        "###Dealing with Nulls"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "21da4C_Bzd-u"
      },
      "source": [
        "####Encoding Categorical Features"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UaNk4gWCLqe4"
      },
      "source": [
        "Here you will encode those categorical variables to be able to use them to train our DL model."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6nJtOvxdzi_G",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "518ba541-a6e5-4224-864a-8e056aa5e855"
      },
      "source": [
        "#Test Your Zaka\n",
        "print(data.isnull().sum()) \n",
        "print(\"There is no null values for the categorical features\")\n",
        "print(\"Ready for encoding\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "id                     0\n",
            "gender                 0\n",
            "age                    0\n",
            "hypertension           0\n",
            "heart_disease          0\n",
            "ever_married           0\n",
            "work_type              0\n",
            "Residence_type         0\n",
            "avg_glucose_level      0\n",
            "bmi                  201\n",
            "smoking_status         0\n",
            "stroke                 0\n",
            "dtype: int64\n",
            "There is no null values for the categorical features\n",
            "Ready for encoding\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn import preprocessing\n",
        "\n",
        "le = preprocessing.LabelEncoder()\n",
        "cat_data=data[['gender', 'hypertension', 'heart_disease', 'ever_married', 'work_type','Residence_type', 'smoking_status']]\n",
        "for i in cat_data:\n",
        "    cat_data[i] = le.fit_transform(cat_data[i])\n",
        "\n",
        "cat_data=cat_data.astype('float')  \n",
        "\n",
        "print(\"Test to see if the features were correctly hot-encoded\")\n",
        "print(\"\\nThe value counts of the categories for the original work type features\")\n",
        "print(data.work_type.value_counts())\n",
        "\n",
        "print(\"\\nTransformed to\")\n",
        "print(cat_data.work_type.value_counts())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "L_zkqGkPOyS3",
        "outputId": "a4ce0b65-cb8d-450c-e60a-81643b3a1609"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test to see if the features were correctly hot-encoded\n",
            "\n",
            "The value counts of the categories for the original work type features\n",
            "Private          2925\n",
            "Self-employed     819\n",
            "children          687\n",
            "Govt_job          657\n",
            "Never_worked       22\n",
            "Name: work_type, dtype: int64\n",
            "\n",
            "Transformed to\n",
            "2.0    2925\n",
            "3.0     819\n",
            "4.0     687\n",
            "0.0     657\n",
            "1.0      22\n",
            "Name: work_type, dtype: int64\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:6: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RtJKDDePlHJl"
      },
      "source": [
        "Fill the nulls with the mean value, and make sure you have no nulls anymore"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uOUE6TcrlIRo",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "92a0c967-4f79-40cd-a094-867106c85768"
      },
      "source": [
        "#Test Your Zaka\n",
        "num_data=data[['id', 'age','avg_glucose_level','bmi']]\n",
        "\n",
        "num_data['bmi'].fillna(num_data['bmi'].mean(), inplace=True)\n",
        "print(num_data.info())"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 5110 entries, 0 to 5109\n",
            "Data columns (total 4 columns):\n",
            " #   Column             Non-Null Count  Dtype  \n",
            "---  ------             --------------  -----  \n",
            " 0   id                 5110 non-null   int64  \n",
            " 1   age                5110 non-null   float64\n",
            " 2   avg_glucose_level  5110 non-null   float64\n",
            " 3   bmi                5110 non-null   float64\n",
            "dtypes: float64(3), int64(1)\n",
            "memory usage: 159.8 KB\n",
            "None\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/pandas/core/generic.py:6392: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  return self._update_inplace(result)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d1GOfAgt4M-Q"
      },
      "source": [
        "###Normalizing Features"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EPpkMCXELwty"
      },
      "source": [
        "Now you normalize the input data by dividing with the max value of each column."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BtI_XA-m33Bx",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "outputId": "4d6341ac-b842-41a7-8bc7-023ec409be96"
      },
      "source": [
        "#Test Your Zaka\n",
        "norm_data = pd.concat([cat_data, num_data,data.iloc[:,-1]], axis=1)\n",
        "norm_data=norm_data.divide(norm_data.max())\n",
        "norm_data.head() # gender has 0,0.5,1 since there were 3 classes (male, female, other)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "   gender  hypertension  heart_disease  ever_married  work_type  \\\n",
              "0     0.5           0.0            1.0           1.0       0.50   \n",
              "1     0.0           0.0            0.0           1.0       0.75   \n",
              "2     0.5           0.0            1.0           1.0       0.50   \n",
              "3     0.0           0.0            0.0           1.0       0.50   \n",
              "4     0.0           1.0            0.0           1.0       0.75   \n",
              "\n",
              "   Residence_type  smoking_status        id       age  avg_glucose_level  \\\n",
              "0             1.0        0.333333  0.124020  0.817073           0.841577   \n",
              "1             0.0        0.666667  0.708473  0.743902           0.744130   \n",
              "2             0.0        0.666667  0.426542  0.975610           0.389784   \n",
              "3             1.0        1.000000  0.825089  0.597561           0.630124   \n",
              "4             0.0        0.666667  0.022827  0.963415           0.640760   \n",
              "\n",
              "        bmi  stroke  \n",
              "0  0.375000     1.0  \n",
              "1  0.296037     1.0  \n",
              "2  0.332992     1.0  \n",
              "3  0.352459     1.0  \n",
              "4  0.245902     1.0  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-f9438057-9dc1-4f22-9681-36abca56191b\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>gender</th>\n",
              "      <th>hypertension</th>\n",
              "      <th>heart_disease</th>\n",
              "      <th>ever_married</th>\n",
              "      <th>work_type</th>\n",
              "      <th>Residence_type</th>\n",
              "      <th>smoking_status</th>\n",
              "      <th>id</th>\n",
              "      <th>age</th>\n",
              "      <th>avg_glucose_level</th>\n",
              "      <th>bmi</th>\n",
              "      <th>stroke</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0.5</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.50</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.333333</td>\n",
              "      <td>0.124020</td>\n",
              "      <td>0.817073</td>\n",
              "      <td>0.841577</td>\n",
              "      <td>0.375000</td>\n",
              "      <td>1.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.75</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.666667</td>\n",
              "      <td>0.708473</td>\n",
              "      <td>0.743902</td>\n",
              "      <td>0.744130</td>\n",
              "      <td>0.296037</td>\n",
              "      <td>1.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0.5</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.50</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.666667</td>\n",
              "      <td>0.426542</td>\n",
              "      <td>0.975610</td>\n",
              "      <td>0.389784</td>\n",
              "      <td>0.332992</td>\n",
              "      <td>1.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.50</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.825089</td>\n",
              "      <td>0.597561</td>\n",
              "      <td>0.630124</td>\n",
              "      <td>0.352459</td>\n",
              "      <td>1.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.75</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.666667</td>\n",
              "      <td>0.022827</td>\n",
              "      <td>0.963415</td>\n",
              "      <td>0.640760</td>\n",
              "      <td>0.245902</td>\n",
              "      <td>1.0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-f9438057-9dc1-4f22-9681-36abca56191b')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-f9438057-9dc1-4f22-9681-36abca56191b button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-f9438057-9dc1-4f22-9681-36abca56191b');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KICuY0lg5nUD"
      },
      "source": [
        "###Removing Unnecessary Features"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ua71U2YxL5Ls"
      },
      "source": [
        "From the features that we have, remove one that is irrelevant for our predictions"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8AHNqYbh5sE7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "outputId": "228a680f-5cc6-4191-e4be-26a2f5b7a50b"
      },
      "source": [
        "#Test Your Zaka\n",
        "norm_data=norm_data.drop(['id'], axis=1)\n",
        "norm_data.head()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "   gender  hypertension  heart_disease  ever_married  work_type  \\\n",
              "0     0.5           0.0            1.0           1.0       0.50   \n",
              "1     0.0           0.0            0.0           1.0       0.75   \n",
              "2     0.5           0.0            1.0           1.0       0.50   \n",
              "3     0.0           0.0            0.0           1.0       0.50   \n",
              "4     0.0           1.0            0.0           1.0       0.75   \n",
              "\n",
              "   Residence_type  smoking_status       age  avg_glucose_level       bmi  \\\n",
              "0             1.0        0.333333  0.817073           0.841577  0.375000   \n",
              "1             0.0        0.666667  0.743902           0.744130  0.296037   \n",
              "2             0.0        0.666667  0.975610           0.389784  0.332992   \n",
              "3             1.0        1.000000  0.597561           0.630124  0.352459   \n",
              "4             0.0        0.666667  0.963415           0.640760  0.245902   \n",
              "\n",
              "   stroke  \n",
              "0     1.0  \n",
              "1     1.0  \n",
              "2     1.0  \n",
              "3     1.0  \n",
              "4     1.0  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-c248d2d0-7193-4136-a846-830f8b505f1f\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>gender</th>\n",
              "      <th>hypertension</th>\n",
              "      <th>heart_disease</th>\n",
              "      <th>ever_married</th>\n",
              "      <th>work_type</th>\n",
              "      <th>Residence_type</th>\n",
              "      <th>smoking_status</th>\n",
              "      <th>age</th>\n",
              "      <th>avg_glucose_level</th>\n",
              "      <th>bmi</th>\n",
              "      <th>stroke</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0.5</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.50</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.333333</td>\n",
              "      <td>0.817073</td>\n",
              "      <td>0.841577</td>\n",
              "      <td>0.375000</td>\n",
              "      <td>1.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.75</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.666667</td>\n",
              "      <td>0.743902</td>\n",
              "      <td>0.744130</td>\n",
              "      <td>0.296037</td>\n",
              "      <td>1.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0.5</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.50</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.666667</td>\n",
              "      <td>0.975610</td>\n",
              "      <td>0.389784</td>\n",
              "      <td>0.332992</td>\n",
              "      <td>1.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.50</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.597561</td>\n",
              "      <td>0.630124</td>\n",
              "      <td>0.352459</td>\n",
              "      <td>1.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.75</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.666667</td>\n",
              "      <td>0.963415</td>\n",
              "      <td>0.640760</td>\n",
              "      <td>0.245902</td>\n",
              "      <td>1.0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-c248d2d0-7193-4136-a846-830f8b505f1f')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-c248d2d0-7193-4136-a846-830f8b505f1f button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-c248d2d0-7193-4136-a846-830f8b505f1f');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5k-KoLpH5C9R"
      },
      "source": [
        "#Building the DL Model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uI2VtlafMBeN"
      },
      "source": [
        "Now it's time to build the actual model, and observe a summary of it.<br>\n",
        "The sizes of the **hidden** layers that you should use are: [32,16,8,4,2].\n",
        "The activation for each of those hidden layers is 'relu'\n",
        "<br>\n",
        "Print the summary of your model."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AZvKqqT65E0W",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e38d7396-f711-4473-a160-24b0e3f4c5cb"
      },
      "source": [
        "#Test Your Zaka\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense\n",
        "\n",
        "def create_model():\n",
        "\n",
        "  model=Sequential()\n",
        "  model.add(Dense(32, input_dim=10, activation='relu'))\n",
        "  model.add(Dense(16, activation='relu'))\n",
        "  model.add(Dense(8, activation='relu'))\n",
        "  model.add(Dense(4, activation='relu'))\n",
        "  model.add(Dense(2, activation='relu'))\n",
        "  model.add(Dense(1, activation='sigmoid'))\n",
        "\n",
        "  print(model.summary())\n",
        "\n",
        "  return model\n",
        "\n",
        "model=create_model()\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential_1\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " dense_6 (Dense)             (None, 32)                352       \n",
            "                                                                 \n",
            " dense_7 (Dense)             (None, 16)                528       \n",
            "                                                                 \n",
            " dense_8 (Dense)             (None, 8)                 136       \n",
            "                                                                 \n",
            " dense_9 (Dense)             (None, 4)                 36        \n",
            "                                                                 \n",
            " dense_10 (Dense)            (None, 2)                 10        \n",
            "                                                                 \n",
            " dense_11 (Dense)            (None, 1)                 3         \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 1,065\n",
            "Trainable params: 1,065\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "None\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AD57fE2n7QP4"
      },
      "source": [
        "###Compiling the model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "seGmh1x1-qH9"
      },
      "source": [
        "Now we compile the model. Here we want to measure the accuracy as well as the precision and recall to know better about the performance of our model.\n",
        "We will use 'adam' as optimizer."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "woSsSTEm61_U"
      },
      "source": [
        "#Test Your Zaka\n",
        "\n",
        "from keras import backend as K\n",
        "\n",
        "def recall(y_true, y_pred):\n",
        "    true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
        "    possible_positives = K.sum(K.round(K.clip(y_true, 0, 1)))\n",
        "    recall = true_positives / (possible_positives + K.epsilon())\n",
        "    return recall\n",
        "\n",
        "def precision(y_true, y_pred):\n",
        "    true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
        "    predicted_positives = K.sum(K.round(K.clip(y_pred, 0, 1)))\n",
        "    precision = true_positives / (predicted_positives + K.epsilon())\n",
        "    return precision\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense\n",
        "\n",
        "def create_model():\n",
        "\n",
        "  model=Sequential()\n",
        "  model.add(Dense(32, input_dim=10, activation='relu'))\n",
        "  model.add(Dense(16, activation='relu'))\n",
        "  model.add(Dense(8, activation='relu'))\n",
        "  model.add(Dense(4, activation='relu'))\n",
        "  model.add(Dense(2, activation='relu'))\n",
        "  model.add(Dense(1, activation='sigmoid'))\n",
        "\n",
        "  print(model.summary())\n",
        "  model.compile(loss='binary_crossentropy',optimizer='adam', metrics=['accuracy', precision, recall])\n",
        "\n",
        "  return model\n",
        "\n",
        "model1=create_model()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "n5TJPuUaTQV4",
        "outputId": "062d80bf-2bb3-445a-c4aa-b10f2b5e3d0c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential_2\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " dense_12 (Dense)            (None, 32)                352       \n",
            "                                                                 \n",
            " dense_13 (Dense)            (None, 16)                528       \n",
            "                                                                 \n",
            " dense_14 (Dense)            (None, 8)                 136       \n",
            "                                                                 \n",
            " dense_15 (Dense)            (None, 4)                 36        \n",
            "                                                                 \n",
            " dense_16 (Dense)            (None, 2)                 10        \n",
            "                                                                 \n",
            " dense_17 (Dense)            (None, 1)                 3         \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 1,065\n",
            "Trainable params: 1,065\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "None\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U5zevRH57X8v"
      },
      "source": [
        "###Fitting the data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UhaEU26KMUWK"
      },
      "source": [
        "we split our dataset between training and testing, and we fit the model on training data (70%), and validate on the testing data (30%). The training happens for 15 epochs."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rsVOVfn47MLn",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "bb04042f-0b3d-47d7-c9f6-651e9cdae74c"
      },
      "source": [
        "#Test Your Zaka\n",
        "from sklearn.model_selection import train_test_split \n",
        "\n",
        "x=norm_data.iloc[:,:-1].values \n",
        "y=norm_data.iloc[:,-1].values\n",
        "\n",
        "x_train, x_test, y_train, y_test = train_test_split(x,y, test_size=0.3)\n",
        "\n",
        "print(f\" We have {x_train.shape} example for trainning and {x_test.shape} example for testing\")\n",
        "\n",
        "model1.fit(x_train, y_train, epochs=15)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " We have (3577, 10) example for trainning and (1533, 10) example for testing\n",
            "Epoch 1/15\n",
            "112/112 [==============================] - 1s 2ms/step - loss: 0.6689 - accuracy: 0.9508 - precision: 0.0000e+00 - recall: 0.0000e+00\n",
            "Epoch 2/15\n",
            "112/112 [==============================] - 0s 2ms/step - loss: 0.6229 - accuracy: 0.9508 - precision: 0.0000e+00 - recall: 0.0000e+00\n",
            "Epoch 3/15\n",
            "112/112 [==============================] - 0s 2ms/step - loss: 0.5811 - accuracy: 0.9508 - precision: 0.0000e+00 - recall: 0.0000e+00\n",
            "Epoch 4/15\n",
            "112/112 [==============================] - 0s 2ms/step - loss: 0.5432 - accuracy: 0.9508 - precision: 0.0000e+00 - recall: 0.0000e+00\n",
            "Epoch 5/15\n",
            "112/112 [==============================] - 0s 2ms/step - loss: 0.5089 - accuracy: 0.9508 - precision: 0.0000e+00 - recall: 0.0000e+00\n",
            "Epoch 6/15\n",
            "112/112 [==============================] - 0s 2ms/step - loss: 0.4780 - accuracy: 0.9508 - precision: 0.0000e+00 - recall: 0.0000e+00\n",
            "Epoch 7/15\n",
            "112/112 [==============================] - 0s 2ms/step - loss: 0.4501 - accuracy: 0.9508 - precision: 0.0000e+00 - recall: 0.0000e+00\n",
            "Epoch 8/15\n",
            "112/112 [==============================] - 0s 2ms/step - loss: 0.4249 - accuracy: 0.9508 - precision: 0.0000e+00 - recall: 0.0000e+00\n",
            "Epoch 9/15\n",
            "112/112 [==============================] - 0s 2ms/step - loss: 0.4021 - accuracy: 0.9508 - precision: 0.0000e+00 - recall: 0.0000e+00\n",
            "Epoch 10/15\n",
            "112/112 [==============================] - 0s 1ms/step - loss: 0.3816 - accuracy: 0.9508 - precision: 0.0000e+00 - recall: 0.0000e+00\n",
            "Epoch 11/15\n",
            "112/112 [==============================] - 0s 2ms/step - loss: 0.3631 - accuracy: 0.9508 - precision: 0.0000e+00 - recall: 0.0000e+00\n",
            "Epoch 12/15\n",
            "112/112 [==============================] - 0s 2ms/step - loss: 0.3464 - accuracy: 0.9508 - precision: 0.0000e+00 - recall: 0.0000e+00\n",
            "Epoch 13/15\n",
            "112/112 [==============================] - 0s 2ms/step - loss: 0.3314 - accuracy: 0.9508 - precision: 0.0000e+00 - recall: 0.0000e+00\n",
            "Epoch 14/15\n",
            "112/112 [==============================] - 0s 2ms/step - loss: 0.3178 - accuracy: 0.9508 - precision: 0.0000e+00 - recall: 0.0000e+00\n",
            "Epoch 15/15\n",
            "112/112 [==============================] - 0s 2ms/step - loss: 0.3056 - accuracy: 0.9508 - precision: 0.0000e+00 - recall: 0.0000e+00\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7f2fdcebdb90>"
            ]
          },
          "metadata": {},
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "scores=model1.evaluate(x_test, y_test)\n",
        "\n",
        "print(\"\\n%s: %.2f%%\" % (model1.metrics_names[1], scores[1]*100))\n",
        "print(\"\\n%s: %.2f%%\" % (model1.metrics_names[2], scores[2]*100))\n",
        "print(\"\\n%s: %.2f%%\" % (model1.metrics_names[3], scores[3]*100))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AncPWPiQ_8kP",
        "outputId": "fa29fb9a-2dc8-4570-f8de-ff23f0584546"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "48/48 [==============================] - 0s 1ms/step - loss: 0.2976 - accuracy: 0.9524 - precision: 0.0000e+00 - recall: 0.0000e+00\n",
            "\n",
            "accuracy: 95.24%\n",
            "\n",
            "precision: 0.00%\n",
            "\n",
            "recall: 0.00%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_sPVIoABAa8Q"
      },
      "source": [
        "What can you deduce from the results you obtained?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "owY9hyWFimQQ"
      },
      "source": [
        "**our model wasn't able to predict positive cases. (Since precision and recall were 0 then it just learnt for negative)**\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "voqUo00lAqCZ"
      },
      "source": [
        "#Improving DL Models"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IImSYWQGBSz6"
      },
      "source": [
        "###Checking For Data Imbalance"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZHxCONODMkNN"
      },
      "source": [
        "Plot a histogram that shows the distribution of 'stroke' column"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1bF6G8c58SOE",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 316
        },
        "outputId": "ed7b8fe2-a1f3-4a35-d70c-b1feb7f30496"
      },
      "source": [
        "#Test Your Zaka\n",
        "\n",
        "had_stroke=norm_data[(norm_data['stroke']==1)]\n",
        "no_stroke=norm_data[(norm_data['stroke']==0)]\n",
        "\n",
        "print(f\"Number who had stroke= {had_stroke.shape[0]}\")\n",
        "print(f\"Number who had not stroke= {no_stroke.shape[0]}\")\n",
        "\n",
        "plt.hist(had_stroke['stroke'], label='yes')\n",
        "plt.hist(no_stroke['stroke'], label='no')\n",
        "\n",
        "plt.title(\"Distribution of those who had stroke and those of had not\")\n",
        "plt.ylabel(\"Frequency\")\n",
        "plt.legend()\n",
        "plt.show()\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number who had stroke= 249\n",
            "Number who had not stroke= 4861\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gzZBB5GMCOld"
      },
      "source": [
        "We have a huge imbalance in the data, this is why we fix it with oversamppling and undersampling.\n",
        "\n",
        "This time, you will learn to oversample using the SMOTE() function instead of random oversampling, and this is because SMOTE will generate new data based on the data that we have, so we avoid overfitting."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w66slO25jE1O"
      },
      "source": [
        "After doing that, plot the new histogram showing the proportions of people having stroke or not"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "US4xON4LBX8I",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 316
        },
        "outputId": "1b9deaf1-94e8-4302-aadd-d23353709e18"
      },
      "source": [
        "#Test Your Zaka\n",
        "# SMOTE: Synthetic Minority Oversampling Technique\n",
        "import imblearn\n",
        "from imblearn.over_sampling import SMOTE\n",
        "from collections import Counter\n",
        "\n",
        "\n",
        "# transform the dataset\n",
        "oversample = SMOTE()\n",
        "x_bal, y_bal = oversample.fit_resample(x, y)\n",
        "\n",
        "counter = Counter(y_bal)\n",
        "print(f\"The new balanced dataset is now composed of {counter}\")\n",
        "\n",
        "plt.hist(y_bal)\n",
        "plt.title(\"Distribution of those who had stroke and those of had not\")\n",
        "plt.ylabel(\"Frequency\")\n",
        "plt.legend()\n",
        "plt.show()\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "No handles with labels found to put in legend.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The new balanced dataset is now composed of Counter({1.0: 4861, 0.0: 4861})\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u6MOHwo_Ew08"
      },
      "source": [
        "Now we will fit our same model on the new balanced data that we have, with the same conditions we had before (train/test splits, epochs, etc)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "26YTispLEm9N",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b18a3711-e1e4-436a-cae9-dcfc3fd4ad62"
      },
      "source": [
        "#Test Your Zaka\n",
        "\n",
        "x_train2, x_test2, y_train2, y_test2 = train_test_split(x_bal,y_bal, test_size=0.3)\n",
        "\n",
        "print(f\" We have {x_train2.shape} example for training and {x_test2.shape} example for testing\")\n",
        "\n",
        "model2=create_model()\n",
        "model2.fit(x_train2, y_train2, epochs=15)\n",
        "\n",
        "scores2=model2.evaluate(x_test2, y_test2)\n",
        "\n",
        "print(\"\\n%s: %.2f%%\" % (model2.metrics_names[1], scores2[1]*100))\n",
        "print(\"\\n%s: %.2f%%\" % (model2.metrics_names[2], scores2[2]*100))\n",
        "print(\"\\n%s: %.2f%%\" % (model2.metrics_names[3], scores2[3]*100))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " We have (6805, 10) example for training and (2917, 10) example for testing\n",
            "Model: \"sequential_3\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " dense_18 (Dense)            (None, 32)                352       \n",
            "                                                                 \n",
            " dense_19 (Dense)            (None, 16)                528       \n",
            "                                                                 \n",
            " dense_20 (Dense)            (None, 8)                 136       \n",
            "                                                                 \n",
            " dense_21 (Dense)            (None, 4)                 36        \n",
            "                                                                 \n",
            " dense_22 (Dense)            (None, 2)                 10        \n",
            "                                                                 \n",
            " dense_23 (Dense)            (None, 1)                 3         \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 1,065\n",
            "Trainable params: 1,065\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "None\n",
            "Epoch 1/15\n",
            "213/213 [==============================] - 1s 2ms/step - loss: 0.6422 - accuracy: 0.6464 - precision: 0.6052 - recall: 0.8917\n",
            "Epoch 2/15\n",
            "213/213 [==============================] - 0s 2ms/step - loss: 0.5836 - accuracy: 0.7680 - precision: 0.7172 - recall: 0.8970\n",
            "Epoch 3/15\n",
            "213/213 [==============================] - 0s 2ms/step - loss: 0.5546 - accuracy: 0.7853 - precision: 0.7424 - recall: 0.8830\n",
            "Epoch 4/15\n",
            "213/213 [==============================] - 0s 2ms/step - loss: 0.5122 - accuracy: 0.7906 - precision: 0.7600 - recall: 0.8578\n",
            "Epoch 5/15\n",
            "213/213 [==============================] - 0s 2ms/step - loss: 0.4582 - accuracy: 0.7844 - precision: 0.7514 - recall: 0.8599\n",
            "Epoch 6/15\n",
            "213/213 [==============================] - 0s 1ms/step - loss: 0.4483 - accuracy: 0.7960 - precision: 0.7643 - recall: 0.8590\n",
            "Epoch 7/15\n",
            "213/213 [==============================] - 0s 1ms/step - loss: 0.4406 - accuracy: 0.7975 - precision: 0.7700 - recall: 0.8603\n",
            "Epoch 8/15\n",
            "213/213 [==============================] - 0s 2ms/step - loss: 0.4362 - accuracy: 0.7990 - precision: 0.7726 - recall: 0.8593\n",
            "Epoch 9/15\n",
            "213/213 [==============================] - 0s 2ms/step - loss: 0.4294 - accuracy: 0.8026 - precision: 0.7753 - recall: 0.8618\n",
            "Epoch 10/15\n",
            "213/213 [==============================] - 0s 2ms/step - loss: 0.4273 - accuracy: 0.8047 - precision: 0.7811 - recall: 0.8560\n",
            "Epoch 11/15\n",
            "213/213 [==============================] - 0s 2ms/step - loss: 0.4261 - accuracy: 0.8053 - precision: 0.7811 - recall: 0.8535\n",
            "Epoch 12/15\n",
            "213/213 [==============================] - 0s 1ms/step - loss: 0.4181 - accuracy: 0.8091 - precision: 0.7861 - recall: 0.8530\n",
            "Epoch 13/15\n",
            "213/213 [==============================] - 0s 2ms/step - loss: 0.4154 - accuracy: 0.8078 - precision: 0.7843 - recall: 0.8519\n",
            "Epoch 14/15\n",
            "213/213 [==============================] - 0s 2ms/step - loss: 0.4106 - accuracy: 0.8143 - precision: 0.7914 - recall: 0.8570\n",
            "Epoch 15/15\n",
            "213/213 [==============================] - 0s 2ms/step - loss: 0.4074 - accuracy: 0.8159 - precision: 0.7936 - recall: 0.8558\n",
            "92/92 [==============================] - 0s 1ms/step - loss: 0.4014 - accuracy: 0.8149 - precision: 0.7809 - recall: 0.8759\n",
            "\n",
            "accuracy: 81.49%\n",
            "\n",
            "precision: 78.09%\n",
            "\n",
            "recall: 87.59%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hhfhpIaWGtz2"
      },
      "source": [
        "Comment the performance you obtained"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rwQClRsEjkUb"
      },
      "source": [
        "**The accuracy decreased since in the previous one it just learnt on the negative cases(since they were the majority) and tested on negative cases in the majority of instances. However, now it was trained in both negative and positive cases. The precision and recall were boosted in this model since now it was trained enough on positive cases so it was able to correclty predict on new cases.**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ngJVLbRKG7U_"
      },
      "source": [
        "###Model Design"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sMdcXspCHxIo"
      },
      "source": [
        "Now you will introduce batch normalization after each layer of your network"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yK78-g-hHrmq"
      },
      "source": [
        "#Test Your Zaka\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense\n",
        "from tensorflow.keras.layers import BatchNormalization\n",
        "\n",
        "def create_model2():\n",
        "  model=Sequential()\n",
        "  model.add(Dense(32, input_dim=10, activation='relu'))\n",
        "  model.add(BatchNormalization())\n",
        "  model.add(Dense(16, activation='relu'))\n",
        "  model.add(BatchNormalization())\n",
        "  model.add(Dense(8, activation='relu'))\n",
        "  model.add(BatchNormalization())\n",
        "  model.add(Dense(4, activation='relu'))\n",
        "  model.add(BatchNormalization())\n",
        "  model.add(Dense(2, activation='relu'))\n",
        "  model.add(BatchNormalization())\n",
        "  model.add(Dense(1, activation='sigmoid'))\n",
        "\n",
        "  print(model.summary())\n",
        "  model.compile(loss='binary_crossentropy',optimizer='adam', metrics=['accuracy', precision, recall])\n",
        "\n",
        "  return model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l3uBII4wJPbk"
      },
      "source": [
        "Let's train the model with the same conditions as before"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "REDrQVWLJLs5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a2d53ef8-b066-43d5-d386-a987c698ad56"
      },
      "source": [
        "#Test Your Zaka\n",
        "\n",
        "model3=create_model2()\n",
        "model3.fit(x_train2, y_train2, epochs=15)\n",
        "\n",
        "scores2=model3.evaluate(x_test2, y_test2)\n",
        "\n",
        "print(\"\\n%s: %.2f%%\" % (model3.metrics_names[1], scores2[1]*100))\n",
        "print(\"\\n%s: %.2f%%\" % (model3.metrics_names[2], scores2[2]*100))\n",
        "print(\"\\n%s: %.2f%%\" % (model3.metrics_names[3], scores2[3]*100))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential_4\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " dense_24 (Dense)            (None, 32)                352       \n",
            "                                                                 \n",
            " batch_normalization (BatchN  (None, 32)               128       \n",
            " ormalization)                                                   \n",
            "                                                                 \n",
            " dense_25 (Dense)            (None, 16)                528       \n",
            "                                                                 \n",
            " batch_normalization_1 (Batc  (None, 16)               64        \n",
            " hNormalization)                                                 \n",
            "                                                                 \n",
            " dense_26 (Dense)            (None, 8)                 136       \n",
            "                                                                 \n",
            " batch_normalization_2 (Batc  (None, 8)                32        \n",
            " hNormalization)                                                 \n",
            "                                                                 \n",
            " dense_27 (Dense)            (None, 4)                 36        \n",
            "                                                                 \n",
            " batch_normalization_3 (Batc  (None, 4)                16        \n",
            " hNormalization)                                                 \n",
            "                                                                 \n",
            " dense_28 (Dense)            (None, 2)                 10        \n",
            "                                                                 \n",
            " batch_normalization_4 (Batc  (None, 2)                8         \n",
            " hNormalization)                                                 \n",
            "                                                                 \n",
            " dense_29 (Dense)            (None, 1)                 3         \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 1,313\n",
            "Trainable params: 1,189\n",
            "Non-trainable params: 124\n",
            "_________________________________________________________________\n",
            "None\n",
            "Epoch 1/15\n",
            "213/213 [==============================] - 2s 2ms/step - loss: 0.6179 - accuracy: 0.6824 - precision: 0.6984 - recall: 0.6561\n",
            "Epoch 2/15\n",
            "213/213 [==============================] - 1s 2ms/step - loss: 0.5126 - accuracy: 0.7706 - precision: 0.7542 - recall: 0.8143\n",
            "Epoch 3/15\n",
            "213/213 [==============================] - 0s 2ms/step - loss: 0.4815 - accuracy: 0.7847 - precision: 0.7580 - recall: 0.8498\n",
            "Epoch 4/15\n",
            "213/213 [==============================] - 0s 2ms/step - loss: 0.4635 - accuracy: 0.7882 - precision: 0.7520 - recall: 0.8722\n",
            "Epoch 5/15\n",
            "213/213 [==============================] - 0s 2ms/step - loss: 0.4485 - accuracy: 0.7931 - precision: 0.7553 - recall: 0.8776\n",
            "Epoch 6/15\n",
            "213/213 [==============================] - 1s 3ms/step - loss: 0.4433 - accuracy: 0.8007 - precision: 0.7668 - recall: 0.8735\n",
            "Epoch 7/15\n",
            "213/213 [==============================] - 1s 2ms/step - loss: 0.4383 - accuracy: 0.7976 - precision: 0.7583 - recall: 0.8884\n",
            "Epoch 8/15\n",
            "213/213 [==============================] - 0s 2ms/step - loss: 0.4304 - accuracy: 0.8035 - precision: 0.7704 - recall: 0.8808\n",
            "Epoch 9/15\n",
            "213/213 [==============================] - 1s 3ms/step - loss: 0.4221 - accuracy: 0.8100 - precision: 0.7762 - recall: 0.8830\n",
            "Epoch 10/15\n",
            "213/213 [==============================] - 1s 3ms/step - loss: 0.4137 - accuracy: 0.8132 - precision: 0.7808 - recall: 0.8850\n",
            "Epoch 11/15\n",
            "213/213 [==============================] - 0s 2ms/step - loss: 0.4101 - accuracy: 0.8159 - precision: 0.7804 - recall: 0.8873\n",
            "Epoch 12/15\n",
            "213/213 [==============================] - 1s 2ms/step - loss: 0.4025 - accuracy: 0.8175 - precision: 0.7871 - recall: 0.8833\n",
            "Epoch 13/15\n",
            "213/213 [==============================] - 1s 2ms/step - loss: 0.4040 - accuracy: 0.8096 - precision: 0.7742 - recall: 0.8845\n",
            "Epoch 14/15\n",
            "213/213 [==============================] - 0s 2ms/step - loss: 0.3958 - accuracy: 0.8197 - precision: 0.7858 - recall: 0.8901\n",
            "Epoch 15/15\n",
            "213/213 [==============================] - 0s 2ms/step - loss: 0.4007 - accuracy: 0.8172 - precision: 0.7864 - recall: 0.8861\n",
            "92/92 [==============================] - 0s 1ms/step - loss: 0.3595 - accuracy: 0.8450 - precision: 0.8074 - recall: 0.9049\n",
            "\n",
            "accuracy: 84.50%\n",
            "\n",
            "precision: 80.74%\n",
            "\n",
            "recall: 90.49%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hhkSEThVKEdm"
      },
      "source": [
        "Comment the performance of your model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dghb5Pkaj4fs"
      },
      "source": [
        "**All the metrics scores increased when we perfomed batch normalization**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mXf6cpO_KWTs"
      },
      "source": [
        "###Hyperparameter Tuning"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l2rMJJEcNDil"
      },
      "source": [
        "Now we will tune some hyperparameters of our model."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PQl4VxrOLQGO"
      },
      "source": [
        "We start by wrapping our model inside a kerasClassifier to be able to use it in Scikit Learn"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PYBO5H5LLW4w",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "98e6830a-480e-43bd-ee39-33d698f0cf57"
      },
      "source": [
        "#Test Your Zaka\n",
        "from tensorflow.keras.wrappers.scikit_learn import KerasClassifier\n",
        "model4 = KerasClassifier(build_fn=create_model2)\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:3: DeprecationWarning: KerasClassifier is deprecated, use Sci-Keras (https://github.com/adriangb/scikeras) instead. See https://www.adriangb.com/scikeras/stable/migration.html for help migrating.\n",
            "  This is separate from the ipykernel package so we can avoid doing imports until\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aIqMITAEKoue"
      },
      "source": [
        "We will tune the batch size (it can be 50 or 100) and the number of epochs (it can be 50 or 100).\n",
        "We will use a 3 folds cross validation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E_XXldXwJ-Ch",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "99386222-75a4-423e-9b68-c9f064434f3a"
      },
      "source": [
        "#Test Your Zaka\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "\n",
        "param_grid = {\n",
        "    'batch_size': [50,100],\n",
        "    'epochs': [50, 100],\n",
        "}\n",
        "\n",
        "DL_search = GridSearchCV(model4, param_grid=param_grid, verbose = 3, cv=3)\n",
        "\n",
        "# fitting the model for grid search \n",
        "DL_search.fit(x_train2 , y_train2)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fitting 3 folds for each of 4 candidates, totalling 12 fits\n",
            "Model: \"sequential_5\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " dense_30 (Dense)            (None, 32)                352       \n",
            "                                                                 \n",
            " batch_normalization_5 (Batc  (None, 32)               128       \n",
            " hNormalization)                                                 \n",
            "                                                                 \n",
            " dense_31 (Dense)            (None, 16)                528       \n",
            "                                                                 \n",
            " batch_normalization_6 (Batc  (None, 16)               64        \n",
            " hNormalization)                                                 \n",
            "                                                                 \n",
            " dense_32 (Dense)            (None, 8)                 136       \n",
            "                                                                 \n",
            " batch_normalization_7 (Batc  (None, 8)                32        \n",
            " hNormalization)                                                 \n",
            "                                                                 \n",
            " dense_33 (Dense)            (None, 4)                 36        \n",
            "                                                                 \n",
            " batch_normalization_8 (Batc  (None, 4)                16        \n",
            " hNormalization)                                                 \n",
            "                                                                 \n",
            " dense_34 (Dense)            (None, 2)                 10        \n",
            "                                                                 \n",
            " batch_normalization_9 (Batc  (None, 2)                8         \n",
            " hNormalization)                                                 \n",
            "                                                                 \n",
            " dense_35 (Dense)            (None, 1)                 3         \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 1,313\n",
            "Trainable params: 1,189\n",
            "Non-trainable params: 124\n",
            "_________________________________________________________________\n",
            "None\n",
            "Epoch 1/50\n",
            "91/91 [==============================] - 2s 3ms/step - loss: 0.6184 - accuracy: 0.6495 - precision: 0.6129 - recall: 0.8523\n",
            "Epoch 2/50\n",
            "91/91 [==============================] - 0s 3ms/step - loss: 0.5352 - accuracy: 0.7465 - precision: 0.7030 - recall: 0.8693\n",
            "Epoch 3/50\n",
            "91/91 [==============================] - 0s 3ms/step - loss: 0.4918 - accuracy: 0.7804 - precision: 0.7467 - recall: 0.8627\n",
            "Epoch 4/50\n",
            "91/91 [==============================] - 0s 3ms/step - loss: 0.4728 - accuracy: 0.7930 - precision: 0.7659 - recall: 0.8551\n",
            "Epoch 5/50\n",
            "91/91 [==============================] - 0s 3ms/step - loss: 0.4569 - accuracy: 0.7961 - precision: 0.7660 - recall: 0.8672\n",
            "Epoch 6/50\n",
            "91/91 [==============================] - 0s 3ms/step - loss: 0.4446 - accuracy: 0.8016 - precision: 0.7763 - recall: 0.8592\n",
            "Epoch 7/50\n",
            "91/91 [==============================] - 0s 3ms/step - loss: 0.4357 - accuracy: 0.8053 - precision: 0.7814 - recall: 0.8581\n",
            "Epoch 8/50\n",
            "91/91 [==============================] - 0s 3ms/step - loss: 0.4362 - accuracy: 0.8064 - precision: 0.7865 - recall: 0.8536\n",
            "Epoch 9/50\n",
            "91/91 [==============================] - 0s 2ms/step - loss: 0.4262 - accuracy: 0.8045 - precision: 0.7828 - recall: 0.8512\n",
            "Epoch 10/50\n",
            "91/91 [==============================] - 0s 3ms/step - loss: 0.4226 - accuracy: 0.8084 - precision: 0.7825 - recall: 0.8652\n",
            "Epoch 11/50\n",
            "91/91 [==============================] - 0s 2ms/step - loss: 0.4201 - accuracy: 0.8113 - precision: 0.7890 - recall: 0.8594\n",
            "Epoch 12/50\n",
            "91/91 [==============================] - 0s 3ms/step - loss: 0.4101 - accuracy: 0.8073 - precision: 0.7867 - recall: 0.8513\n",
            "Epoch 13/50\n",
            "91/91 [==============================] - 0s 3ms/step - loss: 0.4097 - accuracy: 0.8115 - precision: 0.7887 - recall: 0.8657\n",
            "Epoch 14/50\n",
            "91/91 [==============================] - 0s 3ms/step - loss: 0.4091 - accuracy: 0.8126 - precision: 0.7889 - recall: 0.8694\n",
            "Epoch 15/50\n",
            "91/91 [==============================] - 0s 3ms/step - loss: 0.3952 - accuracy: 0.8139 - precision: 0.7910 - recall: 0.8697\n",
            "Epoch 16/50\n",
            "91/91 [==============================] - 0s 3ms/step - loss: 0.3939 - accuracy: 0.8219 - precision: 0.7938 - recall: 0.8850\n",
            "Epoch 17/50\n",
            "91/91 [==============================] - 0s 3ms/step - loss: 0.3877 - accuracy: 0.8254 - precision: 0.7970 - recall: 0.8851\n",
            "Epoch 18/50\n",
            "91/91 [==============================] - 0s 2ms/step - loss: 0.3818 - accuracy: 0.8307 - precision: 0.8033 - recall: 0.8873\n",
            "Epoch 19/50\n",
            "91/91 [==============================] - 0s 2ms/step - loss: 0.3787 - accuracy: 0.8236 - precision: 0.7943 - recall: 0.8845\n",
            "Epoch 20/50\n",
            "91/91 [==============================] - 0s 2ms/step - loss: 0.3778 - accuracy: 0.8256 - precision: 0.8029 - recall: 0.8764\n",
            "Epoch 21/50\n",
            "91/91 [==============================] - 0s 3ms/step - loss: 0.3658 - accuracy: 0.8298 - precision: 0.8087 - recall: 0.8765\n",
            "Epoch 22/50\n",
            "91/91 [==============================] - 0s 3ms/step - loss: 0.3682 - accuracy: 0.8320 - precision: 0.8072 - recall: 0.8836\n",
            "Epoch 23/50\n",
            "91/91 [==============================] - 0s 2ms/step - loss: 0.3732 - accuracy: 0.8261 - precision: 0.8004 - recall: 0.8834\n",
            "Epoch 24/50\n",
            "91/91 [==============================] - 0s 2ms/step - loss: 0.3618 - accuracy: 0.8382 - precision: 0.8087 - recall: 0.8965\n",
            "Epoch 25/50\n",
            "91/91 [==============================] - 0s 2ms/step - loss: 0.3708 - accuracy: 0.8329 - precision: 0.8035 - recall: 0.8959\n",
            "Epoch 26/50\n",
            "91/91 [==============================] - 0s 2ms/step - loss: 0.3626 - accuracy: 0.8386 - precision: 0.8063 - recall: 0.9017\n",
            "Epoch 27/50\n",
            "91/91 [==============================] - 0s 3ms/step - loss: 0.3554 - accuracy: 0.8424 - precision: 0.8104 - recall: 0.9054\n",
            "Epoch 28/50\n",
            "91/91 [==============================] - 0s 3ms/step - loss: 0.3618 - accuracy: 0.8408 - precision: 0.8096 - recall: 0.8996\n",
            "Epoch 29/50\n",
            "91/91 [==============================] - 0s 2ms/step - loss: 0.3533 - accuracy: 0.8468 - precision: 0.8163 - recall: 0.9048\n",
            "Epoch 30/50\n",
            "91/91 [==============================] - 0s 2ms/step - loss: 0.3468 - accuracy: 0.8481 - precision: 0.8164 - recall: 0.9092\n",
            "Epoch 31/50\n",
            "91/91 [==============================] - 0s 3ms/step - loss: 0.3471 - accuracy: 0.8444 - precision: 0.8159 - recall: 0.8992\n",
            "Epoch 32/50\n",
            "91/91 [==============================] - 0s 2ms/step - loss: 0.3479 - accuracy: 0.8483 - precision: 0.8138 - recall: 0.9119\n",
            "Epoch 33/50\n",
            "91/91 [==============================] - 0s 3ms/step - loss: 0.3492 - accuracy: 0.8472 - precision: 0.8140 - recall: 0.9080\n",
            "Epoch 34/50\n",
            "91/91 [==============================] - 0s 3ms/step - loss: 0.3477 - accuracy: 0.8470 - precision: 0.8135 - recall: 0.9088\n",
            "Epoch 35/50\n",
            "91/91 [==============================] - 0s 2ms/step - loss: 0.3450 - accuracy: 0.8461 - precision: 0.8141 - recall: 0.9076\n",
            "Epoch 36/50\n",
            "91/91 [==============================] - 0s 3ms/step - loss: 0.3400 - accuracy: 0.8494 - precision: 0.8183 - recall: 0.9100\n",
            "Epoch 37/50\n",
            "91/91 [==============================] - 0s 3ms/step - loss: 0.3418 - accuracy: 0.8474 - precision: 0.8185 - recall: 0.9024\n",
            "Epoch 38/50\n",
            "91/91 [==============================] - 0s 2ms/step - loss: 0.3372 - accuracy: 0.8503 - precision: 0.8208 - recall: 0.9065\n",
            "Epoch 39/50\n",
            "91/91 [==============================] - 0s 2ms/step - loss: 0.3371 - accuracy: 0.8545 - precision: 0.8240 - recall: 0.9109\n",
            "Epoch 40/50\n",
            "91/91 [==============================] - 0s 2ms/step - loss: 0.3397 - accuracy: 0.8525 - precision: 0.8195 - recall: 0.9131\n",
            "Epoch 41/50\n",
            "91/91 [==============================] - 0s 3ms/step - loss: 0.3346 - accuracy: 0.8565 - precision: 0.8247 - recall: 0.9147\n",
            "Epoch 42/50\n",
            "91/91 [==============================] - 0s 3ms/step - loss: 0.3305 - accuracy: 0.8541 - precision: 0.8230 - recall: 0.9126\n",
            "Epoch 43/50\n",
            "91/91 [==============================] - 0s 2ms/step - loss: 0.3329 - accuracy: 0.8596 - precision: 0.8325 - recall: 0.9102\n",
            "Epoch 44/50\n",
            "91/91 [==============================] - 0s 3ms/step - loss: 0.3256 - accuracy: 0.8589 - precision: 0.8299 - recall: 0.9133\n",
            "Epoch 45/50\n",
            "91/91 [==============================] - 0s 2ms/step - loss: 0.3315 - accuracy: 0.8613 - precision: 0.8328 - recall: 0.9132\n",
            "Epoch 46/50\n",
            "91/91 [==============================] - 0s 3ms/step - loss: 0.3314 - accuracy: 0.8582 - precision: 0.8278 - recall: 0.9144\n",
            "Epoch 47/50\n",
            "91/91 [==============================] - 0s 2ms/step - loss: 0.3301 - accuracy: 0.8569 - precision: 0.8267 - recall: 0.9109\n",
            "Epoch 48/50\n",
            "91/91 [==============================] - 0s 3ms/step - loss: 0.3177 - accuracy: 0.8629 - precision: 0.8356 - recall: 0.9155\n",
            "Epoch 49/50\n",
            "91/91 [==============================] - 0s 3ms/step - loss: 0.3180 - accuracy: 0.8591 - precision: 0.8227 - recall: 0.9223\n",
            "Epoch 50/50\n",
            "91/91 [==============================] - 0s 3ms/step - loss: 0.3269 - accuracy: 0.8556 - precision: 0.8269 - recall: 0.9097\n",
            "46/46 [==============================] - 0s 1ms/step - loss: 0.3586 - accuracy: 0.8484 - precision: 0.8007 - recall: 0.9320\n",
            "[CV 1/3] END ..........batch_size=50, epochs=50;, score=0.848 total time=  22.6s\n",
            "Model: \"sequential_6\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " dense_36 (Dense)            (None, 32)                352       \n",
            "                                                                 \n",
            " batch_normalization_10 (Bat  (None, 32)               128       \n",
            " chNormalization)                                                \n",
            "                                                                 \n",
            " dense_37 (Dense)            (None, 16)                528       \n",
            "                                                                 \n",
            " batch_normalization_11 (Bat  (None, 16)               64        \n",
            " chNormalization)                                                \n",
            "                                                                 \n",
            " dense_38 (Dense)            (None, 8)                 136       \n",
            "                                                                 \n",
            " batch_normalization_12 (Bat  (None, 8)                32        \n",
            " chNormalization)                                                \n",
            "                                                                 \n",
            " dense_39 (Dense)            (None, 4)                 36        \n",
            "                                                                 \n",
            " batch_normalization_13 (Bat  (None, 4)                16        \n",
            " chNormalization)                                                \n",
            "                                                                 \n",
            " dense_40 (Dense)            (None, 2)                 10        \n",
            "                                                                 \n",
            " batch_normalization_14 (Bat  (None, 2)                8         \n",
            " chNormalization)                                                \n",
            "                                                                 \n",
            " dense_41 (Dense)            (None, 1)                 3         \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 1,313\n",
            "Trainable params: 1,189\n",
            "Non-trainable params: 124\n",
            "_________________________________________________________________\n",
            "None\n",
            "Epoch 1/50\n",
            "91/91 [==============================] - 2s 2ms/step - loss: 0.6635 - accuracy: 0.6039 - precision: 0.6664 - recall: 0.4104\n",
            "Epoch 2/50\n",
            "91/91 [==============================] - 0s 2ms/step - loss: 0.5465 - accuracy: 0.7035 - precision: 0.7539 - recall: 0.6099\n",
            "Epoch 3/50\n",
            "91/91 [==============================] - 0s 2ms/step - loss: 0.4981 - accuracy: 0.7496 - precision: 0.7276 - recall: 0.8134\n",
            "Epoch 4/50\n",
            "91/91 [==============================] - 0s 2ms/step - loss: 0.4800 - accuracy: 0.7683 - precision: 0.7332 - recall: 0.8492\n",
            "Epoch 5/50\n",
            "91/91 [==============================] - 0s 3ms/step - loss: 0.4650 - accuracy: 0.7741 - precision: 0.7314 - recall: 0.8698\n",
            "Epoch 6/50\n",
            "91/91 [==============================] - 0s 2ms/step - loss: 0.4542 - accuracy: 0.7723 - precision: 0.7360 - recall: 0.8595\n",
            "Epoch 7/50\n",
            "91/91 [==============================] - 0s 3ms/step - loss: 0.4388 - accuracy: 0.7844 - precision: 0.7441 - recall: 0.8727\n",
            "Epoch 8/50\n",
            "91/91 [==============================] - 0s 2ms/step - loss: 0.4359 - accuracy: 0.7862 - precision: 0.7464 - recall: 0.8720\n",
            "Epoch 9/50\n",
            "91/91 [==============================] - 0s 3ms/step - loss: 0.4259 - accuracy: 0.7862 - precision: 0.7419 - recall: 0.8819\n",
            "Epoch 10/50\n",
            "91/91 [==============================] - 0s 3ms/step - loss: 0.4213 - accuracy: 0.8010 - precision: 0.7592 - recall: 0.8853\n",
            "Epoch 11/50\n",
            "91/91 [==============================] - 0s 3ms/step - loss: 0.4154 - accuracy: 0.7946 - precision: 0.7624 - recall: 0.8594\n",
            "Epoch 12/50\n",
            "91/91 [==============================] - 0s 3ms/step - loss: 0.4010 - accuracy: 0.8058 - precision: 0.7787 - recall: 0.8573\n",
            "Epoch 13/50\n",
            "91/91 [==============================] - 0s 2ms/step - loss: 0.4020 - accuracy: 0.8001 - precision: 0.7744 - recall: 0.8572\n",
            "Epoch 14/50\n",
            "91/91 [==============================] - 0s 3ms/step - loss: 0.4046 - accuracy: 0.8007 - precision: 0.7737 - recall: 0.8562\n",
            "Epoch 15/50\n",
            "91/91 [==============================] - 0s 2ms/step - loss: 0.4014 - accuracy: 0.8010 - precision: 0.7731 - recall: 0.8632\n",
            "Epoch 16/50\n",
            "91/91 [==============================] - 0s 3ms/step - loss: 0.3983 - accuracy: 0.8129 - precision: 0.7841 - recall: 0.8722\n",
            "Epoch 17/50\n",
            "91/91 [==============================] - 0s 3ms/step - loss: 0.3834 - accuracy: 0.8124 - precision: 0.7863 - recall: 0.8619\n",
            "Epoch 18/50\n",
            "91/91 [==============================] - 0s 3ms/step - loss: 0.3871 - accuracy: 0.8164 - precision: 0.7907 - recall: 0.8690\n",
            "Epoch 19/50\n",
            "91/91 [==============================] - 0s 3ms/step - loss: 0.3862 - accuracy: 0.8197 - precision: 0.7955 - recall: 0.8696\n",
            "Epoch 20/50\n",
            "91/91 [==============================] - 0s 3ms/step - loss: 0.3790 - accuracy: 0.8173 - precision: 0.7884 - recall: 0.8725\n",
            "Epoch 21/50\n",
            "91/91 [==============================] - 0s 2ms/step - loss: 0.3734 - accuracy: 0.8208 - precision: 0.7898 - recall: 0.8833\n",
            "Epoch 22/50\n",
            "91/91 [==============================] - 0s 3ms/step - loss: 0.3832 - accuracy: 0.8149 - precision: 0.7887 - recall: 0.8659\n",
            "Epoch 23/50\n",
            "91/91 [==============================] - 0s 3ms/step - loss: 0.3716 - accuracy: 0.8219 - precision: 0.7909 - recall: 0.8774\n",
            "Epoch 24/50\n",
            "91/91 [==============================] - 0s 3ms/step - loss: 0.3822 - accuracy: 0.8135 - precision: 0.7877 - recall: 0.8685\n",
            "Epoch 25/50\n",
            "91/91 [==============================] - 0s 2ms/step - loss: 0.3686 - accuracy: 0.8226 - precision: 0.7927 - recall: 0.8831\n",
            "Epoch 26/50\n",
            "91/91 [==============================] - 0s 2ms/step - loss: 0.3681 - accuracy: 0.8261 - precision: 0.7987 - recall: 0.8781\n",
            "Epoch 27/50\n",
            "91/91 [==============================] - 0s 3ms/step - loss: 0.3647 - accuracy: 0.8305 - precision: 0.8023 - recall: 0.8811\n",
            "Epoch 28/50\n",
            "91/91 [==============================] - 0s 2ms/step - loss: 0.3617 - accuracy: 0.8294 - precision: 0.8044 - recall: 0.8777\n",
            "Epoch 29/50\n",
            "91/91 [==============================] - 0s 3ms/step - loss: 0.3588 - accuracy: 0.8323 - precision: 0.8041 - recall: 0.8842\n",
            "Epoch 30/50\n",
            "91/91 [==============================] - 0s 2ms/step - loss: 0.3589 - accuracy: 0.8327 - precision: 0.8050 - recall: 0.8822\n",
            "Epoch 31/50\n",
            "91/91 [==============================] - 0s 3ms/step - loss: 0.3628 - accuracy: 0.8354 - precision: 0.8073 - recall: 0.8843\n",
            "Epoch 32/50\n",
            "91/91 [==============================] - 0s 3ms/step - loss: 0.3618 - accuracy: 0.8329 - precision: 0.8017 - recall: 0.8906\n",
            "Epoch 33/50\n",
            "91/91 [==============================] - 0s 2ms/step - loss: 0.3556 - accuracy: 0.8296 - precision: 0.7992 - recall: 0.8857\n",
            "Epoch 34/50\n",
            "91/91 [==============================] - 0s 2ms/step - loss: 0.3591 - accuracy: 0.8296 - precision: 0.8013 - recall: 0.8824\n",
            "Epoch 35/50\n",
            "91/91 [==============================] - 0s 3ms/step - loss: 0.3600 - accuracy: 0.8331 - precision: 0.8003 - recall: 0.8929\n",
            "Epoch 36/50\n",
            "91/91 [==============================] - 0s 2ms/step - loss: 0.3528 - accuracy: 0.8325 - precision: 0.8037 - recall: 0.8857\n",
            "Epoch 37/50\n",
            "91/91 [==============================] - 0s 3ms/step - loss: 0.3602 - accuracy: 0.8318 - precision: 0.8015 - recall: 0.8907\n",
            "Epoch 38/50\n",
            "91/91 [==============================] - 0s 3ms/step - loss: 0.3515 - accuracy: 0.8336 - precision: 0.8035 - recall: 0.8904\n",
            "Epoch 39/50\n",
            "91/91 [==============================] - 0s 2ms/step - loss: 0.3432 - accuracy: 0.8404 - precision: 0.8088 - recall: 0.8958\n",
            "Epoch 40/50\n",
            "91/91 [==============================] - 0s 3ms/step - loss: 0.3484 - accuracy: 0.8349 - precision: 0.8004 - recall: 0.8959\n",
            "Epoch 41/50\n",
            "91/91 [==============================] - 0s 2ms/step - loss: 0.3463 - accuracy: 0.8349 - precision: 0.8061 - recall: 0.8880\n",
            "Epoch 42/50\n",
            "91/91 [==============================] - 0s 2ms/step - loss: 0.3449 - accuracy: 0.8384 - precision: 0.8082 - recall: 0.8948\n",
            "Epoch 43/50\n",
            "91/91 [==============================] - 0s 2ms/step - loss: 0.3488 - accuracy: 0.8358 - precision: 0.8085 - recall: 0.8849\n",
            "Epoch 44/50\n",
            "91/91 [==============================] - 0s 3ms/step - loss: 0.3383 - accuracy: 0.8422 - precision: 0.8134 - recall: 0.8949\n",
            "Epoch 45/50\n",
            "91/91 [==============================] - 0s 2ms/step - loss: 0.3389 - accuracy: 0.8439 - precision: 0.8129 - recall: 0.8992\n",
            "Epoch 46/50\n",
            "91/91 [==============================] - 0s 2ms/step - loss: 0.3389 - accuracy: 0.8446 - precision: 0.8131 - recall: 0.8983\n",
            "Epoch 47/50\n",
            "91/91 [==============================] - 0s 2ms/step - loss: 0.3387 - accuracy: 0.8479 - precision: 0.8235 - recall: 0.8915\n",
            "Epoch 48/50\n",
            "91/91 [==============================] - 0s 3ms/step - loss: 0.3382 - accuracy: 0.8437 - precision: 0.8110 - recall: 0.8989\n",
            "Epoch 49/50\n",
            "91/91 [==============================] - 0s 3ms/step - loss: 0.3403 - accuracy: 0.8393 - precision: 0.8153 - recall: 0.8825\n",
            "Epoch 50/50\n",
            "91/91 [==============================] - 0s 3ms/step - loss: 0.3394 - accuracy: 0.8470 - precision: 0.8145 - recall: 0.9056\n",
            "46/46 [==============================] - 0s 1ms/step - loss: 0.3468 - accuracy: 0.8545 - precision: 0.8013 - recall: 0.9498\n",
            "[CV 2/3] END ..........batch_size=50, epochs=50;, score=0.854 total time=  13.9s\n",
            "Model: \"sequential_7\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " dense_42 (Dense)            (None, 32)                352       \n",
            "                                                                 \n",
            " batch_normalization_15 (Bat  (None, 32)               128       \n",
            " chNormalization)                                                \n",
            "                                                                 \n",
            " dense_43 (Dense)            (None, 16)                528       \n",
            "                                                                 \n",
            " batch_normalization_16 (Bat  (None, 16)               64        \n",
            " chNormalization)                                                \n",
            "                                                                 \n",
            " dense_44 (Dense)            (None, 8)                 136       \n",
            "                                                                 \n",
            " batch_normalization_17 (Bat  (None, 8)                32        \n",
            " chNormalization)                                                \n",
            "                                                                 \n",
            " dense_45 (Dense)            (None, 4)                 36        \n",
            "                                                                 \n",
            " batch_normalization_18 (Bat  (None, 4)                16        \n",
            " chNormalization)                                                \n",
            "                                                                 \n",
            " dense_46 (Dense)            (None, 2)                 10        \n",
            "                                                                 \n",
            " batch_normalization_19 (Bat  (None, 2)                8         \n",
            " chNormalization)                                                \n",
            "                                                                 \n",
            " dense_47 (Dense)            (None, 1)                 3         \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 1,313\n",
            "Trainable params: 1,189\n",
            "Non-trainable params: 124\n",
            "_________________________________________________________________\n",
            "None\n",
            "Epoch 1/50\n",
            "91/91 [==============================] - 2s 2ms/step - loss: 0.7193 - accuracy: 0.5854 - precision: 0.6027 - recall: 0.5382\n",
            "Epoch 2/50\n",
            "91/91 [==============================] - 0s 2ms/step - loss: 0.6207 - accuracy: 0.6665 - precision: 0.6748 - recall: 0.6637\n",
            "Epoch 3/50\n",
            "91/91 [==============================] - 0s 2ms/step - loss: 0.5742 - accuracy: 0.7245 - precision: 0.7289 - recall: 0.7295\n",
            "Epoch 4/50\n",
            "91/91 [==============================] - 0s 3ms/step - loss: 0.5626 - accuracy: 0.7240 - precision: 0.7299 - recall: 0.7237\n",
            "Epoch 5/50\n",
            "91/91 [==============================] - 0s 2ms/step - loss: 0.5345 - accuracy: 0.7584 - precision: 0.7625 - recall: 0.7641\n",
            "Epoch 6/50\n",
            "91/91 [==============================] - 0s 3ms/step - loss: 0.5312 - accuracy: 0.7584 - precision: 0.7566 - recall: 0.7703\n",
            "Epoch 7/50\n",
            "91/91 [==============================] - 0s 3ms/step - loss: 0.5168 - accuracy: 0.7624 - precision: 0.7570 - recall: 0.7810\n",
            "Epoch 8/50\n",
            "91/91 [==============================] - 0s 3ms/step - loss: 0.5075 - accuracy: 0.7699 - precision: 0.7717 - recall: 0.7785\n",
            "Epoch 9/50\n",
            "91/91 [==============================] - 0s 3ms/step - loss: 0.4890 - accuracy: 0.7838 - precision: 0.7775 - recall: 0.8033\n",
            "Epoch 10/50\n",
            "91/91 [==============================] - 0s 2ms/step - loss: 0.4845 - accuracy: 0.7866 - precision: 0.7829 - recall: 0.8019\n",
            "Epoch 11/50\n",
            "91/91 [==============================] - 0s 3ms/step - loss: 0.4782 - accuracy: 0.7866 - precision: 0.7808 - recall: 0.8118\n",
            "Epoch 12/50\n",
            "91/91 [==============================] - 0s 2ms/step - loss: 0.4595 - accuracy: 0.8021 - precision: 0.7891 - recall: 0.8353\n",
            "Epoch 13/50\n",
            "91/91 [==============================] - 0s 2ms/step - loss: 0.4582 - accuracy: 0.7968 - precision: 0.7859 - recall: 0.8255\n",
            "Epoch 14/50\n",
            "91/91 [==============================] - 0s 3ms/step - loss: 0.4470 - accuracy: 0.8047 - precision: 0.7891 - recall: 0.8438\n",
            "Epoch 15/50\n",
            "91/91 [==============================] - 0s 3ms/step - loss: 0.4485 - accuracy: 0.8036 - precision: 0.7884 - recall: 0.8391\n",
            "Epoch 16/50\n",
            "91/91 [==============================] - 0s 3ms/step - loss: 0.4360 - accuracy: 0.8100 - precision: 0.7882 - recall: 0.8574\n",
            "Epoch 17/50\n",
            "91/91 [==============================] - 0s 3ms/step - loss: 0.4296 - accuracy: 0.8133 - precision: 0.7896 - recall: 0.8648\n",
            "Epoch 18/50\n",
            "91/91 [==============================] - 0s 3ms/step - loss: 0.4279 - accuracy: 0.8135 - precision: 0.7936 - recall: 0.8626\n",
            "Epoch 19/50\n",
            "91/91 [==============================] - 0s 3ms/step - loss: 0.4240 - accuracy: 0.8129 - precision: 0.7918 - recall: 0.8617\n",
            "Epoch 20/50\n",
            "91/91 [==============================] - 0s 2ms/step - loss: 0.4161 - accuracy: 0.8144 - precision: 0.7885 - recall: 0.8687\n",
            "Epoch 21/50\n",
            "91/91 [==============================] - 0s 2ms/step - loss: 0.4148 - accuracy: 0.8153 - precision: 0.7911 - recall: 0.8687\n",
            "Epoch 22/50\n",
            "91/91 [==============================] - 0s 3ms/step - loss: 0.4059 - accuracy: 0.8188 - precision: 0.7955 - recall: 0.8684\n",
            "Epoch 23/50\n",
            "91/91 [==============================] - 0s 3ms/step - loss: 0.4028 - accuracy: 0.8237 - precision: 0.7971 - recall: 0.8757\n",
            "Epoch 24/50\n",
            "91/91 [==============================] - 0s 3ms/step - loss: 0.3951 - accuracy: 0.8270 - precision: 0.8023 - recall: 0.8769\n",
            "Epoch 25/50\n",
            "91/91 [==============================] - 0s 2ms/step - loss: 0.4083 - accuracy: 0.8166 - precision: 0.7911 - recall: 0.8718\n",
            "Epoch 26/50\n",
            "91/91 [==============================] - 0s 3ms/step - loss: 0.3993 - accuracy: 0.8250 - precision: 0.8005 - recall: 0.8738\n",
            "Epoch 27/50\n",
            "91/91 [==============================] - 0s 3ms/step - loss: 0.3969 - accuracy: 0.8265 - precision: 0.7978 - recall: 0.8862\n",
            "Epoch 28/50\n",
            "91/91 [==============================] - 0s 3ms/step - loss: 0.3942 - accuracy: 0.8228 - precision: 0.7968 - recall: 0.8795\n",
            "Epoch 29/50\n",
            "91/91 [==============================] - 0s 3ms/step - loss: 0.3972 - accuracy: 0.8261 - precision: 0.8001 - recall: 0.8831\n",
            "Epoch 30/50\n",
            "91/91 [==============================] - 0s 2ms/step - loss: 0.4015 - accuracy: 0.8186 - precision: 0.7879 - recall: 0.8817\n",
            "Epoch 31/50\n",
            "91/91 [==============================] - 0s 2ms/step - loss: 0.3872 - accuracy: 0.8316 - precision: 0.8035 - recall: 0.8891\n",
            "Epoch 32/50\n",
            "91/91 [==============================] - 0s 3ms/step - loss: 0.3911 - accuracy: 0.8279 - precision: 0.8019 - recall: 0.8811\n",
            "Epoch 33/50\n",
            "91/91 [==============================] - 0s 3ms/step - loss: 0.3933 - accuracy: 0.8210 - precision: 0.7960 - recall: 0.8756\n",
            "Epoch 34/50\n",
            "91/91 [==============================] - 0s 3ms/step - loss: 0.3919 - accuracy: 0.8221 - precision: 0.7976 - recall: 0.8773\n",
            "Epoch 35/50\n",
            "91/91 [==============================] - 0s 2ms/step - loss: 0.3789 - accuracy: 0.8360 - precision: 0.8091 - recall: 0.8874\n",
            "Epoch 36/50\n",
            "91/91 [==============================] - 0s 3ms/step - loss: 0.3771 - accuracy: 0.8336 - precision: 0.8054 - recall: 0.8906\n",
            "Epoch 37/50\n",
            "91/91 [==============================] - 0s 2ms/step - loss: 0.3764 - accuracy: 0.8365 - precision: 0.8055 - recall: 0.8986\n",
            "Epoch 38/50\n",
            "91/91 [==============================] - 0s 2ms/step - loss: 0.3776 - accuracy: 0.8329 - precision: 0.8001 - recall: 0.8998\n",
            "Epoch 39/50\n",
            "91/91 [==============================] - 0s 2ms/step - loss: 0.3862 - accuracy: 0.8281 - precision: 0.8015 - recall: 0.8848\n",
            "Epoch 40/50\n",
            "91/91 [==============================] - 0s 3ms/step - loss: 0.3708 - accuracy: 0.8354 - precision: 0.8070 - recall: 0.8911\n",
            "Epoch 41/50\n",
            "91/91 [==============================] - 0s 3ms/step - loss: 0.3667 - accuracy: 0.8424 - precision: 0.8132 - recall: 0.9006\n",
            "Epoch 42/50\n",
            "91/91 [==============================] - 0s 3ms/step - loss: 0.3747 - accuracy: 0.8325 - precision: 0.8069 - recall: 0.8866\n",
            "Epoch 43/50\n",
            "91/91 [==============================] - 0s 3ms/step - loss: 0.3709 - accuracy: 0.8369 - precision: 0.8062 - recall: 0.8973\n",
            "Epoch 44/50\n",
            "91/91 [==============================] - 0s 3ms/step - loss: 0.3618 - accuracy: 0.8455 - precision: 0.8174 - recall: 0.9024\n",
            "Epoch 45/50\n",
            "91/91 [==============================] - 0s 3ms/step - loss: 0.3700 - accuracy: 0.8389 - precision: 0.8097 - recall: 0.8930\n",
            "Epoch 46/50\n",
            "91/91 [==============================] - 0s 3ms/step - loss: 0.3703 - accuracy: 0.8406 - precision: 0.8101 - recall: 0.8997\n",
            "Epoch 47/50\n",
            "91/91 [==============================] - 0s 3ms/step - loss: 0.3607 - accuracy: 0.8411 - precision: 0.8105 - recall: 0.8996\n",
            "Epoch 48/50\n",
            "91/91 [==============================] - 0s 3ms/step - loss: 0.3588 - accuracy: 0.8468 - precision: 0.8192 - recall: 0.9000\n",
            "Epoch 49/50\n",
            "91/91 [==============================] - 0s 2ms/step - loss: 0.3613 - accuracy: 0.8455 - precision: 0.8194 - recall: 0.8973\n",
            "Epoch 50/50\n",
            "91/91 [==============================] - 0s 2ms/step - loss: 0.3602 - accuracy: 0.8446 - precision: 0.8184 - recall: 0.8982\n",
            "46/46 [==============================] - 0s 1ms/step - loss: 0.3624 - accuracy: 0.8496 - precision: 0.7903 - recall: 0.9494\n",
            "[CV 3/3] END ..........batch_size=50, epochs=50;, score=0.850 total time=  14.1s\n",
            "Model: \"sequential_8\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " dense_48 (Dense)            (None, 32)                352       \n",
            "                                                                 \n",
            " batch_normalization_20 (Bat  (None, 32)               128       \n",
            " chNormalization)                                                \n",
            "                                                                 \n",
            " dense_49 (Dense)            (None, 16)                528       \n",
            "                                                                 \n",
            " batch_normalization_21 (Bat  (None, 16)               64        \n",
            " chNormalization)                                                \n",
            "                                                                 \n",
            " dense_50 (Dense)            (None, 8)                 136       \n",
            "                                                                 \n",
            " batch_normalization_22 (Bat  (None, 8)                32        \n",
            " chNormalization)                                                \n",
            "                                                                 \n",
            " dense_51 (Dense)            (None, 4)                 36        \n",
            "                                                                 \n",
            " batch_normalization_23 (Bat  (None, 4)                16        \n",
            " chNormalization)                                                \n",
            "                                                                 \n",
            " dense_52 (Dense)            (None, 2)                 10        \n",
            "                                                                 \n",
            " batch_normalization_24 (Bat  (None, 2)                8         \n",
            " chNormalization)                                                \n",
            "                                                                 \n",
            " dense_53 (Dense)            (None, 1)                 3         \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 1,313\n",
            "Trainable params: 1,189\n",
            "Non-trainable params: 124\n",
            "_________________________________________________________________\n",
            "None\n",
            "Epoch 1/100\n",
            "91/91 [==============================] - 2s 2ms/step - loss: 0.6755 - accuracy: 0.6290 - precision: 0.6379 - recall: 0.6149\n",
            "Epoch 2/100\n",
            "91/91 [==============================] - 0s 3ms/step - loss: 0.5477 - accuracy: 0.7057 - precision: 0.7126 - recall: 0.7087\n",
            "Epoch 3/100\n",
            "91/91 [==============================] - 0s 3ms/step - loss: 0.5016 - accuracy: 0.7623 - precision: 0.7499 - recall: 0.8038\n",
            "Epoch 4/100\n",
            "91/91 [==============================] - 0s 3ms/step - loss: 0.4825 - accuracy: 0.7725 - precision: 0.7517 - recall: 0.8280\n",
            "Epoch 5/100\n",
            "91/91 [==============================] - 0s 2ms/step - loss: 0.4667 - accuracy: 0.7886 - precision: 0.7634 - recall: 0.8476\n",
            "Epoch 6/100\n",
            "91/91 [==============================] - 0s 3ms/step - loss: 0.4673 - accuracy: 0.7870 - precision: 0.7625 - recall: 0.8499\n",
            "Epoch 7/100\n",
            "91/91 [==============================] - 0s 3ms/step - loss: 0.4499 - accuracy: 0.7932 - precision: 0.7691 - recall: 0.8494\n",
            "Epoch 8/100\n",
            "91/91 [==============================] - 0s 3ms/step - loss: 0.4490 - accuracy: 0.7934 - precision: 0.7690 - recall: 0.8496\n",
            "Epoch 9/100\n",
            "91/91 [==============================] - 0s 2ms/step - loss: 0.4434 - accuracy: 0.7930 - precision: 0.7681 - recall: 0.8558\n",
            "Epoch 10/100\n",
            "91/91 [==============================] - 0s 3ms/step - loss: 0.4317 - accuracy: 0.8022 - precision: 0.7791 - recall: 0.8563\n",
            "Epoch 11/100\n",
            "91/91 [==============================] - 0s 2ms/step - loss: 0.4317 - accuracy: 0.8045 - precision: 0.7742 - recall: 0.8707\n",
            "Epoch 12/100\n",
            "91/91 [==============================] - 0s 2ms/step - loss: 0.4222 - accuracy: 0.8051 - precision: 0.7750 - recall: 0.8688\n",
            "Epoch 13/100\n",
            "91/91 [==============================] - 0s 3ms/step - loss: 0.4192 - accuracy: 0.8051 - precision: 0.7768 - recall: 0.8670\n",
            "Epoch 14/100\n",
            "91/91 [==============================] - 0s 3ms/step - loss: 0.4162 - accuracy: 0.8089 - precision: 0.7769 - recall: 0.8801\n",
            "Epoch 15/100\n",
            "91/91 [==============================] - 0s 2ms/step - loss: 0.4188 - accuracy: 0.8064 - precision: 0.7764 - recall: 0.8765\n",
            "Epoch 16/100\n",
            "91/91 [==============================] - 0s 3ms/step - loss: 0.4061 - accuracy: 0.8153 - precision: 0.7789 - recall: 0.8918\n",
            "Epoch 17/100\n",
            "91/91 [==============================] - 0s 3ms/step - loss: 0.4085 - accuracy: 0.8183 - precision: 0.7876 - recall: 0.8869\n",
            "Epoch 18/100\n",
            "91/91 [==============================] - 0s 2ms/step - loss: 0.4087 - accuracy: 0.8150 - precision: 0.7887 - recall: 0.8716\n",
            "Epoch 19/100\n",
            "91/91 [==============================] - 0s 2ms/step - loss: 0.3945 - accuracy: 0.8201 - precision: 0.7909 - recall: 0.8810\n",
            "Epoch 20/100\n",
            "91/91 [==============================] - 0s 3ms/step - loss: 0.3963 - accuracy: 0.8155 - precision: 0.7851 - recall: 0.8784\n",
            "Epoch 21/100\n",
            "91/91 [==============================] - 0s 3ms/step - loss: 0.3877 - accuracy: 0.8258 - precision: 0.7960 - recall: 0.8906\n",
            "Epoch 22/100\n",
            "91/91 [==============================] - 0s 3ms/step - loss: 0.3868 - accuracy: 0.8285 - precision: 0.7958 - recall: 0.8935\n",
            "Epoch 23/100\n",
            "91/91 [==============================] - 0s 3ms/step - loss: 0.3798 - accuracy: 0.8316 - precision: 0.7942 - recall: 0.9035\n",
            "Epoch 24/100\n",
            "91/91 [==============================] - 0s 2ms/step - loss: 0.3717 - accuracy: 0.8358 - precision: 0.8053 - recall: 0.8970\n",
            "Epoch 25/100\n",
            "91/91 [==============================] - 0s 3ms/step - loss: 0.3846 - accuracy: 0.8212 - precision: 0.7925 - recall: 0.8815\n",
            "Epoch 26/100\n",
            "91/91 [==============================] - 0s 3ms/step - loss: 0.3825 - accuracy: 0.8307 - precision: 0.7971 - recall: 0.8982\n",
            "Epoch 27/100\n",
            "91/91 [==============================] - 0s 3ms/step - loss: 0.3828 - accuracy: 0.8256 - precision: 0.7957 - recall: 0.8890\n",
            "Epoch 28/100\n",
            "91/91 [==============================] - 0s 3ms/step - loss: 0.3779 - accuracy: 0.8272 - precision: 0.7922 - recall: 0.8981\n",
            "Epoch 29/100\n",
            "91/91 [==============================] - 0s 3ms/step - loss: 0.3742 - accuracy: 0.8298 - precision: 0.7893 - recall: 0.9096\n",
            "Epoch 30/100\n",
            "91/91 [==============================] - 0s 3ms/step - loss: 0.3751 - accuracy: 0.8276 - precision: 0.7937 - recall: 0.8980\n",
            "Epoch 31/100\n",
            "91/91 [==============================] - 0s 3ms/step - loss: 0.3734 - accuracy: 0.8333 - precision: 0.7969 - recall: 0.9064\n",
            "Epoch 32/100\n",
            "91/91 [==============================] - 0s 3ms/step - loss: 0.3685 - accuracy: 0.8313 - precision: 0.7972 - recall: 0.8994\n",
            "Epoch 33/100\n",
            "91/91 [==============================] - 0s 3ms/step - loss: 0.3702 - accuracy: 0.8342 - precision: 0.7950 - recall: 0.9115\n",
            "Epoch 34/100\n",
            "91/91 [==============================] - 0s 3ms/step - loss: 0.3594 - accuracy: 0.8331 - precision: 0.7978 - recall: 0.9023\n",
            "Epoch 35/100\n",
            "91/91 [==============================] - 0s 3ms/step - loss: 0.3644 - accuracy: 0.8353 - precision: 0.7995 - recall: 0.9065\n",
            "Epoch 36/100\n",
            "91/91 [==============================] - 0s 3ms/step - loss: 0.3643 - accuracy: 0.8373 - precision: 0.8066 - recall: 0.9006\n",
            "Epoch 37/100\n",
            "91/91 [==============================] - 0s 3ms/step - loss: 0.3567 - accuracy: 0.8410 - precision: 0.8072 - recall: 0.9045\n",
            "Epoch 38/100\n",
            "91/91 [==============================] - 0s 3ms/step - loss: 0.3572 - accuracy: 0.8355 - precision: 0.8038 - recall: 0.9020\n",
            "Epoch 39/100\n",
            "91/91 [==============================] - 0s 3ms/step - loss: 0.3627 - accuracy: 0.8388 - precision: 0.8039 - recall: 0.9093\n",
            "Epoch 40/100\n",
            "91/91 [==============================] - 0s 3ms/step - loss: 0.3591 - accuracy: 0.8397 - precision: 0.8057 - recall: 0.9078\n",
            "Epoch 41/100\n",
            "91/91 [==============================] - 0s 2ms/step - loss: 0.3466 - accuracy: 0.8446 - precision: 0.8091 - recall: 0.9082\n",
            "Epoch 42/100\n",
            "91/91 [==============================] - 0s 3ms/step - loss: 0.3536 - accuracy: 0.8437 - precision: 0.8109 - recall: 0.9089\n",
            "Epoch 43/100\n",
            "91/91 [==============================] - 0s 3ms/step - loss: 0.3522 - accuracy: 0.8433 - precision: 0.8135 - recall: 0.9019\n",
            "Epoch 44/100\n",
            "91/91 [==============================] - 0s 3ms/step - loss: 0.3489 - accuracy: 0.8452 - precision: 0.8115 - recall: 0.9078\n",
            "Epoch 45/100\n",
            "91/91 [==============================] - 0s 3ms/step - loss: 0.3565 - accuracy: 0.8444 - precision: 0.8116 - recall: 0.9063\n",
            "Epoch 46/100\n",
            "91/91 [==============================] - 0s 3ms/step - loss: 0.3535 - accuracy: 0.8388 - precision: 0.8047 - recall: 0.9042\n",
            "Epoch 47/100\n",
            "91/91 [==============================] - 0s 3ms/step - loss: 0.3451 - accuracy: 0.8483 - precision: 0.8169 - recall: 0.9083\n",
            "Epoch 48/100\n",
            "91/91 [==============================] - 0s 3ms/step - loss: 0.3495 - accuracy: 0.8373 - precision: 0.8070 - recall: 0.8987\n",
            "Epoch 49/100\n",
            "91/91 [==============================] - 0s 3ms/step - loss: 0.3454 - accuracy: 0.8435 - precision: 0.8122 - recall: 0.9047\n",
            "Epoch 50/100\n",
            "91/91 [==============================] - 0s 3ms/step - loss: 0.3477 - accuracy: 0.8406 - precision: 0.8100 - recall: 0.8999\n",
            "Epoch 51/100\n",
            "91/91 [==============================] - 0s 3ms/step - loss: 0.3502 - accuracy: 0.8393 - precision: 0.8080 - recall: 0.9029\n",
            "Epoch 52/100\n",
            "91/91 [==============================] - 0s 3ms/step - loss: 0.3350 - accuracy: 0.8530 - precision: 0.8179 - recall: 0.9177\n",
            "Epoch 53/100\n",
            "91/91 [==============================] - 0s 2ms/step - loss: 0.3465 - accuracy: 0.8468 - precision: 0.8169 - recall: 0.9082\n",
            "Epoch 54/100\n",
            "91/91 [==============================] - 0s 3ms/step - loss: 0.3407 - accuracy: 0.8477 - precision: 0.8177 - recall: 0.9049\n",
            "Epoch 55/100\n",
            "91/91 [==============================] - 0s 3ms/step - loss: 0.3353 - accuracy: 0.8543 - precision: 0.8314 - recall: 0.9023\n",
            "Epoch 56/100\n",
            "91/91 [==============================] - 0s 3ms/step - loss: 0.3361 - accuracy: 0.8494 - precision: 0.8166 - recall: 0.9086\n",
            "Epoch 57/100\n",
            "91/91 [==============================] - 0s 3ms/step - loss: 0.3258 - accuracy: 0.8569 - precision: 0.8271 - recall: 0.9088\n",
            "Epoch 58/100\n",
            "91/91 [==============================] - 0s 3ms/step - loss: 0.3412 - accuracy: 0.8496 - precision: 0.8194 - recall: 0.9065\n",
            "Epoch 59/100\n",
            "91/91 [==============================] - 0s 3ms/step - loss: 0.3372 - accuracy: 0.8519 - precision: 0.8200 - recall: 0.9111\n",
            "Epoch 60/100\n",
            "91/91 [==============================] - 0s 3ms/step - loss: 0.3355 - accuracy: 0.8532 - precision: 0.8237 - recall: 0.9084\n",
            "Epoch 61/100\n",
            "91/91 [==============================] - 0s 3ms/step - loss: 0.3271 - accuracy: 0.8582 - precision: 0.8288 - recall: 0.9129\n",
            "Epoch 62/100\n",
            "91/91 [==============================] - 0s 3ms/step - loss: 0.3319 - accuracy: 0.8580 - precision: 0.8294 - recall: 0.9113\n",
            "Epoch 63/100\n",
            "91/91 [==============================] - 0s 3ms/step - loss: 0.3195 - accuracy: 0.8640 - precision: 0.8335 - recall: 0.9172\n",
            "Epoch 64/100\n",
            "91/91 [==============================] - 0s 3ms/step - loss: 0.3249 - accuracy: 0.8593 - precision: 0.8310 - recall: 0.9103\n",
            "Epoch 65/100\n",
            "91/91 [==============================] - 0s 3ms/step - loss: 0.3246 - accuracy: 0.8585 - precision: 0.8255 - recall: 0.9168\n",
            "Epoch 66/100\n",
            "91/91 [==============================] - 0s 3ms/step - loss: 0.3102 - accuracy: 0.8695 - precision: 0.8441 - recall: 0.9135\n",
            "Epoch 67/100\n",
            "91/91 [==============================] - 0s 3ms/step - loss: 0.3208 - accuracy: 0.8602 - precision: 0.8329 - recall: 0.9119\n",
            "Epoch 68/100\n",
            "91/91 [==============================] - 0s 3ms/step - loss: 0.3353 - accuracy: 0.8501 - precision: 0.8186 - recall: 0.9104\n",
            "Epoch 69/100\n",
            "91/91 [==============================] - 0s 3ms/step - loss: 0.3288 - accuracy: 0.8567 - precision: 0.8301 - recall: 0.9086\n",
            "Epoch 70/100\n",
            "91/91 [==============================] - 0s 3ms/step - loss: 0.3273 - accuracy: 0.8591 - precision: 0.8268 - recall: 0.9160\n",
            "Epoch 71/100\n",
            "91/91 [==============================] - 0s 3ms/step - loss: 0.3214 - accuracy: 0.8582 - precision: 0.8310 - recall: 0.9092\n",
            "Epoch 72/100\n",
            "91/91 [==============================] - 0s 3ms/step - loss: 0.3185 - accuracy: 0.8644 - precision: 0.8316 - recall: 0.9225\n",
            "Epoch 73/100\n",
            "91/91 [==============================] - 0s 3ms/step - loss: 0.3219 - accuracy: 0.8622 - precision: 0.8340 - recall: 0.9155\n",
            "Epoch 74/100\n",
            "91/91 [==============================] - 0s 3ms/step - loss: 0.3215 - accuracy: 0.8591 - precision: 0.8297 - recall: 0.9123\n",
            "Epoch 75/100\n",
            "91/91 [==============================] - 0s 3ms/step - loss: 0.3112 - accuracy: 0.8695 - precision: 0.8387 - recall: 0.9247\n",
            "Epoch 76/100\n",
            "91/91 [==============================] - 0s 3ms/step - loss: 0.3252 - accuracy: 0.8589 - precision: 0.8283 - recall: 0.9156\n",
            "Epoch 77/100\n",
            "91/91 [==============================] - 0s 3ms/step - loss: 0.3189 - accuracy: 0.8616 - precision: 0.8301 - recall: 0.9189\n",
            "Epoch 78/100\n",
            "91/91 [==============================] - 0s 3ms/step - loss: 0.3170 - accuracy: 0.8589 - precision: 0.8309 - recall: 0.9112\n",
            "Epoch 79/100\n",
            "91/91 [==============================] - 0s 2ms/step - loss: 0.3281 - accuracy: 0.8611 - precision: 0.8304 - recall: 0.9173\n",
            "Epoch 80/100\n",
            "91/91 [==============================] - 0s 3ms/step - loss: 0.3270 - accuracy: 0.8598 - precision: 0.8314 - recall: 0.9145\n",
            "Epoch 81/100\n",
            "91/91 [==============================] - 0s 3ms/step - loss: 0.3084 - accuracy: 0.8666 - precision: 0.8361 - recall: 0.9210\n",
            "Epoch 82/100\n",
            "91/91 [==============================] - 0s 3ms/step - loss: 0.3162 - accuracy: 0.8638 - precision: 0.8335 - recall: 0.9176\n",
            "Epoch 83/100\n",
            "91/91 [==============================] - 0s 3ms/step - loss: 0.3109 - accuracy: 0.8693 - precision: 0.8414 - recall: 0.9195\n",
            "Epoch 84/100\n",
            "91/91 [==============================] - 0s 3ms/step - loss: 0.3093 - accuracy: 0.8649 - precision: 0.8347 - recall: 0.9200\n",
            "Epoch 85/100\n",
            "91/91 [==============================] - 0s 3ms/step - loss: 0.3128 - accuracy: 0.8644 - precision: 0.8342 - recall: 0.9203\n",
            "Epoch 86/100\n",
            "91/91 [==============================] - 0s 3ms/step - loss: 0.3165 - accuracy: 0.8633 - precision: 0.8314 - recall: 0.9192\n",
            "Epoch 87/100\n",
            "91/91 [==============================] - 0s 3ms/step - loss: 0.3128 - accuracy: 0.8631 - precision: 0.8333 - recall: 0.9202\n",
            "Epoch 88/100\n",
            "91/91 [==============================] - 0s 3ms/step - loss: 0.3088 - accuracy: 0.8679 - precision: 0.8350 - recall: 0.9232\n",
            "Epoch 89/100\n",
            "91/91 [==============================] - 0s 3ms/step - loss: 0.3026 - accuracy: 0.8671 - precision: 0.8392 - recall: 0.9169\n",
            "Epoch 90/100\n",
            "91/91 [==============================] - 0s 3ms/step - loss: 0.3092 - accuracy: 0.8675 - precision: 0.8400 - recall: 0.9165\n",
            "Epoch 91/100\n",
            "91/91 [==============================] - 0s 3ms/step - loss: 0.3010 - accuracy: 0.8715 - precision: 0.8379 - recall: 0.9287\n",
            "Epoch 92/100\n",
            "91/91 [==============================] - 0s 3ms/step - loss: 0.3024 - accuracy: 0.8690 - precision: 0.8420 - recall: 0.9165\n",
            "Epoch 93/100\n",
            "91/91 [==============================] - 0s 3ms/step - loss: 0.2971 - accuracy: 0.8719 - precision: 0.8404 - recall: 0.9259\n",
            "Epoch 94/100\n",
            "91/91 [==============================] - 0s 3ms/step - loss: 0.3109 - accuracy: 0.8732 - precision: 0.8433 - recall: 0.9259\n",
            "Epoch 95/100\n",
            "91/91 [==============================] - 0s 3ms/step - loss: 0.3052 - accuracy: 0.8743 - precision: 0.8406 - recall: 0.9303\n",
            "Epoch 96/100\n",
            "91/91 [==============================] - 0s 3ms/step - loss: 0.3014 - accuracy: 0.8708 - precision: 0.8422 - recall: 0.9205\n",
            "Epoch 97/100\n",
            "91/91 [==============================] - 0s 3ms/step - loss: 0.3023 - accuracy: 0.8715 - precision: 0.8409 - recall: 0.9236\n",
            "Epoch 98/100\n",
            "91/91 [==============================] - 0s 3ms/step - loss: 0.3018 - accuracy: 0.8660 - precision: 0.8354 - recall: 0.9212\n",
            "Epoch 99/100\n",
            "91/91 [==============================] - 0s 3ms/step - loss: 0.2971 - accuracy: 0.8717 - precision: 0.8447 - recall: 0.9183\n",
            "Epoch 100/100\n",
            "91/91 [==============================] - 0s 3ms/step - loss: 0.3044 - accuracy: 0.8721 - precision: 0.8417 - recall: 0.9253\n",
            "46/46 [==============================] - 0s 2ms/step - loss: 0.3298 - accuracy: 0.8643 - precision: 0.8222 - recall: 0.9304\n",
            "[CV 1/3] END .........batch_size=50, epochs=100;, score=0.864 total time=  43.3s\n",
            "Model: \"sequential_9\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " dense_54 (Dense)            (None, 32)                352       \n",
            "                                                                 \n",
            " batch_normalization_25 (Bat  (None, 32)               128       \n",
            " chNormalization)                                                \n",
            "                                                                 \n",
            " dense_55 (Dense)            (None, 16)                528       \n",
            "                                                                 \n",
            " batch_normalization_26 (Bat  (None, 16)               64        \n",
            " chNormalization)                                                \n",
            "                                                                 \n",
            " dense_56 (Dense)            (None, 8)                 136       \n",
            "                                                                 \n",
            " batch_normalization_27 (Bat  (None, 8)                32        \n",
            " chNormalization)                                                \n",
            "                                                                 \n",
            " dense_57 (Dense)            (None, 4)                 36        \n",
            "                                                                 \n",
            " batch_normalization_28 (Bat  (None, 4)                16        \n",
            " chNormalization)                                                \n",
            "                                                                 \n",
            " dense_58 (Dense)            (None, 2)                 10        \n",
            "                                                                 \n",
            " batch_normalization_29 (Bat  (None, 2)                8         \n",
            " chNormalization)                                                \n",
            "                                                                 \n",
            " dense_59 (Dense)            (None, 1)                 3         \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 1,313\n",
            "Trainable params: 1,189\n",
            "Non-trainable params: 124\n",
            "_________________________________________________________________\n",
            "None\n",
            "Epoch 1/100\n",
            "91/91 [==============================] - 2s 3ms/step - loss: 0.6186 - accuracy: 0.6434 - precision: 0.5975 - recall: 0.8787\n",
            "Epoch 2/100\n",
            "91/91 [==============================] - 0s 3ms/step - loss: 0.5581 - accuracy: 0.6826 - precision: 0.6580 - recall: 0.7951\n",
            "Epoch 3/100\n",
            "91/91 [==============================] - 0s 3ms/step - loss: 0.5254 - accuracy: 0.7476 - precision: 0.7297 - recall: 0.7881\n",
            "Epoch 4/100\n",
            "91/91 [==============================] - 0s 3ms/step - loss: 0.5097 - accuracy: 0.7580 - precision: 0.7386 - recall: 0.7989\n",
            "Epoch 5/100\n",
            "91/91 [==============================] - 0s 3ms/step - loss: 0.4832 - accuracy: 0.7626 - precision: 0.7399 - recall: 0.8126\n",
            "Epoch 6/100\n",
            "91/91 [==============================] - 0s 2ms/step - loss: 0.4655 - accuracy: 0.7730 - precision: 0.7496 - recall: 0.8210\n",
            "Epoch 7/100\n",
            "91/91 [==============================] - 0s 3ms/step - loss: 0.4586 - accuracy: 0.7884 - precision: 0.7693 - recall: 0.8258\n",
            "Epoch 8/100\n",
            "91/91 [==============================] - 0s 3ms/step - loss: 0.4511 - accuracy: 0.7851 - precision: 0.7609 - recall: 0.8335\n",
            "Epoch 9/100\n",
            "91/91 [==============================] - 0s 3ms/step - loss: 0.4393 - accuracy: 0.7888 - precision: 0.7688 - recall: 0.8295\n",
            "Epoch 10/100\n",
            "91/91 [==============================] - 0s 3ms/step - loss: 0.4377 - accuracy: 0.7853 - precision: 0.7670 - recall: 0.8232\n",
            "Epoch 11/100\n",
            "91/91 [==============================] - 0s 3ms/step - loss: 0.4329 - accuracy: 0.7884 - precision: 0.7695 - recall: 0.8215\n",
            "Epoch 12/100\n",
            "91/91 [==============================] - 0s 3ms/step - loss: 0.4287 - accuracy: 0.7928 - precision: 0.7727 - recall: 0.8312\n",
            "Epoch 13/100\n",
            "91/91 [==============================] - 0s 3ms/step - loss: 0.4309 - accuracy: 0.7937 - precision: 0.7733 - recall: 0.8346\n",
            "Epoch 14/100\n",
            "91/91 [==============================] - 0s 3ms/step - loss: 0.4244 - accuracy: 0.8021 - precision: 0.7832 - recall: 0.8367\n",
            "Epoch 15/100\n",
            "91/91 [==============================] - 0s 3ms/step - loss: 0.4225 - accuracy: 0.8054 - precision: 0.7862 - recall: 0.8466\n",
            "Epoch 16/100\n",
            "91/91 [==============================] - 0s 3ms/step - loss: 0.4099 - accuracy: 0.8065 - precision: 0.7872 - recall: 0.8431\n",
            "Epoch 17/100\n",
            "91/91 [==============================] - 0s 3ms/step - loss: 0.4069 - accuracy: 0.8115 - precision: 0.7893 - recall: 0.8523\n",
            "Epoch 18/100\n",
            "91/91 [==============================] - 0s 4ms/step - loss: 0.4135 - accuracy: 0.8047 - precision: 0.7897 - recall: 0.8329\n",
            "Epoch 19/100\n",
            "91/91 [==============================] - 0s 4ms/step - loss: 0.3973 - accuracy: 0.8186 - precision: 0.7954 - recall: 0.8575\n",
            "Epoch 20/100\n",
            "91/91 [==============================] - 0s 5ms/step - loss: 0.4121 - accuracy: 0.8065 - precision: 0.7856 - recall: 0.8488\n",
            "Epoch 21/100\n",
            "91/91 [==============================] - 0s 5ms/step - loss: 0.3984 - accuracy: 0.8193 - precision: 0.7962 - recall: 0.8600\n",
            "Epoch 22/100\n",
            "91/91 [==============================] - 0s 5ms/step - loss: 0.3976 - accuracy: 0.8160 - precision: 0.7917 - recall: 0.8586\n",
            "Epoch 23/100\n",
            "91/91 [==============================] - 0s 5ms/step - loss: 0.3938 - accuracy: 0.8184 - precision: 0.7958 - recall: 0.8601\n",
            "Epoch 24/100\n",
            "91/91 [==============================] - 0s 5ms/step - loss: 0.4023 - accuracy: 0.8071 - precision: 0.7823 - recall: 0.8563\n",
            "Epoch 25/100\n",
            "91/91 [==============================] - 0s 5ms/step - loss: 0.3899 - accuracy: 0.8201 - precision: 0.7987 - recall: 0.8609\n",
            "Epoch 26/100\n",
            "91/91 [==============================] - 0s 5ms/step - loss: 0.3915 - accuracy: 0.8223 - precision: 0.8038 - recall: 0.8553\n",
            "Epoch 27/100\n",
            "91/91 [==============================] - 0s 5ms/step - loss: 0.3880 - accuracy: 0.8208 - precision: 0.7982 - recall: 0.8619\n",
            "Epoch 28/100\n",
            "91/91 [==============================] - 0s 5ms/step - loss: 0.3845 - accuracy: 0.8230 - precision: 0.7999 - recall: 0.8638\n",
            "Epoch 29/100\n",
            "91/91 [==============================] - 0s 5ms/step - loss: 0.3739 - accuracy: 0.8365 - precision: 0.8168 - recall: 0.8695\n",
            "Epoch 30/100\n",
            "91/91 [==============================] - 1s 8ms/step - loss: 0.3885 - accuracy: 0.8226 - precision: 0.8014 - recall: 0.8623\n",
            "Epoch 31/100\n",
            "91/91 [==============================] - 0s 3ms/step - loss: 0.3828 - accuracy: 0.8296 - precision: 0.8045 - recall: 0.8734\n",
            "Epoch 32/100\n",
            "91/91 [==============================] - 0s 3ms/step - loss: 0.3881 - accuracy: 0.8184 - precision: 0.7945 - recall: 0.8597\n",
            "Epoch 33/100\n",
            "91/91 [==============================] - 0s 3ms/step - loss: 0.3766 - accuracy: 0.8305 - precision: 0.8052 - recall: 0.8728\n",
            "Epoch 34/100\n",
            "91/91 [==============================] - 0s 3ms/step - loss: 0.3747 - accuracy: 0.8331 - precision: 0.8101 - recall: 0.8747\n",
            "Epoch 35/100\n",
            "91/91 [==============================] - 0s 3ms/step - loss: 0.3741 - accuracy: 0.8314 - precision: 0.8040 - recall: 0.8810\n",
            "Epoch 36/100\n",
            "91/91 [==============================] - 0s 3ms/step - loss: 0.3703 - accuracy: 0.8331 - precision: 0.8136 - recall: 0.8683\n",
            "Epoch 37/100\n",
            "91/91 [==============================] - 0s 3ms/step - loss: 0.3768 - accuracy: 0.8279 - precision: 0.8049 - recall: 0.8715\n",
            "Epoch 38/100\n",
            "91/91 [==============================] - 0s 3ms/step - loss: 0.3685 - accuracy: 0.8318 - precision: 0.8093 - recall: 0.8696\n",
            "Epoch 39/100\n",
            "91/91 [==============================] - 0s 3ms/step - loss: 0.3646 - accuracy: 0.8369 - precision: 0.8153 - recall: 0.8751\n",
            "Epoch 40/100\n",
            "91/91 [==============================] - 0s 3ms/step - loss: 0.3691 - accuracy: 0.8351 - precision: 0.8168 - recall: 0.8654\n",
            "Epoch 41/100\n",
            "91/91 [==============================] - 0s 3ms/step - loss: 0.3716 - accuracy: 0.8301 - precision: 0.8096 - recall: 0.8688\n",
            "Epoch 42/100\n",
            "91/91 [==============================] - 0s 3ms/step - loss: 0.3635 - accuracy: 0.8340 - precision: 0.8074 - recall: 0.8835\n",
            "Epoch 43/100\n",
            "91/91 [==============================] - 0s 3ms/step - loss: 0.3652 - accuracy: 0.8358 - precision: 0.8128 - recall: 0.8781\n",
            "Epoch 44/100\n",
            "91/91 [==============================] - 0s 3ms/step - loss: 0.3618 - accuracy: 0.8371 - precision: 0.8159 - recall: 0.8769\n",
            "Epoch 45/100\n",
            "91/91 [==============================] - 0s 3ms/step - loss: 0.3636 - accuracy: 0.8371 - precision: 0.8146 - recall: 0.8774\n",
            "Epoch 46/100\n",
            "91/91 [==============================] - 0s 3ms/step - loss: 0.3559 - accuracy: 0.8378 - precision: 0.8128 - recall: 0.8826\n",
            "Epoch 47/100\n",
            "91/91 [==============================] - 0s 3ms/step - loss: 0.3484 - accuracy: 0.8453 - precision: 0.8177 - recall: 0.8938\n",
            "Epoch 48/100\n",
            "91/91 [==============================] - 0s 2ms/step - loss: 0.3608 - accuracy: 0.8371 - precision: 0.8107 - recall: 0.8834\n",
            "Epoch 49/100\n",
            "91/91 [==============================] - 0s 3ms/step - loss: 0.3504 - accuracy: 0.8424 - precision: 0.8205 - recall: 0.8828\n",
            "Epoch 50/100\n",
            "91/91 [==============================] - 0s 3ms/step - loss: 0.3568 - accuracy: 0.8320 - precision: 0.8038 - recall: 0.8848\n",
            "Epoch 51/100\n",
            "91/91 [==============================] - 0s 3ms/step - loss: 0.3532 - accuracy: 0.8398 - precision: 0.8188 - recall: 0.8745\n",
            "Epoch 52/100\n",
            "91/91 [==============================] - 0s 3ms/step - loss: 0.3526 - accuracy: 0.8398 - precision: 0.8095 - recall: 0.8931\n",
            "Epoch 53/100\n",
            "91/91 [==============================] - 0s 3ms/step - loss: 0.3497 - accuracy: 0.8369 - precision: 0.8146 - recall: 0.8778\n",
            "Epoch 54/100\n",
            "91/91 [==============================] - 0s 3ms/step - loss: 0.3690 - accuracy: 0.8336 - precision: 0.8068 - recall: 0.8843\n",
            "Epoch 55/100\n",
            "91/91 [==============================] - 0s 3ms/step - loss: 0.3457 - accuracy: 0.8470 - precision: 0.8220 - recall: 0.8907\n",
            "Epoch 56/100\n",
            "91/91 [==============================] - 0s 3ms/step - loss: 0.3522 - accuracy: 0.8424 - precision: 0.8182 - recall: 0.8853\n",
            "Epoch 57/100\n",
            "91/91 [==============================] - 0s 3ms/step - loss: 0.3444 - accuracy: 0.8417 - precision: 0.8124 - recall: 0.8936\n",
            "Epoch 58/100\n",
            "91/91 [==============================] - 0s 3ms/step - loss: 0.3462 - accuracy: 0.8464 - precision: 0.8202 - recall: 0.8896\n",
            "Epoch 59/100\n",
            "91/91 [==============================] - 0s 3ms/step - loss: 0.3453 - accuracy: 0.8439 - precision: 0.8196 - recall: 0.8867\n",
            "Epoch 60/100\n",
            "91/91 [==============================] - 0s 3ms/step - loss: 0.3433 - accuracy: 0.8435 - precision: 0.8155 - recall: 0.8926\n",
            "Epoch 61/100\n",
            "91/91 [==============================] - 0s 3ms/step - loss: 0.3360 - accuracy: 0.8497 - precision: 0.8188 - recall: 0.9009\n",
            "Epoch 62/100\n",
            "91/91 [==============================] - 0s 3ms/step - loss: 0.3360 - accuracy: 0.8521 - precision: 0.8258 - recall: 0.8953\n",
            "Epoch 63/100\n",
            "91/91 [==============================] - 0s 3ms/step - loss: 0.3391 - accuracy: 0.8501 - precision: 0.8257 - recall: 0.8939\n",
            "Epoch 64/100\n",
            "91/91 [==============================] - 0s 3ms/step - loss: 0.3402 - accuracy: 0.8512 - precision: 0.8288 - recall: 0.8924\n",
            "Epoch 65/100\n",
            "91/91 [==============================] - 0s 3ms/step - loss: 0.3270 - accuracy: 0.8530 - precision: 0.8251 - recall: 0.9008\n",
            "Epoch 66/100\n",
            "91/91 [==============================] - 0s 3ms/step - loss: 0.3296 - accuracy: 0.8512 - precision: 0.8258 - recall: 0.8947\n",
            "Epoch 67/100\n",
            "91/91 [==============================] - 0s 3ms/step - loss: 0.3379 - accuracy: 0.8508 - precision: 0.8233 - recall: 0.8985\n",
            "Epoch 68/100\n",
            "91/91 [==============================] - 0s 3ms/step - loss: 0.3342 - accuracy: 0.8492 - precision: 0.8246 - recall: 0.8908\n",
            "Epoch 69/100\n",
            "91/91 [==============================] - 0s 3ms/step - loss: 0.3314 - accuracy: 0.8514 - precision: 0.8262 - recall: 0.8948\n",
            "Epoch 70/100\n",
            "91/91 [==============================] - 0s 3ms/step - loss: 0.3211 - accuracy: 0.8627 - precision: 0.8377 - recall: 0.9022\n",
            "Epoch 71/100\n",
            "91/91 [==============================] - 0s 3ms/step - loss: 0.3312 - accuracy: 0.8442 - precision: 0.8145 - recall: 0.8945\n",
            "Epoch 72/100\n",
            "91/91 [==============================] - 0s 3ms/step - loss: 0.3273 - accuracy: 0.8475 - precision: 0.8165 - recall: 0.9041\n",
            "Epoch 73/100\n",
            "91/91 [==============================] - 0s 3ms/step - loss: 0.3210 - accuracy: 0.8536 - precision: 0.8237 - recall: 0.9034\n",
            "Epoch 74/100\n",
            "91/91 [==============================] - 0s 3ms/step - loss: 0.3255 - accuracy: 0.8541 - precision: 0.8281 - recall: 0.8963\n",
            "Epoch 75/100\n",
            "91/91 [==============================] - 0s 3ms/step - loss: 0.3268 - accuracy: 0.8514 - precision: 0.8231 - recall: 0.9001\n",
            "Epoch 76/100\n",
            "91/91 [==============================] - 0s 3ms/step - loss: 0.3289 - accuracy: 0.8565 - precision: 0.8308 - recall: 0.9017\n",
            "Epoch 77/100\n",
            "91/91 [==============================] - 0s 3ms/step - loss: 0.3210 - accuracy: 0.8603 - precision: 0.8319 - recall: 0.9082\n",
            "Epoch 78/100\n",
            "91/91 [==============================] - 0s 3ms/step - loss: 0.3207 - accuracy: 0.8594 - precision: 0.8331 - recall: 0.9041\n",
            "Epoch 79/100\n",
            "91/91 [==============================] - 0s 3ms/step - loss: 0.3166 - accuracy: 0.8605 - precision: 0.8334 - recall: 0.9047\n",
            "Epoch 80/100\n",
            "91/91 [==============================] - 0s 3ms/step - loss: 0.3134 - accuracy: 0.8609 - precision: 0.8322 - recall: 0.9070\n",
            "Epoch 81/100\n",
            "91/91 [==============================] - 0s 3ms/step - loss: 0.3200 - accuracy: 0.8565 - precision: 0.8349 - recall: 0.8966\n",
            "Epoch 82/100\n",
            "91/91 [==============================] - 0s 3ms/step - loss: 0.3195 - accuracy: 0.8605 - precision: 0.8345 - recall: 0.9052\n",
            "Epoch 83/100\n",
            "91/91 [==============================] - 0s 3ms/step - loss: 0.3121 - accuracy: 0.8605 - precision: 0.8314 - recall: 0.9084\n",
            "Epoch 84/100\n",
            "91/91 [==============================] - 0s 3ms/step - loss: 0.3205 - accuracy: 0.8598 - precision: 0.8348 - recall: 0.9039\n",
            "Epoch 85/100\n",
            "91/91 [==============================] - 0s 3ms/step - loss: 0.3128 - accuracy: 0.8660 - precision: 0.8390 - recall: 0.9113\n",
            "Epoch 86/100\n",
            "91/91 [==============================] - 0s 3ms/step - loss: 0.3113 - accuracy: 0.8695 - precision: 0.8448 - recall: 0.9094\n",
            "Epoch 87/100\n",
            "91/91 [==============================] - 0s 3ms/step - loss: 0.3155 - accuracy: 0.8561 - precision: 0.8328 - recall: 0.8972\n",
            "Epoch 88/100\n",
            "91/91 [==============================] - 0s 3ms/step - loss: 0.3140 - accuracy: 0.8609 - precision: 0.8343 - recall: 0.9051\n",
            "Epoch 89/100\n",
            "91/91 [==============================] - 0s 3ms/step - loss: 0.3080 - accuracy: 0.8678 - precision: 0.8433 - recall: 0.9045\n",
            "Epoch 90/100\n",
            "91/91 [==============================] - 0s 3ms/step - loss: 0.3126 - accuracy: 0.8603 - precision: 0.8275 - recall: 0.9139\n",
            "Epoch 91/100\n",
            "91/91 [==============================] - 0s 3ms/step - loss: 0.3054 - accuracy: 0.8636 - precision: 0.8404 - recall: 0.9021\n",
            "Epoch 92/100\n",
            "91/91 [==============================] - 0s 3ms/step - loss: 0.3155 - accuracy: 0.8603 - precision: 0.8350 - recall: 0.9004\n",
            "Epoch 93/100\n",
            "91/91 [==============================] - 0s 3ms/step - loss: 0.3125 - accuracy: 0.8616 - precision: 0.8389 - recall: 0.8980\n",
            "Epoch 94/100\n",
            "91/91 [==============================] - 0s 3ms/step - loss: 0.3104 - accuracy: 0.8636 - precision: 0.8371 - recall: 0.9093\n",
            "Epoch 95/100\n",
            "91/91 [==============================] - 0s 3ms/step - loss: 0.3136 - accuracy: 0.8625 - precision: 0.8321 - recall: 0.9100\n",
            "Epoch 96/100\n",
            "91/91 [==============================] - 0s 3ms/step - loss: 0.3111 - accuracy: 0.8631 - precision: 0.8389 - recall: 0.9011\n",
            "Epoch 97/100\n",
            "91/91 [==============================] - 0s 3ms/step - loss: 0.3052 - accuracy: 0.8662 - precision: 0.8367 - recall: 0.9132\n",
            "Epoch 98/100\n",
            "91/91 [==============================] - 0s 3ms/step - loss: 0.3016 - accuracy: 0.8680 - precision: 0.8406 - recall: 0.9089\n",
            "Epoch 99/100\n",
            "91/91 [==============================] - 0s 3ms/step - loss: 0.3100 - accuracy: 0.8616 - precision: 0.8380 - recall: 0.9022\n",
            "Epoch 100/100\n",
            "91/91 [==============================] - 0s 3ms/step - loss: 0.3081 - accuracy: 0.8644 - precision: 0.8375 - recall: 0.9099\n",
            "46/46 [==============================] - 0s 2ms/step - loss: 0.2978 - accuracy: 0.8840 - precision: 0.8427 - recall: 0.9494\n",
            "[CV 2/3] END .........batch_size=50, epochs=100;, score=0.884 total time=  31.3s\n",
            "Model: \"sequential_10\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " dense_60 (Dense)            (None, 32)                352       \n",
            "                                                                 \n",
            " batch_normalization_30 (Bat  (None, 32)               128       \n",
            " chNormalization)                                                \n",
            "                                                                 \n",
            " dense_61 (Dense)            (None, 16)                528       \n",
            "                                                                 \n",
            " batch_normalization_31 (Bat  (None, 16)               64        \n",
            " chNormalization)                                                \n",
            "                                                                 \n",
            " dense_62 (Dense)            (None, 8)                 136       \n",
            "                                                                 \n",
            " batch_normalization_32 (Bat  (None, 8)                32        \n",
            " chNormalization)                                                \n",
            "                                                                 \n",
            " dense_63 (Dense)            (None, 4)                 36        \n",
            "                                                                 \n",
            " batch_normalization_33 (Bat  (None, 4)                16        \n",
            " chNormalization)                                                \n",
            "                                                                 \n",
            " dense_64 (Dense)            (None, 2)                 10        \n",
            "                                                                 \n",
            " batch_normalization_34 (Bat  (None, 2)                8         \n",
            " chNormalization)                                                \n",
            "                                                                 \n",
            " dense_65 (Dense)            (None, 1)                 3         \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 1,313\n",
            "Trainable params: 1,189\n",
            "Non-trainable params: 124\n",
            "_________________________________________________________________\n",
            "None\n",
            "Epoch 1/100\n",
            "91/91 [==============================] - 2s 3ms/step - loss: 0.6839 - accuracy: 0.6522 - precision: 0.6620 - recall: 0.6436\n",
            "Epoch 2/100\n",
            "91/91 [==============================] - 0s 3ms/step - loss: 0.5310 - accuracy: 0.7514 - precision: 0.7502 - recall: 0.7716\n",
            "Epoch 3/100\n",
            "91/91 [==============================] - 0s 3ms/step - loss: 0.5027 - accuracy: 0.7758 - precision: 0.7526 - recall: 0.8348\n",
            "Epoch 4/100\n",
            "91/91 [==============================] - 0s 3ms/step - loss: 0.4928 - accuracy: 0.7818 - precision: 0.7500 - recall: 0.8554\n",
            "Epoch 5/100\n",
            "91/91 [==============================] - 0s 3ms/step - loss: 0.4712 - accuracy: 0.7955 - precision: 0.7588 - recall: 0.8756\n",
            "Epoch 6/100\n",
            "91/91 [==============================] - 0s 3ms/step - loss: 0.4625 - accuracy: 0.7937 - precision: 0.7585 - recall: 0.8712\n",
            "Epoch 7/100\n",
            "91/91 [==============================] - 0s 3ms/step - loss: 0.4540 - accuracy: 0.8047 - precision: 0.7639 - recall: 0.8905\n",
            "Epoch 8/100\n",
            "91/91 [==============================] - 0s 3ms/step - loss: 0.4430 - accuracy: 0.8085 - precision: 0.7710 - recall: 0.8899\n",
            "Epoch 9/100\n",
            "91/91 [==============================] - 0s 3ms/step - loss: 0.4424 - accuracy: 0.8069 - precision: 0.7681 - recall: 0.8877\n",
            "Epoch 10/100\n",
            "91/91 [==============================] - 0s 3ms/step - loss: 0.4265 - accuracy: 0.8166 - precision: 0.7799 - recall: 0.8937\n",
            "Epoch 11/100\n",
            "91/91 [==============================] - 0s 3ms/step - loss: 0.4184 - accuracy: 0.8199 - precision: 0.7788 - recall: 0.9063\n",
            "Epoch 12/100\n",
            "91/91 [==============================] - 0s 3ms/step - loss: 0.4137 - accuracy: 0.8193 - precision: 0.7807 - recall: 0.8955\n",
            "Epoch 13/100\n",
            "91/91 [==============================] - 0s 3ms/step - loss: 0.4054 - accuracy: 0.8228 - precision: 0.7850 - recall: 0.8996\n",
            "Epoch 14/100\n",
            "91/91 [==============================] - 0s 3ms/step - loss: 0.4061 - accuracy: 0.8257 - precision: 0.7910 - recall: 0.8947\n",
            "Epoch 15/100\n",
            "91/91 [==============================] - 0s 3ms/step - loss: 0.4017 - accuracy: 0.8259 - precision: 0.7869 - recall: 0.9023\n",
            "Epoch 16/100\n",
            "91/91 [==============================] - 0s 3ms/step - loss: 0.4025 - accuracy: 0.8263 - precision: 0.7874 - recall: 0.9031\n",
            "Epoch 17/100\n",
            "91/91 [==============================] - 0s 3ms/step - loss: 0.3994 - accuracy: 0.8232 - precision: 0.7812 - recall: 0.9068\n",
            "Epoch 18/100\n",
            "91/91 [==============================] - 0s 3ms/step - loss: 0.3966 - accuracy: 0.8343 - precision: 0.7964 - recall: 0.9086\n",
            "Epoch 19/100\n",
            "91/91 [==============================] - 0s 3ms/step - loss: 0.3893 - accuracy: 0.8312 - precision: 0.7972 - recall: 0.8981\n",
            "Epoch 20/100\n",
            "91/91 [==============================] - 0s 3ms/step - loss: 0.3901 - accuracy: 0.8329 - precision: 0.7933 - recall: 0.9094\n",
            "Epoch 21/100\n",
            "91/91 [==============================] - 0s 3ms/step - loss: 0.3818 - accuracy: 0.8312 - precision: 0.7949 - recall: 0.9017\n",
            "Epoch 22/100\n",
            "91/91 [==============================] - 0s 3ms/step - loss: 0.3812 - accuracy: 0.8354 - precision: 0.8004 - recall: 0.9012\n",
            "Epoch 23/100\n",
            "91/91 [==============================] - 0s 3ms/step - loss: 0.3798 - accuracy: 0.8349 - precision: 0.7993 - recall: 0.9043\n",
            "Epoch 24/100\n",
            "91/91 [==============================] - 0s 3ms/step - loss: 0.3822 - accuracy: 0.8358 - precision: 0.7935 - recall: 0.9167\n",
            "Epoch 25/100\n",
            "91/91 [==============================] - 0s 3ms/step - loss: 0.3789 - accuracy: 0.8391 - precision: 0.8012 - recall: 0.9112\n",
            "Epoch 26/100\n",
            "91/91 [==============================] - 0s 3ms/step - loss: 0.3798 - accuracy: 0.8404 - precision: 0.8019 - recall: 0.9137\n",
            "Epoch 27/100\n",
            "91/91 [==============================] - 0s 3ms/step - loss: 0.3768 - accuracy: 0.8393 - precision: 0.8035 - recall: 0.9083\n",
            "Epoch 28/100\n",
            "91/91 [==============================] - 0s 3ms/step - loss: 0.3747 - accuracy: 0.8422 - precision: 0.8021 - recall: 0.9191\n",
            "Epoch 29/100\n",
            "91/91 [==============================] - 0s 3ms/step - loss: 0.3705 - accuracy: 0.8431 - precision: 0.8054 - recall: 0.9138\n",
            "Epoch 30/100\n",
            "91/91 [==============================] - 0s 3ms/step - loss: 0.3741 - accuracy: 0.8373 - precision: 0.8084 - recall: 0.8961\n",
            "Epoch 31/100\n",
            "91/91 [==============================] - 0s 3ms/step - loss: 0.3689 - accuracy: 0.8435 - precision: 0.8070 - recall: 0.9117\n",
            "Epoch 32/100\n",
            "91/91 [==============================] - 0s 3ms/step - loss: 0.3680 - accuracy: 0.8462 - precision: 0.8087 - recall: 0.9196\n",
            "Epoch 33/100\n",
            "91/91 [==============================] - 0s 3ms/step - loss: 0.3706 - accuracy: 0.8442 - precision: 0.8127 - recall: 0.9042\n",
            "Epoch 34/100\n",
            "91/91 [==============================] - 0s 3ms/step - loss: 0.3659 - accuracy: 0.8455 - precision: 0.8083 - recall: 0.9149\n",
            "Epoch 35/100\n",
            "91/91 [==============================] - 0s 3ms/step - loss: 0.3639 - accuracy: 0.8470 - precision: 0.8121 - recall: 0.9108\n",
            "Epoch 36/100\n",
            "91/91 [==============================] - 0s 3ms/step - loss: 0.3583 - accuracy: 0.8475 - precision: 0.8112 - recall: 0.9149\n",
            "Epoch 37/100\n",
            "91/91 [==============================] - 0s 3ms/step - loss: 0.3686 - accuracy: 0.8488 - precision: 0.8137 - recall: 0.9143\n",
            "Epoch 38/100\n",
            "91/91 [==============================] - 0s 3ms/step - loss: 0.3676 - accuracy: 0.8433 - precision: 0.8084 - recall: 0.9101\n",
            "Epoch 39/100\n",
            "91/91 [==============================] - 0s 3ms/step - loss: 0.3593 - accuracy: 0.8510 - precision: 0.8145 - recall: 0.9179\n",
            "Epoch 40/100\n",
            "91/91 [==============================] - 0s 3ms/step - loss: 0.3610 - accuracy: 0.8470 - precision: 0.8140 - recall: 0.9076\n",
            "Epoch 41/100\n",
            "91/91 [==============================] - 0s 3ms/step - loss: 0.3605 - accuracy: 0.8484 - precision: 0.8160 - recall: 0.9072\n",
            "Epoch 42/100\n",
            "91/91 [==============================] - 0s 3ms/step - loss: 0.3569 - accuracy: 0.8492 - precision: 0.8124 - recall: 0.9165\n",
            "Epoch 43/100\n",
            "91/91 [==============================] - 0s 3ms/step - loss: 0.3480 - accuracy: 0.8561 - precision: 0.8171 - recall: 0.9256\n",
            "Epoch 44/100\n",
            "91/91 [==============================] - 0s 3ms/step - loss: 0.3492 - accuracy: 0.8503 - precision: 0.8146 - recall: 0.9156\n",
            "Epoch 45/100\n",
            "91/91 [==============================] - 0s 2ms/step - loss: 0.3584 - accuracy: 0.8466 - precision: 0.8163 - recall: 0.9051\n",
            "Epoch 46/100\n",
            "91/91 [==============================] - 0s 3ms/step - loss: 0.3555 - accuracy: 0.8559 - precision: 0.8176 - recall: 0.9253\n",
            "Epoch 47/100\n",
            "91/91 [==============================] - 0s 3ms/step - loss: 0.3569 - accuracy: 0.8501 - precision: 0.8144 - recall: 0.9169\n",
            "Epoch 48/100\n",
            "91/91 [==============================] - 0s 3ms/step - loss: 0.3602 - accuracy: 0.8457 - precision: 0.8099 - recall: 0.9124\n",
            "Epoch 49/100\n",
            "91/91 [==============================] - 0s 3ms/step - loss: 0.3456 - accuracy: 0.8517 - precision: 0.8180 - recall: 0.9150\n",
            "Epoch 50/100\n",
            "91/91 [==============================] - 0s 3ms/step - loss: 0.3420 - accuracy: 0.8585 - precision: 0.8302 - recall: 0.9079\n",
            "Epoch 51/100\n",
            "91/91 [==============================] - 0s 3ms/step - loss: 0.3281 - accuracy: 0.8614 - precision: 0.8291 - recall: 0.9194\n",
            "Epoch 52/100\n",
            "91/91 [==============================] - 0s 3ms/step - loss: 0.3459 - accuracy: 0.8556 - precision: 0.8198 - recall: 0.9179\n",
            "Epoch 53/100\n",
            "91/91 [==============================] - 0s 3ms/step - loss: 0.3377 - accuracy: 0.8581 - precision: 0.8271 - recall: 0.9147\n",
            "Epoch 54/100\n",
            "91/91 [==============================] - 0s 3ms/step - loss: 0.3389 - accuracy: 0.8603 - precision: 0.8339 - recall: 0.9086\n",
            "Epoch 55/100\n",
            "91/91 [==============================] - 0s 3ms/step - loss: 0.3367 - accuracy: 0.8622 - precision: 0.8319 - recall: 0.9162\n",
            "Epoch 56/100\n",
            "91/91 [==============================] - 0s 3ms/step - loss: 0.3397 - accuracy: 0.8622 - precision: 0.8294 - recall: 0.9210\n",
            "Epoch 57/100\n",
            "91/91 [==============================] - 0s 3ms/step - loss: 0.3360 - accuracy: 0.8578 - precision: 0.8283 - recall: 0.9103\n",
            "Epoch 58/100\n",
            "91/91 [==============================] - 0s 3ms/step - loss: 0.3337 - accuracy: 0.8603 - precision: 0.8317 - recall: 0.9128\n",
            "Epoch 59/100\n",
            "91/91 [==============================] - 0s 3ms/step - loss: 0.3390 - accuracy: 0.8565 - precision: 0.8213 - recall: 0.9210\n",
            "Epoch 60/100\n",
            "91/91 [==============================] - 0s 3ms/step - loss: 0.3290 - accuracy: 0.8625 - precision: 0.8318 - recall: 0.9168\n",
            "Epoch 61/100\n",
            "91/91 [==============================] - 0s 3ms/step - loss: 0.3276 - accuracy: 0.8636 - precision: 0.8273 - recall: 0.9272\n",
            "Epoch 62/100\n",
            "91/91 [==============================] - 0s 3ms/step - loss: 0.3417 - accuracy: 0.8589 - precision: 0.8281 - recall: 0.9146\n",
            "Epoch 63/100\n",
            "91/91 [==============================] - 0s 3ms/step - loss: 0.3439 - accuracy: 0.8545 - precision: 0.8237 - recall: 0.9139\n",
            "Epoch 64/100\n",
            "91/91 [==============================] - 0s 3ms/step - loss: 0.3270 - accuracy: 0.8649 - precision: 0.8308 - recall: 0.9243\n",
            "Epoch 65/100\n",
            "91/91 [==============================] - 0s 3ms/step - loss: 0.3201 - accuracy: 0.8702 - precision: 0.8358 - recall: 0.9293\n",
            "Epoch 66/100\n",
            "91/91 [==============================] - 0s 3ms/step - loss: 0.3307 - accuracy: 0.8614 - precision: 0.8300 - recall: 0.9166\n",
            "Epoch 67/100\n",
            "91/91 [==============================] - 0s 3ms/step - loss: 0.3306 - accuracy: 0.8658 - precision: 0.8305 - recall: 0.9248\n",
            "Epoch 68/100\n",
            "91/91 [==============================] - 0s 3ms/step - loss: 0.3259 - accuracy: 0.8642 - precision: 0.8337 - recall: 0.9193\n",
            "Epoch 69/100\n",
            "91/91 [==============================] - 0s 3ms/step - loss: 0.3271 - accuracy: 0.8689 - precision: 0.8359 - recall: 0.9273\n",
            "Epoch 70/100\n",
            "91/91 [==============================] - 0s 3ms/step - loss: 0.3277 - accuracy: 0.8669 - precision: 0.8350 - recall: 0.9231\n",
            "Epoch 71/100\n",
            "91/91 [==============================] - 0s 3ms/step - loss: 0.3184 - accuracy: 0.8693 - precision: 0.8366 - recall: 0.9242\n",
            "Epoch 72/100\n",
            "91/91 [==============================] - 0s 3ms/step - loss: 0.3245 - accuracy: 0.8662 - precision: 0.8373 - recall: 0.9160\n",
            "Epoch 73/100\n",
            "91/91 [==============================] - 0s 5ms/step - loss: 0.3213 - accuracy: 0.8675 - precision: 0.8373 - recall: 0.9210\n",
            "Epoch 74/100\n",
            "91/91 [==============================] - 0s 5ms/step - loss: 0.3170 - accuracy: 0.8664 - precision: 0.8332 - recall: 0.9252\n",
            "Epoch 75/100\n",
            "91/91 [==============================] - 1s 6ms/step - loss: 0.3292 - accuracy: 0.8611 - precision: 0.8364 - recall: 0.9098\n",
            "Epoch 76/100\n",
            "91/91 [==============================] - 0s 5ms/step - loss: 0.3164 - accuracy: 0.8667 - precision: 0.8304 - recall: 0.9264\n",
            "Epoch 77/100\n",
            "91/91 [==============================] - 0s 5ms/step - loss: 0.3200 - accuracy: 0.8706 - precision: 0.8370 - recall: 0.9274\n",
            "Epoch 78/100\n",
            "91/91 [==============================] - 1s 6ms/step - loss: 0.3236 - accuracy: 0.8651 - precision: 0.8338 - recall: 0.9199\n",
            "Epoch 79/100\n",
            "91/91 [==============================] - 0s 5ms/step - loss: 0.3175 - accuracy: 0.8693 - precision: 0.8379 - recall: 0.9220\n",
            "Epoch 80/100\n",
            "91/91 [==============================] - 0s 4ms/step - loss: 0.3175 - accuracy: 0.8728 - precision: 0.8400 - recall: 0.9293\n",
            "Epoch 81/100\n",
            "91/91 [==============================] - 0s 4ms/step - loss: 0.3212 - accuracy: 0.8693 - precision: 0.8402 - recall: 0.9222\n",
            "Epoch 82/100\n",
            "91/91 [==============================] - 0s 4ms/step - loss: 0.3196 - accuracy: 0.8702 - precision: 0.8427 - recall: 0.9197\n",
            "Epoch 83/100\n",
            "91/91 [==============================] - 0s 5ms/step - loss: 0.3144 - accuracy: 0.8766 - precision: 0.8446 - recall: 0.9301\n",
            "Epoch 84/100\n",
            "91/91 [==============================] - 1s 9ms/step - loss: 0.3101 - accuracy: 0.8695 - precision: 0.8350 - recall: 0.9265\n",
            "Epoch 85/100\n",
            "91/91 [==============================] - 0s 3ms/step - loss: 0.3123 - accuracy: 0.8722 - precision: 0.8402 - recall: 0.9263\n",
            "Epoch 86/100\n",
            "91/91 [==============================] - 0s 3ms/step - loss: 0.3134 - accuracy: 0.8728 - precision: 0.8421 - recall: 0.9270\n",
            "Epoch 87/100\n",
            "91/91 [==============================] - 0s 3ms/step - loss: 0.3189 - accuracy: 0.8728 - precision: 0.8407 - recall: 0.9270\n",
            "Epoch 88/100\n",
            "91/91 [==============================] - 0s 3ms/step - loss: 0.3299 - accuracy: 0.8574 - precision: 0.8276 - recall: 0.9128\n",
            "Epoch 89/100\n",
            "91/91 [==============================] - 0s 3ms/step - loss: 0.3084 - accuracy: 0.8775 - precision: 0.8424 - recall: 0.9357\n",
            "Epoch 90/100\n",
            "91/91 [==============================] - 0s 3ms/step - loss: 0.3098 - accuracy: 0.8744 - precision: 0.8465 - recall: 0.9219\n",
            "Epoch 91/100\n",
            "91/91 [==============================] - 0s 3ms/step - loss: 0.3134 - accuracy: 0.8739 - precision: 0.8435 - recall: 0.9254\n",
            "Epoch 92/100\n",
            "91/91 [==============================] - 0s 3ms/step - loss: 0.3081 - accuracy: 0.8779 - precision: 0.8487 - recall: 0.9280\n",
            "Epoch 93/100\n",
            "91/91 [==============================] - 0s 3ms/step - loss: 0.3068 - accuracy: 0.8772 - precision: 0.8446 - recall: 0.9311\n",
            "Epoch 94/100\n",
            "91/91 [==============================] - 0s 3ms/step - loss: 0.3098 - accuracy: 0.8730 - precision: 0.8467 - recall: 0.9203\n",
            "Epoch 95/100\n",
            "91/91 [==============================] - 0s 3ms/step - loss: 0.3062 - accuracy: 0.8757 - precision: 0.8476 - recall: 0.9249\n",
            "Epoch 96/100\n",
            "91/91 [==============================] - 0s 3ms/step - loss: 0.3084 - accuracy: 0.8746 - precision: 0.8430 - recall: 0.9287\n",
            "Epoch 97/100\n",
            "91/91 [==============================] - 0s 3ms/step - loss: 0.3056 - accuracy: 0.8759 - precision: 0.8465 - recall: 0.9270\n",
            "Epoch 98/100\n",
            "91/91 [==============================] - 0s 3ms/step - loss: 0.3166 - accuracy: 0.8671 - precision: 0.8361 - recall: 0.9210\n",
            "Epoch 99/100\n",
            "91/91 [==============================] - 0s 3ms/step - loss: 0.3069 - accuracy: 0.8750 - precision: 0.8456 - recall: 0.9261\n",
            "Epoch 100/100\n",
            "91/91 [==============================] - 0s 3ms/step - loss: 0.2998 - accuracy: 0.8775 - precision: 0.8468 - recall: 0.9280\n",
            "46/46 [==============================] - 0s 2ms/step - loss: 0.3068 - accuracy: 0.8735 - precision: 0.8348 - recall: 0.9350\n",
            "[CV 3/3] END .........batch_size=50, epochs=100;, score=0.873 total time=  32.2s\n",
            "Model: \"sequential_11\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " dense_66 (Dense)            (None, 32)                352       \n",
            "                                                                 \n",
            " batch_normalization_35 (Bat  (None, 32)               128       \n",
            " chNormalization)                                                \n",
            "                                                                 \n",
            " dense_67 (Dense)            (None, 16)                528       \n",
            "                                                                 \n",
            " batch_normalization_36 (Bat  (None, 16)               64        \n",
            " chNormalization)                                                \n",
            "                                                                 \n",
            " dense_68 (Dense)            (None, 8)                 136       \n",
            "                                                                 \n",
            " batch_normalization_37 (Bat  (None, 8)                32        \n",
            " chNormalization)                                                \n",
            "                                                                 \n",
            " dense_69 (Dense)            (None, 4)                 36        \n",
            "                                                                 \n",
            " batch_normalization_38 (Bat  (None, 4)                16        \n",
            " chNormalization)                                                \n",
            "                                                                 \n",
            " dense_70 (Dense)            (None, 2)                 10        \n",
            "                                                                 \n",
            " batch_normalization_39 (Bat  (None, 2)                8         \n",
            " chNormalization)                                                \n",
            "                                                                 \n",
            " dense_71 (Dense)            (None, 1)                 3         \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 1,313\n",
            "Trainable params: 1,189\n",
            "Non-trainable params: 124\n",
            "_________________________________________________________________\n",
            "None\n",
            "Epoch 1/50\n",
            "46/46 [==============================] - 2s 3ms/step - loss: 0.6589 - accuracy: 0.6598 - precision: 0.6404 - recall: 0.7803\n",
            "Epoch 2/50\n",
            "46/46 [==============================] - 0s 3ms/step - loss: 0.6049 - accuracy: 0.7652 - precision: 0.7216 - recall: 0.8761\n",
            "Epoch 3/50\n",
            "46/46 [==============================] - 0s 3ms/step - loss: 0.5627 - accuracy: 0.7914 - precision: 0.7523 - recall: 0.8873\n",
            "Epoch 4/50\n",
            "46/46 [==============================] - 0s 3ms/step - loss: 0.5308 - accuracy: 0.7967 - precision: 0.7530 - recall: 0.8921\n",
            "Epoch 5/50\n",
            "46/46 [==============================] - 0s 3ms/step - loss: 0.5020 - accuracy: 0.8036 - precision: 0.7638 - recall: 0.8892\n",
            "Epoch 6/50\n",
            "46/46 [==============================] - 0s 3ms/step - loss: 0.4762 - accuracy: 0.8166 - precision: 0.7755 - recall: 0.8975\n",
            "Epoch 7/50\n",
            "46/46 [==============================] - 0s 3ms/step - loss: 0.4580 - accuracy: 0.8179 - precision: 0.7772 - recall: 0.8955\n",
            "Epoch 8/50\n",
            "46/46 [==============================] - 0s 3ms/step - loss: 0.4368 - accuracy: 0.8219 - precision: 0.7820 - recall: 0.8978\n",
            "Epoch 9/50\n",
            "46/46 [==============================] - 0s 3ms/step - loss: 0.4273 - accuracy: 0.8214 - precision: 0.7859 - recall: 0.8938\n",
            "Epoch 10/50\n",
            "46/46 [==============================] - 0s 3ms/step - loss: 0.4171 - accuracy: 0.8230 - precision: 0.7853 - recall: 0.8938\n",
            "Epoch 11/50\n",
            "46/46 [==============================] - 0s 3ms/step - loss: 0.4069 - accuracy: 0.8269 - precision: 0.7861 - recall: 0.9053\n",
            "Epoch 12/50\n",
            "46/46 [==============================] - 0s 3ms/step - loss: 0.4034 - accuracy: 0.8243 - precision: 0.7856 - recall: 0.8995\n",
            "Epoch 13/50\n",
            "46/46 [==============================] - 0s 3ms/step - loss: 0.3945 - accuracy: 0.8316 - precision: 0.7933 - recall: 0.9043\n",
            "Epoch 14/50\n",
            "46/46 [==============================] - 0s 3ms/step - loss: 0.3954 - accuracy: 0.8294 - precision: 0.7909 - recall: 0.9070\n",
            "Epoch 15/50\n",
            "46/46 [==============================] - 0s 3ms/step - loss: 0.3866 - accuracy: 0.8305 - precision: 0.7873 - recall: 0.9113\n",
            "Epoch 16/50\n",
            "46/46 [==============================] - 0s 3ms/step - loss: 0.3781 - accuracy: 0.8349 - precision: 0.7946 - recall: 0.9106\n",
            "Epoch 17/50\n",
            "46/46 [==============================] - 0s 3ms/step - loss: 0.3720 - accuracy: 0.8395 - precision: 0.7965 - recall: 0.9222\n",
            "Epoch 18/50\n",
            "46/46 [==============================] - 0s 5ms/step - loss: 0.3665 - accuracy: 0.8413 - precision: 0.8009 - recall: 0.9129\n",
            "Epoch 19/50\n",
            "46/46 [==============================] - 0s 7ms/step - loss: 0.3771 - accuracy: 0.8311 - precision: 0.7884 - recall: 0.9170\n",
            "Epoch 20/50\n",
            "46/46 [==============================] - 0s 5ms/step - loss: 0.3688 - accuracy: 0.8399 - precision: 0.7932 - recall: 0.9243\n",
            "Epoch 21/50\n",
            "46/46 [==============================] - 0s 6ms/step - loss: 0.3624 - accuracy: 0.8395 - precision: 0.8007 - recall: 0.9166\n",
            "Epoch 22/50\n",
            "46/46 [==============================] - 0s 5ms/step - loss: 0.3611 - accuracy: 0.8417 - precision: 0.8027 - recall: 0.9150\n",
            "Epoch 23/50\n",
            "46/46 [==============================] - 0s 4ms/step - loss: 0.3567 - accuracy: 0.8395 - precision: 0.7969 - recall: 0.9190\n",
            "Epoch 24/50\n",
            "46/46 [==============================] - 0s 5ms/step - loss: 0.3511 - accuracy: 0.8422 - precision: 0.8010 - recall: 0.9169\n",
            "Epoch 25/50\n",
            "46/46 [==============================] - 0s 5ms/step - loss: 0.3558 - accuracy: 0.8424 - precision: 0.7992 - recall: 0.9229\n",
            "Epoch 26/50\n",
            "46/46 [==============================] - 0s 6ms/step - loss: 0.3576 - accuracy: 0.8369 - precision: 0.7871 - recall: 0.9277\n",
            "Epoch 27/50\n",
            "46/46 [==============================] - 0s 6ms/step - loss: 0.3480 - accuracy: 0.8461 - precision: 0.8016 - recall: 0.9258\n",
            "Epoch 28/50\n",
            "46/46 [==============================] - 0s 6ms/step - loss: 0.3499 - accuracy: 0.8433 - precision: 0.8008 - recall: 0.9199\n",
            "Epoch 29/50\n",
            "46/46 [==============================] - 0s 6ms/step - loss: 0.3401 - accuracy: 0.8512 - precision: 0.8083 - recall: 0.9279\n",
            "Epoch 30/50\n",
            "46/46 [==============================] - 0s 6ms/step - loss: 0.3480 - accuracy: 0.8501 - precision: 0.7990 - recall: 0.9368\n",
            "Epoch 31/50\n",
            "46/46 [==============================] - 0s 4ms/step - loss: 0.3451 - accuracy: 0.8488 - precision: 0.8052 - recall: 0.9281\n",
            "Epoch 32/50\n",
            "46/46 [==============================] - 0s 3ms/step - loss: 0.3479 - accuracy: 0.8474 - precision: 0.8020 - recall: 0.9286\n",
            "Epoch 33/50\n",
            "46/46 [==============================] - 0s 9ms/step - loss: 0.3425 - accuracy: 0.8501 - precision: 0.8047 - recall: 0.9353\n",
            "Epoch 34/50\n",
            "46/46 [==============================] - 0s 5ms/step - loss: 0.3412 - accuracy: 0.8488 - precision: 0.8035 - recall: 0.9282\n",
            "Epoch 35/50\n",
            "46/46 [==============================] - 0s 4ms/step - loss: 0.3387 - accuracy: 0.8527 - precision: 0.8062 - recall: 0.9376\n",
            "Epoch 36/50\n",
            "46/46 [==============================] - 0s 4ms/step - loss: 0.3360 - accuracy: 0.8507 - precision: 0.8080 - recall: 0.9296\n",
            "Epoch 37/50\n",
            "46/46 [==============================] - 0s 4ms/step - loss: 0.3386 - accuracy: 0.8455 - precision: 0.8000 - recall: 0.9290\n",
            "Epoch 38/50\n",
            "46/46 [==============================] - 0s 3ms/step - loss: 0.3368 - accuracy: 0.8501 - precision: 0.8096 - recall: 0.9266\n",
            "Epoch 39/50\n",
            "46/46 [==============================] - 0s 5ms/step - loss: 0.3367 - accuracy: 0.8501 - precision: 0.8077 - recall: 0.9290\n",
            "Epoch 40/50\n",
            "46/46 [==============================] - 0s 7ms/step - loss: 0.3312 - accuracy: 0.8503 - precision: 0.8149 - recall: 0.9160\n",
            "Epoch 41/50\n",
            "46/46 [==============================] - 0s 6ms/step - loss: 0.3382 - accuracy: 0.8479 - precision: 0.8046 - recall: 0.9287\n",
            "Epoch 42/50\n",
            "46/46 [==============================] - 0s 4ms/step - loss: 0.3306 - accuracy: 0.8523 - precision: 0.8061 - recall: 0.9370\n",
            "Epoch 43/50\n",
            "46/46 [==============================] - 0s 3ms/step - loss: 0.3322 - accuracy: 0.8477 - precision: 0.8036 - recall: 0.9301\n",
            "Epoch 44/50\n",
            "46/46 [==============================] - 0s 4ms/step - loss: 0.3317 - accuracy: 0.8494 - precision: 0.8079 - recall: 0.9222\n",
            "Epoch 45/50\n",
            "46/46 [==============================] - 0s 4ms/step - loss: 0.3306 - accuracy: 0.8516 - precision: 0.8115 - recall: 0.9250\n",
            "Epoch 46/50\n",
            "46/46 [==============================] - 0s 4ms/step - loss: 0.3215 - accuracy: 0.8547 - precision: 0.8117 - recall: 0.9374\n",
            "Epoch 47/50\n",
            "46/46 [==============================] - 0s 5ms/step - loss: 0.3365 - accuracy: 0.8536 - precision: 0.8149 - recall: 0.9245\n",
            "Epoch 48/50\n",
            "46/46 [==============================] - 0s 5ms/step - loss: 0.3220 - accuracy: 0.8567 - precision: 0.8175 - recall: 0.9284\n",
            "Epoch 49/50\n",
            "46/46 [==============================] - 0s 5ms/step - loss: 0.3272 - accuracy: 0.8525 - precision: 0.8167 - recall: 0.9213\n",
            "Epoch 50/50\n",
            "46/46 [==============================] - 0s 4ms/step - loss: 0.3300 - accuracy: 0.8543 - precision: 0.8139 - recall: 0.9247\n",
            "23/23 [==============================] - 0s 2ms/step - loss: 0.3862 - accuracy: 0.8290 - precision: 0.7921 - recall: 0.8908\n",
            "[CV 1/3] END .........batch_size=100, epochs=50;, score=0.829 total time=  22.5s\n",
            "Model: \"sequential_12\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " dense_72 (Dense)            (None, 32)                352       \n",
            "                                                                 \n",
            " batch_normalization_40 (Bat  (None, 32)               128       \n",
            " chNormalization)                                                \n",
            "                                                                 \n",
            " dense_73 (Dense)            (None, 16)                528       \n",
            "                                                                 \n",
            " batch_normalization_41 (Bat  (None, 16)               64        \n",
            " chNormalization)                                                \n",
            "                                                                 \n",
            " dense_74 (Dense)            (None, 8)                 136       \n",
            "                                                                 \n",
            " batch_normalization_42 (Bat  (None, 8)                32        \n",
            " chNormalization)                                                \n",
            "                                                                 \n",
            " dense_75 (Dense)            (None, 4)                 36        \n",
            "                                                                 \n",
            " batch_normalization_43 (Bat  (None, 4)                16        \n",
            " chNormalization)                                                \n",
            "                                                                 \n",
            " dense_76 (Dense)            (None, 2)                 10        \n",
            "                                                                 \n",
            " batch_normalization_44 (Bat  (None, 2)                8         \n",
            " chNormalization)                                                \n",
            "                                                                 \n",
            " dense_77 (Dense)            (None, 1)                 3         \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 1,313\n",
            "Trainable params: 1,189\n",
            "Non-trainable params: 124\n",
            "_________________________________________________________________\n",
            "None\n",
            "Epoch 1/50\n",
            "46/46 [==============================] - 2s 3ms/step - loss: 0.6399 - accuracy: 0.6544 - precision: 0.6396 - recall: 0.7077\n",
            "Epoch 2/50\n",
            "46/46 [==============================] - 0s 3ms/step - loss: 0.5291 - accuracy: 0.7366 - precision: 0.7163 - recall: 0.7859\n",
            "Epoch 3/50\n",
            "46/46 [==============================] - 0s 3ms/step - loss: 0.4976 - accuracy: 0.7683 - precision: 0.7453 - recall: 0.8126\n",
            "Epoch 4/50\n",
            "46/46 [==============================] - 0s 3ms/step - loss: 0.4827 - accuracy: 0.7805 - precision: 0.7511 - recall: 0.8389\n",
            "Epoch 5/50\n",
            "46/46 [==============================] - 0s 4ms/step - loss: 0.4747 - accuracy: 0.7847 - precision: 0.7522 - recall: 0.8423\n",
            "Epoch 6/50\n",
            "46/46 [==============================] - 0s 3ms/step - loss: 0.4660 - accuracy: 0.7948 - precision: 0.7582 - recall: 0.8623\n",
            "Epoch 7/50\n",
            "46/46 [==============================] - 0s 3ms/step - loss: 0.4620 - accuracy: 0.7937 - precision: 0.7590 - recall: 0.8610\n",
            "Epoch 8/50\n",
            "46/46 [==============================] - 0s 3ms/step - loss: 0.4557 - accuracy: 0.7979 - precision: 0.7564 - recall: 0.8748\n",
            "Epoch 9/50\n",
            "46/46 [==============================] - 0s 3ms/step - loss: 0.4437 - accuracy: 0.8041 - precision: 0.7686 - recall: 0.8722\n",
            "Epoch 10/50\n",
            "46/46 [==============================] - 0s 3ms/step - loss: 0.4459 - accuracy: 0.7999 - precision: 0.7667 - recall: 0.8590\n",
            "Epoch 11/50\n",
            "46/46 [==============================] - 0s 3ms/step - loss: 0.4385 - accuracy: 0.8014 - precision: 0.7611 - recall: 0.8708\n",
            "Epoch 12/50\n",
            "46/46 [==============================] - 0s 4ms/step - loss: 0.4335 - accuracy: 0.8082 - precision: 0.7692 - recall: 0.8723\n",
            "Epoch 13/50\n",
            "46/46 [==============================] - 0s 3ms/step - loss: 0.4276 - accuracy: 0.8074 - precision: 0.7665 - recall: 0.8753\n",
            "Epoch 14/50\n",
            "46/46 [==============================] - 0s 3ms/step - loss: 0.4273 - accuracy: 0.8124 - precision: 0.7794 - recall: 0.8718\n",
            "Epoch 15/50\n",
            "46/46 [==============================] - 0s 3ms/step - loss: 0.4167 - accuracy: 0.8153 - precision: 0.7728 - recall: 0.8891\n",
            "Epoch 16/50\n",
            "46/46 [==============================] - 0s 3ms/step - loss: 0.4161 - accuracy: 0.8157 - precision: 0.7776 - recall: 0.8832\n",
            "Epoch 17/50\n",
            "46/46 [==============================] - 0s 3ms/step - loss: 0.4166 - accuracy: 0.8111 - precision: 0.7758 - recall: 0.8801\n",
            "Epoch 18/50\n",
            "46/46 [==============================] - 0s 4ms/step - loss: 0.4101 - accuracy: 0.8175 - precision: 0.7763 - recall: 0.8893\n",
            "Epoch 19/50\n",
            "46/46 [==============================] - 0s 3ms/step - loss: 0.4118 - accuracy: 0.8151 - precision: 0.7796 - recall: 0.8732\n",
            "Epoch 20/50\n",
            "46/46 [==============================] - 0s 3ms/step - loss: 0.4067 - accuracy: 0.8164 - precision: 0.7744 - recall: 0.8887\n",
            "Epoch 21/50\n",
            "46/46 [==============================] - 0s 3ms/step - loss: 0.4020 - accuracy: 0.8226 - precision: 0.7851 - recall: 0.8876\n",
            "Epoch 22/50\n",
            "46/46 [==============================] - 0s 3ms/step - loss: 0.4094 - accuracy: 0.8144 - precision: 0.7741 - recall: 0.8915\n",
            "Epoch 23/50\n",
            "46/46 [==============================] - 0s 3ms/step - loss: 0.3972 - accuracy: 0.8241 - precision: 0.7876 - recall: 0.8896\n",
            "Epoch 24/50\n",
            "46/46 [==============================] - 0s 3ms/step - loss: 0.3984 - accuracy: 0.8237 - precision: 0.7829 - recall: 0.8955\n",
            "Epoch 25/50\n",
            "46/46 [==============================] - 0s 3ms/step - loss: 0.3926 - accuracy: 0.8241 - precision: 0.7821 - recall: 0.8970\n",
            "Epoch 26/50\n",
            "46/46 [==============================] - 0s 3ms/step - loss: 0.3914 - accuracy: 0.8285 - precision: 0.7854 - recall: 0.9001\n",
            "Epoch 27/50\n",
            "46/46 [==============================] - 0s 4ms/step - loss: 0.3844 - accuracy: 0.8345 - precision: 0.7951 - recall: 0.9016\n",
            "Epoch 28/50\n",
            "46/46 [==============================] - 0s 3ms/step - loss: 0.3830 - accuracy: 0.8312 - precision: 0.7942 - recall: 0.8938\n",
            "Epoch 29/50\n",
            "46/46 [==============================] - 0s 3ms/step - loss: 0.3881 - accuracy: 0.8281 - precision: 0.7899 - recall: 0.8958\n",
            "Epoch 30/50\n",
            "46/46 [==============================] - 0s 3ms/step - loss: 0.3893 - accuracy: 0.8272 - precision: 0.7850 - recall: 0.8970\n",
            "Epoch 31/50\n",
            "46/46 [==============================] - 0s 3ms/step - loss: 0.3786 - accuracy: 0.8345 - precision: 0.7958 - recall: 0.9017\n",
            "Epoch 32/50\n",
            "46/46 [==============================] - 0s 3ms/step - loss: 0.3808 - accuracy: 0.8343 - precision: 0.7997 - recall: 0.8913\n",
            "Epoch 33/50\n",
            "46/46 [==============================] - 0s 3ms/step - loss: 0.3786 - accuracy: 0.8331 - precision: 0.7917 - recall: 0.9005\n",
            "Epoch 34/50\n",
            "46/46 [==============================] - 0s 3ms/step - loss: 0.3814 - accuracy: 0.8369 - precision: 0.8057 - recall: 0.8854\n",
            "Epoch 35/50\n",
            "46/46 [==============================] - 0s 3ms/step - loss: 0.3779 - accuracy: 0.8395 - precision: 0.7987 - recall: 0.9067\n",
            "Epoch 36/50\n",
            "46/46 [==============================] - 0s 3ms/step - loss: 0.3747 - accuracy: 0.8369 - precision: 0.7978 - recall: 0.8940\n",
            "Epoch 37/50\n",
            "46/46 [==============================] - 0s 3ms/step - loss: 0.3736 - accuracy: 0.8358 - precision: 0.7987 - recall: 0.9013\n",
            "Epoch 38/50\n",
            "46/46 [==============================] - 0s 3ms/step - loss: 0.3669 - accuracy: 0.8444 - precision: 0.8023 - recall: 0.9095\n",
            "Epoch 39/50\n",
            "46/46 [==============================] - 0s 3ms/step - loss: 0.3629 - accuracy: 0.8428 - precision: 0.8022 - recall: 0.9055\n",
            "Epoch 40/50\n",
            "46/46 [==============================] - 0s 3ms/step - loss: 0.3678 - accuracy: 0.8466 - precision: 0.8116 - recall: 0.9050\n",
            "Epoch 41/50\n",
            "46/46 [==============================] - 0s 3ms/step - loss: 0.3754 - accuracy: 0.8334 - precision: 0.7988 - recall: 0.8963\n",
            "Epoch 42/50\n",
            "46/46 [==============================] - 0s 3ms/step - loss: 0.3595 - accuracy: 0.8455 - precision: 0.8053 - recall: 0.9078\n",
            "Epoch 43/50\n",
            "46/46 [==============================] - 0s 3ms/step - loss: 0.3675 - accuracy: 0.8448 - precision: 0.8071 - recall: 0.9086\n",
            "Epoch 44/50\n",
            "46/46 [==============================] - 0s 3ms/step - loss: 0.3601 - accuracy: 0.8459 - precision: 0.8048 - recall: 0.9113\n",
            "Epoch 45/50\n",
            "46/46 [==============================] - 0s 3ms/step - loss: 0.3569 - accuracy: 0.8442 - precision: 0.8073 - recall: 0.9026\n",
            "Epoch 46/50\n",
            "46/46 [==============================] - 0s 3ms/step - loss: 0.3644 - accuracy: 0.8395 - precision: 0.7972 - recall: 0.9108\n",
            "Epoch 47/50\n",
            "46/46 [==============================] - 0s 3ms/step - loss: 0.3586 - accuracy: 0.8457 - precision: 0.8053 - recall: 0.9125\n",
            "Epoch 48/50\n",
            "46/46 [==============================] - 0s 3ms/step - loss: 0.3605 - accuracy: 0.8455 - precision: 0.8052 - recall: 0.9092\n",
            "Epoch 49/50\n",
            "46/46 [==============================] - 0s 3ms/step - loss: 0.3533 - accuracy: 0.8492 - precision: 0.8112 - recall: 0.9106\n",
            "Epoch 50/50\n",
            "46/46 [==============================] - 0s 3ms/step - loss: 0.3464 - accuracy: 0.8521 - precision: 0.8118 - recall: 0.9119\n",
            "23/23 [==============================] - 0s 2ms/step - loss: 0.3620 - accuracy: 0.8479 - precision: 0.8382 - recall: 0.8756\n",
            "[CV 2/3] END .........batch_size=100, epochs=50;, score=0.848 total time=  12.2s\n",
            "Model: \"sequential_13\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " dense_78 (Dense)            (None, 32)                352       \n",
            "                                                                 \n",
            " batch_normalization_45 (Bat  (None, 32)               128       \n",
            " chNormalization)                                                \n",
            "                                                                 \n",
            " dense_79 (Dense)            (None, 16)                528       \n",
            "                                                                 \n",
            " batch_normalization_46 (Bat  (None, 16)               64        \n",
            " chNormalization)                                                \n",
            "                                                                 \n",
            " dense_80 (Dense)            (None, 8)                 136       \n",
            "                                                                 \n",
            " batch_normalization_47 (Bat  (None, 8)                32        \n",
            " chNormalization)                                                \n",
            "                                                                 \n",
            " dense_81 (Dense)            (None, 4)                 36        \n",
            "                                                                 \n",
            " batch_normalization_48 (Bat  (None, 4)                16        \n",
            " chNormalization)                                                \n",
            "                                                                 \n",
            " dense_82 (Dense)            (None, 2)                 10        \n",
            "                                                                 \n",
            " batch_normalization_49 (Bat  (None, 2)                8         \n",
            " chNormalization)                                                \n",
            "                                                                 \n",
            " dense_83 (Dense)            (None, 1)                 3         \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 1,313\n",
            "Trainable params: 1,189\n",
            "Non-trainable params: 124\n",
            "_________________________________________________________________\n",
            "None\n",
            "Epoch 1/50\n",
            "46/46 [==============================] - 2s 3ms/step - loss: 0.7081 - accuracy: 0.5620 - precision: 0.5869 - recall: 0.5387\n",
            "Epoch 2/50\n",
            "46/46 [==============================] - 0s 3ms/step - loss: 0.6036 - accuracy: 0.6440 - precision: 0.7380 - recall: 0.5103\n",
            "Epoch 3/50\n",
            "46/46 [==============================] - 0s 3ms/step - loss: 0.5509 - accuracy: 0.6839 - precision: 0.7235 - recall: 0.6952\n",
            "Epoch 4/50\n",
            "46/46 [==============================] - 0s 3ms/step - loss: 0.5120 - accuracy: 0.7355 - precision: 0.6818 - recall: 0.8919\n",
            "Epoch 5/50\n",
            "46/46 [==============================] - 0s 3ms/step - loss: 0.4858 - accuracy: 0.7688 - precision: 0.7123 - recall: 0.9176\n",
            "Epoch 6/50\n",
            "46/46 [==============================] - 0s 3ms/step - loss: 0.4598 - accuracy: 0.7816 - precision: 0.7257 - recall: 0.9127\n",
            "Epoch 7/50\n",
            "46/46 [==============================] - 0s 3ms/step - loss: 0.4539 - accuracy: 0.7926 - precision: 0.7403 - recall: 0.9093\n",
            "Epoch 8/50\n",
            "46/46 [==============================] - 0s 3ms/step - loss: 0.4363 - accuracy: 0.8052 - precision: 0.7520 - recall: 0.9193\n",
            "Epoch 9/50\n",
            "46/46 [==============================] - 0s 4ms/step - loss: 0.4337 - accuracy: 0.8025 - precision: 0.7536 - recall: 0.9038\n",
            "Epoch 10/50\n",
            "46/46 [==============================] - 0s 4ms/step - loss: 0.4130 - accuracy: 0.8177 - precision: 0.7724 - recall: 0.9116\n",
            "Epoch 11/50\n",
            "46/46 [==============================] - 0s 3ms/step - loss: 0.4180 - accuracy: 0.8091 - precision: 0.7613 - recall: 0.9072\n",
            "Epoch 12/50\n",
            "46/46 [==============================] - 0s 4ms/step - loss: 0.4074 - accuracy: 0.8118 - precision: 0.7670 - recall: 0.9039\n",
            "Epoch 13/50\n",
            "46/46 [==============================] - 0s 3ms/step - loss: 0.4061 - accuracy: 0.8124 - precision: 0.7672 - recall: 0.9080\n",
            "Epoch 14/50\n",
            "46/46 [==============================] - 0s 3ms/step - loss: 0.4014 - accuracy: 0.8173 - precision: 0.7755 - recall: 0.9027\n",
            "Epoch 15/50\n",
            "46/46 [==============================] - 0s 4ms/step - loss: 0.3954 - accuracy: 0.8239 - precision: 0.7741 - recall: 0.9176\n",
            "Epoch 16/50\n",
            "46/46 [==============================] - 0s 4ms/step - loss: 0.3952 - accuracy: 0.8217 - precision: 0.7741 - recall: 0.9148\n",
            "Epoch 17/50\n",
            "46/46 [==============================] - 0s 3ms/step - loss: 0.3871 - accuracy: 0.8201 - precision: 0.7744 - recall: 0.9130\n",
            "Epoch 18/50\n",
            "46/46 [==============================] - 0s 3ms/step - loss: 0.3860 - accuracy: 0.8261 - precision: 0.7834 - recall: 0.9076\n",
            "Epoch 19/50\n",
            "46/46 [==============================] - 0s 3ms/step - loss: 0.3781 - accuracy: 0.8307 - precision: 0.7861 - recall: 0.9139\n",
            "Epoch 20/50\n",
            "46/46 [==============================] - 0s 3ms/step - loss: 0.3772 - accuracy: 0.8343 - precision: 0.7884 - recall: 0.9171\n",
            "Epoch 21/50\n",
            "46/46 [==============================] - 0s 4ms/step - loss: 0.3656 - accuracy: 0.8369 - precision: 0.7929 - recall: 0.9203\n",
            "Epoch 22/50\n",
            "46/46 [==============================] - 0s 3ms/step - loss: 0.3792 - accuracy: 0.8294 - precision: 0.7857 - recall: 0.9147\n",
            "Epoch 23/50\n",
            "46/46 [==============================] - 0s 3ms/step - loss: 0.3643 - accuracy: 0.8369 - precision: 0.7945 - recall: 0.9122\n",
            "Epoch 24/50\n",
            "46/46 [==============================] - 0s 3ms/step - loss: 0.3711 - accuracy: 0.8298 - precision: 0.7863 - recall: 0.9111\n",
            "Epoch 25/50\n",
            "46/46 [==============================] - 0s 3ms/step - loss: 0.3632 - accuracy: 0.8384 - precision: 0.7950 - recall: 0.9196\n",
            "Epoch 26/50\n",
            "46/46 [==============================] - 0s 3ms/step - loss: 0.3585 - accuracy: 0.8371 - precision: 0.7957 - recall: 0.9167\n",
            "Epoch 27/50\n",
            "46/46 [==============================] - 0s 3ms/step - loss: 0.3644 - accuracy: 0.8398 - precision: 0.7942 - recall: 0.9276\n",
            "Epoch 28/50\n",
            "46/46 [==============================] - 0s 3ms/step - loss: 0.3578 - accuracy: 0.8411 - precision: 0.7984 - recall: 0.9228\n",
            "Epoch 29/50\n",
            "46/46 [==============================] - 0s 3ms/step - loss: 0.3523 - accuracy: 0.8420 - precision: 0.7997 - recall: 0.9229\n",
            "Epoch 30/50\n",
            "46/46 [==============================] - 0s 3ms/step - loss: 0.3483 - accuracy: 0.8431 - precision: 0.8045 - recall: 0.9202\n",
            "Epoch 31/50\n",
            "46/46 [==============================] - 0s 3ms/step - loss: 0.3483 - accuracy: 0.8453 - precision: 0.8022 - recall: 0.9210\n",
            "Epoch 32/50\n",
            "46/46 [==============================] - 0s 3ms/step - loss: 0.3477 - accuracy: 0.8437 - precision: 0.7985 - recall: 0.9256\n",
            "Epoch 33/50\n",
            "46/46 [==============================] - 0s 3ms/step - loss: 0.3475 - accuracy: 0.8444 - precision: 0.8019 - recall: 0.9229\n",
            "Epoch 34/50\n",
            "46/46 [==============================] - 0s 4ms/step - loss: 0.3502 - accuracy: 0.8387 - precision: 0.8026 - recall: 0.9114\n",
            "Epoch 35/50\n",
            "46/46 [==============================] - 0s 3ms/step - loss: 0.3454 - accuracy: 0.8426 - precision: 0.8069 - recall: 0.9124\n",
            "Epoch 36/50\n",
            "46/46 [==============================] - 0s 3ms/step - loss: 0.3402 - accuracy: 0.8501 - precision: 0.8058 - recall: 0.9342\n",
            "Epoch 37/50\n",
            "46/46 [==============================] - 0s 3ms/step - loss: 0.3406 - accuracy: 0.8448 - precision: 0.8001 - recall: 0.9261\n",
            "Epoch 38/50\n",
            "46/46 [==============================] - 0s 4ms/step - loss: 0.3384 - accuracy: 0.8501 - precision: 0.8025 - recall: 0.9353\n",
            "Epoch 39/50\n",
            "46/46 [==============================] - 0s 3ms/step - loss: 0.3412 - accuracy: 0.8521 - precision: 0.8083 - recall: 0.9270\n",
            "Epoch 40/50\n",
            "46/46 [==============================] - 0s 3ms/step - loss: 0.3285 - accuracy: 0.8563 - precision: 0.8096 - recall: 0.9369\n",
            "Epoch 41/50\n",
            "46/46 [==============================] - 0s 3ms/step - loss: 0.3406 - accuracy: 0.8417 - precision: 0.8002 - recall: 0.9196\n",
            "Epoch 42/50\n",
            "46/46 [==============================] - 0s 3ms/step - loss: 0.3317 - accuracy: 0.8563 - precision: 0.8167 - recall: 0.9254\n",
            "Epoch 43/50\n",
            "46/46 [==============================] - 0s 3ms/step - loss: 0.3239 - accuracy: 0.8585 - precision: 0.8201 - recall: 0.9265\n",
            "Epoch 44/50\n",
            "46/46 [==============================] - 0s 3ms/step - loss: 0.3248 - accuracy: 0.8574 - precision: 0.8168 - recall: 0.9280\n",
            "Epoch 45/50\n",
            "46/46 [==============================] - 0s 3ms/step - loss: 0.3287 - accuracy: 0.8539 - precision: 0.8092 - recall: 0.9313\n",
            "Epoch 46/50\n",
            "46/46 [==============================] - 0s 3ms/step - loss: 0.3188 - accuracy: 0.8625 - precision: 0.8178 - recall: 0.9353\n",
            "Epoch 47/50\n",
            "46/46 [==============================] - 0s 4ms/step - loss: 0.3290 - accuracy: 0.8547 - precision: 0.8112 - recall: 0.9322\n",
            "Epoch 48/50\n",
            "46/46 [==============================] - 0s 3ms/step - loss: 0.3264 - accuracy: 0.8556 - precision: 0.8086 - recall: 0.9353\n",
            "Epoch 49/50\n",
            "46/46 [==============================] - 0s 3ms/step - loss: 0.3240 - accuracy: 0.8576 - precision: 0.8172 - recall: 0.9279\n",
            "Epoch 50/50\n",
            "46/46 [==============================] - 0s 5ms/step - loss: 0.3224 - accuracy: 0.8633 - precision: 0.8167 - recall: 0.9406\n",
            "23/23 [==============================] - 0s 4ms/step - loss: 0.3425 - accuracy: 0.8541 - precision: 0.7954 - recall: 0.9513\n",
            "[CV 3/3] END .........batch_size=100, epochs=50;, score=0.854 total time=  10.9s\n",
            "Model: \"sequential_14\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " dense_84 (Dense)            (None, 32)                352       \n",
            "                                                                 \n",
            " batch_normalization_50 (Bat  (None, 32)               128       \n",
            " chNormalization)                                                \n",
            "                                                                 \n",
            " dense_85 (Dense)            (None, 16)                528       \n",
            "                                                                 \n",
            " batch_normalization_51 (Bat  (None, 16)               64        \n",
            " chNormalization)                                                \n",
            "                                                                 \n",
            " dense_86 (Dense)            (None, 8)                 136       \n",
            "                                                                 \n",
            " batch_normalization_52 (Bat  (None, 8)                32        \n",
            " chNormalization)                                                \n",
            "                                                                 \n",
            " dense_87 (Dense)            (None, 4)                 36        \n",
            "                                                                 \n",
            " batch_normalization_53 (Bat  (None, 4)                16        \n",
            " chNormalization)                                                \n",
            "                                                                 \n",
            " dense_88 (Dense)            (None, 2)                 10        \n",
            "                                                                 \n",
            " batch_normalization_54 (Bat  (None, 2)                8         \n",
            " chNormalization)                                                \n",
            "                                                                 \n",
            " dense_89 (Dense)            (None, 1)                 3         \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 1,313\n",
            "Trainable params: 1,189\n",
            "Non-trainable params: 124\n",
            "_________________________________________________________________\n",
            "None\n",
            "Epoch 1/100\n",
            "46/46 [==============================] - 2s 3ms/step - loss: 0.7313 - accuracy: 0.5988 - precision: 0.6418 - recall: 0.4732\n",
            "Epoch 2/100\n",
            "46/46 [==============================] - 0s 3ms/step - loss: 0.5833 - accuracy: 0.6898 - precision: 0.7330 - recall: 0.6207\n",
            "Epoch 3/100\n",
            "46/46 [==============================] - 0s 3ms/step - loss: 0.5299 - accuracy: 0.7324 - precision: 0.7311 - recall: 0.7627\n",
            "Epoch 4/100\n",
            "46/46 [==============================] - 0s 3ms/step - loss: 0.5015 - accuracy: 0.7599 - precision: 0.7415 - recall: 0.8161\n",
            "Epoch 5/100\n",
            "46/46 [==============================] - 0s 4ms/step - loss: 0.4807 - accuracy: 0.7828 - precision: 0.7591 - recall: 0.8408\n",
            "Epoch 6/100\n",
            "46/46 [==============================] - 0s 4ms/step - loss: 0.4749 - accuracy: 0.7868 - precision: 0.7617 - recall: 0.8436\n",
            "Epoch 7/100\n",
            "46/46 [==============================] - 0s 4ms/step - loss: 0.4588 - accuracy: 0.7930 - precision: 0.7649 - recall: 0.8576\n",
            "Epoch 8/100\n",
            "46/46 [==============================] - 0s 4ms/step - loss: 0.4473 - accuracy: 0.8031 - precision: 0.7793 - recall: 0.8597\n",
            "Epoch 9/100\n",
            "46/46 [==============================] - 0s 3ms/step - loss: 0.4410 - accuracy: 0.8053 - precision: 0.7829 - recall: 0.8559\n",
            "Epoch 10/100\n",
            "46/46 [==============================] - 0s 3ms/step - loss: 0.4290 - accuracy: 0.8089 - precision: 0.7822 - recall: 0.8682\n",
            "Epoch 11/100\n",
            "46/46 [==============================] - 0s 4ms/step - loss: 0.4289 - accuracy: 0.8084 - precision: 0.7752 - recall: 0.8758\n",
            "Epoch 12/100\n",
            "46/46 [==============================] - 0s 3ms/step - loss: 0.4227 - accuracy: 0.8119 - precision: 0.7807 - recall: 0.8721\n",
            "Epoch 13/100\n",
            "46/46 [==============================] - 0s 3ms/step - loss: 0.4177 - accuracy: 0.8117 - precision: 0.7804 - recall: 0.8791\n",
            "Epoch 14/100\n",
            "46/46 [==============================] - 0s 3ms/step - loss: 0.4164 - accuracy: 0.8086 - precision: 0.7733 - recall: 0.8754\n",
            "Epoch 15/100\n",
            "46/46 [==============================] - 0s 4ms/step - loss: 0.4024 - accuracy: 0.8216 - precision: 0.7895 - recall: 0.8823\n",
            "Epoch 16/100\n",
            "46/46 [==============================] - 0s 4ms/step - loss: 0.4061 - accuracy: 0.8164 - precision: 0.7835 - recall: 0.8838\n",
            "Epoch 17/100\n",
            "46/46 [==============================] - 0s 3ms/step - loss: 0.4094 - accuracy: 0.8148 - precision: 0.7830 - recall: 0.8794\n",
            "Epoch 18/100\n",
            "46/46 [==============================] - 0s 3ms/step - loss: 0.4012 - accuracy: 0.8199 - precision: 0.7889 - recall: 0.8801\n",
            "Epoch 19/100\n",
            "46/46 [==============================] - 0s 3ms/step - loss: 0.3977 - accuracy: 0.8272 - precision: 0.7938 - recall: 0.8955\n",
            "Epoch 20/100\n",
            "46/46 [==============================] - 0s 3ms/step - loss: 0.3973 - accuracy: 0.8241 - precision: 0.7939 - recall: 0.8874\n",
            "Epoch 21/100\n",
            "46/46 [==============================] - 0s 3ms/step - loss: 0.3968 - accuracy: 0.8307 - precision: 0.7977 - recall: 0.8911\n",
            "Epoch 22/100\n",
            "46/46 [==============================] - 0s 3ms/step - loss: 0.3899 - accuracy: 0.8254 - precision: 0.7919 - recall: 0.8891\n",
            "Epoch 23/100\n",
            "46/46 [==============================] - 0s 3ms/step - loss: 0.3928 - accuracy: 0.8247 - precision: 0.7969 - recall: 0.8786\n",
            "Epoch 24/100\n",
            "46/46 [==============================] - 0s 3ms/step - loss: 0.3949 - accuracy: 0.8232 - precision: 0.7856 - recall: 0.8935\n",
            "Epoch 25/100\n",
            "46/46 [==============================] - 0s 4ms/step - loss: 0.3869 - accuracy: 0.8307 - precision: 0.7980 - recall: 0.8934\n",
            "Epoch 26/100\n",
            "46/46 [==============================] - 0s 3ms/step - loss: 0.3790 - accuracy: 0.8369 - precision: 0.8060 - recall: 0.8933\n",
            "Epoch 27/100\n",
            "46/46 [==============================] - 0s 3ms/step - loss: 0.3840 - accuracy: 0.8358 - precision: 0.8057 - recall: 0.8923\n",
            "Epoch 28/100\n",
            "46/46 [==============================] - 0s 3ms/step - loss: 0.3836 - accuracy: 0.8278 - precision: 0.7956 - recall: 0.8938\n",
            "Epoch 29/100\n",
            "46/46 [==============================] - 0s 3ms/step - loss: 0.3803 - accuracy: 0.8384 - precision: 0.8092 - recall: 0.8932\n",
            "Epoch 30/100\n",
            "46/46 [==============================] - 0s 3ms/step - loss: 0.3816 - accuracy: 0.8375 - precision: 0.8049 - recall: 0.8979\n",
            "Epoch 31/100\n",
            "46/46 [==============================] - 0s 3ms/step - loss: 0.3722 - accuracy: 0.8384 - precision: 0.8052 - recall: 0.8967\n",
            "Epoch 32/100\n",
            "46/46 [==============================] - 0s 4ms/step - loss: 0.3791 - accuracy: 0.8302 - precision: 0.7970 - recall: 0.8927\n",
            "Epoch 33/100\n",
            "46/46 [==============================] - 0s 3ms/step - loss: 0.3706 - accuracy: 0.8375 - precision: 0.8067 - recall: 0.8971\n",
            "Epoch 34/100\n",
            "46/46 [==============================] - 0s 3ms/step - loss: 0.3687 - accuracy: 0.8419 - precision: 0.8100 - recall: 0.8980\n",
            "Epoch 35/100\n",
            "46/46 [==============================] - 0s 4ms/step - loss: 0.3715 - accuracy: 0.8404 - precision: 0.8083 - recall: 0.8945\n",
            "Epoch 36/100\n",
            "46/46 [==============================] - 0s 3ms/step - loss: 0.3691 - accuracy: 0.8415 - precision: 0.8090 - recall: 0.8991\n",
            "Epoch 37/100\n",
            "46/46 [==============================] - 0s 3ms/step - loss: 0.3678 - accuracy: 0.8399 - precision: 0.8108 - recall: 0.8912\n",
            "Epoch 38/100\n",
            "46/46 [==============================] - 0s 3ms/step - loss: 0.3655 - accuracy: 0.8450 - precision: 0.8112 - recall: 0.9069\n",
            "Epoch 39/100\n",
            "46/46 [==============================] - 0s 3ms/step - loss: 0.3609 - accuracy: 0.8470 - precision: 0.8150 - recall: 0.9057\n",
            "Epoch 40/100\n",
            "46/46 [==============================] - 0s 3ms/step - loss: 0.3536 - accuracy: 0.8512 - precision: 0.8222 - recall: 0.9038\n",
            "Epoch 41/100\n",
            "46/46 [==============================] - 0s 4ms/step - loss: 0.3543 - accuracy: 0.8459 - precision: 0.8175 - recall: 0.8991\n",
            "Epoch 42/100\n",
            "46/46 [==============================] - 0s 3ms/step - loss: 0.3643 - accuracy: 0.8417 - precision: 0.8117 - recall: 0.8965\n",
            "Epoch 43/100\n",
            "46/46 [==============================] - 0s 3ms/step - loss: 0.3480 - accuracy: 0.8558 - precision: 0.8324 - recall: 0.8981\n",
            "Epoch 44/100\n",
            "46/46 [==============================] - 0s 4ms/step - loss: 0.3583 - accuracy: 0.8448 - precision: 0.8157 - recall: 0.8998\n",
            "Epoch 45/100\n",
            "46/46 [==============================] - 0s 3ms/step - loss: 0.3499 - accuracy: 0.8536 - precision: 0.8256 - recall: 0.9062\n",
            "Epoch 46/100\n",
            "46/46 [==============================] - 0s 3ms/step - loss: 0.3514 - accuracy: 0.8516 - precision: 0.8210 - recall: 0.9055\n",
            "Epoch 47/100\n",
            "46/46 [==============================] - 0s 3ms/step - loss: 0.3423 - accuracy: 0.8556 - precision: 0.8275 - recall: 0.9023\n",
            "Epoch 48/100\n",
            "46/46 [==============================] - 0s 3ms/step - loss: 0.3506 - accuracy: 0.8543 - precision: 0.8229 - recall: 0.9116\n",
            "Epoch 49/100\n",
            "46/46 [==============================] - 0s 3ms/step - loss: 0.3675 - accuracy: 0.8439 - precision: 0.8151 - recall: 0.9020\n",
            "Epoch 50/100\n",
            "46/46 [==============================] - 0s 3ms/step - loss: 0.3535 - accuracy: 0.8532 - precision: 0.8256 - recall: 0.8974\n",
            "Epoch 51/100\n",
            "46/46 [==============================] - 0s 3ms/step - loss: 0.3456 - accuracy: 0.8563 - precision: 0.8339 - recall: 0.8949\n",
            "Epoch 52/100\n",
            "46/46 [==============================] - 0s 3ms/step - loss: 0.3450 - accuracy: 0.8543 - precision: 0.8264 - recall: 0.9015\n",
            "Epoch 53/100\n",
            "46/46 [==============================] - 0s 3ms/step - loss: 0.3453 - accuracy: 0.8574 - precision: 0.8304 - recall: 0.9072\n",
            "Epoch 54/100\n",
            "46/46 [==============================] - 0s 3ms/step - loss: 0.3486 - accuracy: 0.8532 - precision: 0.8282 - recall: 0.8972\n",
            "Epoch 55/100\n",
            "46/46 [==============================] - 0s 3ms/step - loss: 0.3451 - accuracy: 0.8600 - precision: 0.8321 - recall: 0.9058\n",
            "Epoch 56/100\n",
            "46/46 [==============================] - 0s 4ms/step - loss: 0.3376 - accuracy: 0.8646 - precision: 0.8387 - recall: 0.9109\n",
            "Epoch 57/100\n",
            "46/46 [==============================] - 0s 3ms/step - loss: 0.3349 - accuracy: 0.8664 - precision: 0.8440 - recall: 0.9035\n",
            "Epoch 58/100\n",
            "46/46 [==============================] - 0s 3ms/step - loss: 0.3300 - accuracy: 0.8660 - precision: 0.8394 - recall: 0.9121\n",
            "Epoch 59/100\n",
            "46/46 [==============================] - 0s 4ms/step - loss: 0.3357 - accuracy: 0.8589 - precision: 0.8321 - recall: 0.9058\n",
            "Epoch 60/100\n",
            "46/46 [==============================] - 0s 3ms/step - loss: 0.3348 - accuracy: 0.8622 - precision: 0.8389 - recall: 0.9066\n",
            "Epoch 61/100\n",
            "46/46 [==============================] - 0s 3ms/step - loss: 0.3467 - accuracy: 0.8543 - precision: 0.8206 - recall: 0.9047\n",
            "Epoch 62/100\n",
            "46/46 [==============================] - 0s 3ms/step - loss: 0.3343 - accuracy: 0.8695 - precision: 0.8379 - recall: 0.9182\n",
            "Epoch 63/100\n",
            "46/46 [==============================] - 0s 3ms/step - loss: 0.3339 - accuracy: 0.8620 - precision: 0.8341 - recall: 0.9073\n",
            "Epoch 64/100\n",
            "46/46 [==============================] - 0s 3ms/step - loss: 0.3366 - accuracy: 0.8585 - precision: 0.8323 - recall: 0.9062\n",
            "Epoch 65/100\n",
            "46/46 [==============================] - 0s 4ms/step - loss: 0.3287 - accuracy: 0.8660 - precision: 0.8414 - recall: 0.9087\n",
            "Epoch 66/100\n",
            "46/46 [==============================] - 0s 3ms/step - loss: 0.3249 - accuracy: 0.8664 - precision: 0.8372 - recall: 0.9162\n",
            "Epoch 67/100\n",
            "46/46 [==============================] - 0s 3ms/step - loss: 0.3313 - accuracy: 0.8611 - precision: 0.8363 - recall: 0.9050\n",
            "Epoch 68/100\n",
            "46/46 [==============================] - 0s 3ms/step - loss: 0.3324 - accuracy: 0.8690 - precision: 0.8453 - recall: 0.9112\n",
            "Epoch 69/100\n",
            "46/46 [==============================] - 0s 3ms/step - loss: 0.3194 - accuracy: 0.8616 - precision: 0.8384 - recall: 0.9037\n",
            "Epoch 70/100\n",
            "46/46 [==============================] - 0s 3ms/step - loss: 0.3195 - accuracy: 0.8649 - precision: 0.8383 - recall: 0.9078\n",
            "Epoch 71/100\n",
            "46/46 [==============================] - 0s 3ms/step - loss: 0.3250 - accuracy: 0.8679 - precision: 0.8433 - recall: 0.9139\n",
            "Epoch 72/100\n",
            "46/46 [==============================] - 0s 3ms/step - loss: 0.3204 - accuracy: 0.8664 - precision: 0.8430 - recall: 0.9087\n",
            "Epoch 73/100\n",
            "46/46 [==============================] - 0s 3ms/step - loss: 0.3234 - accuracy: 0.8662 - precision: 0.8418 - recall: 0.9116\n",
            "Epoch 74/100\n",
            "46/46 [==============================] - 0s 3ms/step - loss: 0.3187 - accuracy: 0.8660 - precision: 0.8397 - recall: 0.9141\n",
            "Epoch 75/100\n",
            "46/46 [==============================] - 0s 3ms/step - loss: 0.3141 - accuracy: 0.8750 - precision: 0.8476 - recall: 0.9183\n",
            "Epoch 76/100\n",
            "46/46 [==============================] - 0s 3ms/step - loss: 0.3135 - accuracy: 0.8715 - precision: 0.8460 - recall: 0.9158\n",
            "Epoch 77/100\n",
            "46/46 [==============================] - 0s 4ms/step - loss: 0.3071 - accuracy: 0.8823 - precision: 0.8559 - recall: 0.9252\n",
            "Epoch 78/100\n",
            "46/46 [==============================] - 0s 3ms/step - loss: 0.3205 - accuracy: 0.8633 - precision: 0.8404 - recall: 0.9012\n",
            "Epoch 79/100\n",
            "46/46 [==============================] - 0s 3ms/step - loss: 0.3136 - accuracy: 0.8699 - precision: 0.8377 - recall: 0.9211\n",
            "Epoch 80/100\n",
            "46/46 [==============================] - 0s 3ms/step - loss: 0.3173 - accuracy: 0.8721 - precision: 0.8469 - recall: 0.9157\n",
            "Epoch 81/100\n",
            "46/46 [==============================] - 0s 3ms/step - loss: 0.3176 - accuracy: 0.8704 - precision: 0.8398 - recall: 0.9144\n",
            "Epoch 82/100\n",
            "46/46 [==============================] - 0s 3ms/step - loss: 0.3101 - accuracy: 0.8701 - precision: 0.8482 - recall: 0.9065\n",
            "Epoch 83/100\n",
            "46/46 [==============================] - 0s 3ms/step - loss: 0.3099 - accuracy: 0.8737 - precision: 0.8437 - recall: 0.9218\n",
            "Epoch 84/100\n",
            "46/46 [==============================] - 0s 3ms/step - loss: 0.3024 - accuracy: 0.8757 - precision: 0.8501 - recall: 0.9159\n",
            "Epoch 85/100\n",
            "46/46 [==============================] - 0s 3ms/step - loss: 0.3154 - accuracy: 0.8660 - precision: 0.8449 - recall: 0.8992\n",
            "Epoch 86/100\n",
            "46/46 [==============================] - 0s 3ms/step - loss: 0.3082 - accuracy: 0.8695 - precision: 0.8438 - recall: 0.9111\n",
            "Epoch 87/100\n",
            "46/46 [==============================] - 0s 4ms/step - loss: 0.3082 - accuracy: 0.8752 - precision: 0.8516 - recall: 0.9151\n",
            "Epoch 88/100\n",
            "46/46 [==============================] - 0s 3ms/step - loss: 0.3007 - accuracy: 0.8761 - precision: 0.8474 - recall: 0.9222\n",
            "Epoch 89/100\n",
            "46/46 [==============================] - 0s 3ms/step - loss: 0.3003 - accuracy: 0.8814 - precision: 0.8539 - recall: 0.9227\n",
            "Epoch 90/100\n",
            "46/46 [==============================] - 0s 3ms/step - loss: 0.3054 - accuracy: 0.8737 - precision: 0.8441 - recall: 0.9162\n",
            "Epoch 91/100\n",
            "46/46 [==============================] - 0s 4ms/step - loss: 0.3050 - accuracy: 0.8746 - precision: 0.8529 - recall: 0.9131\n",
            "Epoch 92/100\n",
            "46/46 [==============================] - 0s 3ms/step - loss: 0.3052 - accuracy: 0.8746 - precision: 0.8493 - recall: 0.9136\n",
            "Epoch 93/100\n",
            "46/46 [==============================] - 0s 3ms/step - loss: 0.3002 - accuracy: 0.8772 - precision: 0.8504 - recall: 0.9158\n",
            "Epoch 94/100\n",
            "46/46 [==============================] - 0s 3ms/step - loss: 0.2995 - accuracy: 0.8763 - precision: 0.8528 - recall: 0.9187\n",
            "Epoch 95/100\n",
            "46/46 [==============================] - 0s 3ms/step - loss: 0.2983 - accuracy: 0.8785 - precision: 0.8500 - recall: 0.9228\n",
            "Epoch 96/100\n",
            "46/46 [==============================] - 0s 3ms/step - loss: 0.2999 - accuracy: 0.8792 - precision: 0.8508 - recall: 0.9231\n",
            "Epoch 97/100\n",
            "46/46 [==============================] - 0s 3ms/step - loss: 0.2989 - accuracy: 0.8818 - precision: 0.8557 - recall: 0.9195\n",
            "Epoch 98/100\n",
            "46/46 [==============================] - 0s 4ms/step - loss: 0.2978 - accuracy: 0.8732 - precision: 0.8537 - recall: 0.9077\n",
            "Epoch 99/100\n",
            "46/46 [==============================] - 0s 3ms/step - loss: 0.2912 - accuracy: 0.8783 - precision: 0.8495 - recall: 0.9178\n",
            "Epoch 100/100\n",
            "46/46 [==============================] - 0s 3ms/step - loss: 0.2967 - accuracy: 0.8763 - precision: 0.8505 - recall: 0.9188\n",
            "23/23 [==============================] - 0s 2ms/step - loss: 0.3509 - accuracy: 0.8647 - precision: 0.8233 - recall: 0.9266\n",
            "[CV 1/3] END ........batch_size=100, epochs=100;, score=0.865 total time=  19.3s\n",
            "Model: \"sequential_15\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " dense_90 (Dense)            (None, 32)                352       \n",
            "                                                                 \n",
            " batch_normalization_55 (Bat  (None, 32)               128       \n",
            " chNormalization)                                                \n",
            "                                                                 \n",
            " dense_91 (Dense)            (None, 16)                528       \n",
            "                                                                 \n",
            " batch_normalization_56 (Bat  (None, 16)               64        \n",
            " chNormalization)                                                \n",
            "                                                                 \n",
            " dense_92 (Dense)            (None, 8)                 136       \n",
            "                                                                 \n",
            " batch_normalization_57 (Bat  (None, 8)                32        \n",
            " chNormalization)                                                \n",
            "                                                                 \n",
            " dense_93 (Dense)            (None, 4)                 36        \n",
            "                                                                 \n",
            " batch_normalization_58 (Bat  (None, 4)                16        \n",
            " chNormalization)                                                \n",
            "                                                                 \n",
            " dense_94 (Dense)            (None, 2)                 10        \n",
            "                                                                 \n",
            " batch_normalization_59 (Bat  (None, 2)                8         \n",
            " chNormalization)                                                \n",
            "                                                                 \n",
            " dense_95 (Dense)            (None, 1)                 3         \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 1,313\n",
            "Trainable params: 1,189\n",
            "Non-trainable params: 124\n",
            "_________________________________________________________________\n",
            "None\n",
            "Epoch 1/100\n",
            "46/46 [==============================] - 2s 3ms/step - loss: 0.7056 - accuracy: 0.5466 - precision: 0.5363 - recall: 0.6417\n",
            "Epoch 2/100\n",
            "46/46 [==============================] - 0s 3ms/step - loss: 0.6032 - accuracy: 0.6696 - precision: 0.6285 - recall: 0.8290\n",
            "Epoch 3/100\n",
            "46/46 [==============================] - 0s 4ms/step - loss: 0.5489 - accuracy: 0.7472 - precision: 0.6938 - recall: 0.8784\n",
            "Epoch 4/100\n",
            "46/46 [==============================] - 0s 3ms/step - loss: 0.5147 - accuracy: 0.7637 - precision: 0.7184 - recall: 0.8621\n",
            "Epoch 5/100\n",
            "46/46 [==============================] - 0s 3ms/step - loss: 0.4929 - accuracy: 0.7800 - precision: 0.7387 - recall: 0.8606\n",
            "Epoch 6/100\n",
            "46/46 [==============================] - 0s 3ms/step - loss: 0.4800 - accuracy: 0.7858 - precision: 0.7446 - recall: 0.8682\n",
            "Epoch 7/100\n",
            "46/46 [==============================] - 0s 3ms/step - loss: 0.4607 - accuracy: 0.7963 - precision: 0.7505 - recall: 0.8775\n",
            "Epoch 8/100\n",
            "46/46 [==============================] - 0s 3ms/step - loss: 0.4500 - accuracy: 0.8036 - precision: 0.7574 - recall: 0.8918\n",
            "Epoch 9/100\n",
            "46/46 [==============================] - 0s 3ms/step - loss: 0.4419 - accuracy: 0.8010 - precision: 0.7599 - recall: 0.8790\n",
            "Epoch 10/100\n",
            "46/46 [==============================] - 0s 3ms/step - loss: 0.4280 - accuracy: 0.8144 - precision: 0.7698 - recall: 0.8935\n",
            "Epoch 11/100\n",
            "46/46 [==============================] - 0s 3ms/step - loss: 0.4229 - accuracy: 0.8164 - precision: 0.7749 - recall: 0.8916\n",
            "Epoch 12/100\n",
            "46/46 [==============================] - 0s 3ms/step - loss: 0.4192 - accuracy: 0.8149 - precision: 0.7722 - recall: 0.8913\n",
            "Epoch 13/100\n",
            "46/46 [==============================] - 0s 4ms/step - loss: 0.4090 - accuracy: 0.8226 - precision: 0.7834 - recall: 0.8899\n",
            "Epoch 14/100\n",
            "46/46 [==============================] - 0s 4ms/step - loss: 0.4121 - accuracy: 0.8204 - precision: 0.7762 - recall: 0.8934\n",
            "Epoch 15/100\n",
            "46/46 [==============================] - 0s 3ms/step - loss: 0.4021 - accuracy: 0.8265 - precision: 0.7790 - recall: 0.9062\n",
            "Epoch 16/100\n",
            "46/46 [==============================] - 0s 3ms/step - loss: 0.4025 - accuracy: 0.8285 - precision: 0.7832 - recall: 0.9051\n",
            "Epoch 17/100\n",
            "46/46 [==============================] - 0s 3ms/step - loss: 0.3920 - accuracy: 0.8312 - precision: 0.7897 - recall: 0.9031\n",
            "Epoch 18/100\n",
            "46/46 [==============================] - 0s 4ms/step - loss: 0.3847 - accuracy: 0.8309 - precision: 0.7926 - recall: 0.8942\n",
            "Epoch 19/100\n",
            "46/46 [==============================] - 0s 3ms/step - loss: 0.3883 - accuracy: 0.8356 - precision: 0.7934 - recall: 0.8984\n",
            "Epoch 20/100\n",
            "46/46 [==============================] - 0s 3ms/step - loss: 0.3821 - accuracy: 0.8413 - precision: 0.8029 - recall: 0.9009\n",
            "Epoch 21/100\n",
            "46/46 [==============================] - 0s 3ms/step - loss: 0.3784 - accuracy: 0.8398 - precision: 0.8024 - recall: 0.8990\n",
            "Epoch 22/100\n",
            "46/46 [==============================] - 0s 4ms/step - loss: 0.3885 - accuracy: 0.8323 - precision: 0.7965 - recall: 0.8901\n",
            "Epoch 23/100\n",
            "46/46 [==============================] - 0s 4ms/step - loss: 0.3782 - accuracy: 0.8373 - precision: 0.7964 - recall: 0.9045\n",
            "Epoch 24/100\n",
            "46/46 [==============================] - 0s 3ms/step - loss: 0.3792 - accuracy: 0.8384 - precision: 0.8030 - recall: 0.9015\n",
            "Epoch 25/100\n",
            "46/46 [==============================] - 0s 3ms/step - loss: 0.3721 - accuracy: 0.8387 - precision: 0.7999 - recall: 0.9008\n",
            "Epoch 26/100\n",
            "46/46 [==============================] - 0s 3ms/step - loss: 0.3665 - accuracy: 0.8442 - precision: 0.8081 - recall: 0.9035\n",
            "Epoch 27/100\n",
            "46/46 [==============================] - 0s 4ms/step - loss: 0.3700 - accuracy: 0.8439 - precision: 0.8036 - recall: 0.9096\n",
            "Epoch 28/100\n",
            "46/46 [==============================] - 0s 3ms/step - loss: 0.3735 - accuracy: 0.8367 - precision: 0.8000 - recall: 0.8968\n",
            "Epoch 29/100\n",
            "46/46 [==============================] - 0s 3ms/step - loss: 0.3599 - accuracy: 0.8470 - precision: 0.8089 - recall: 0.9116\n",
            "Epoch 30/100\n",
            "46/46 [==============================] - 0s 4ms/step - loss: 0.3538 - accuracy: 0.8501 - precision: 0.8134 - recall: 0.9058\n",
            "Epoch 31/100\n",
            "46/46 [==============================] - 0s 3ms/step - loss: 0.3560 - accuracy: 0.8523 - precision: 0.8166 - recall: 0.9060\n",
            "Epoch 32/100\n",
            "46/46 [==============================] - 0s 3ms/step - loss: 0.3509 - accuracy: 0.8574 - precision: 0.8175 - recall: 0.9195\n",
            "Epoch 33/100\n",
            "46/46 [==============================] - 0s 3ms/step - loss: 0.3556 - accuracy: 0.8490 - precision: 0.8158 - recall: 0.8999\n",
            "Epoch 34/100\n",
            "46/46 [==============================] - 0s 4ms/step - loss: 0.3489 - accuracy: 0.8545 - precision: 0.8166 - recall: 0.9143\n",
            "Epoch 35/100\n",
            "46/46 [==============================] - 0s 4ms/step - loss: 0.3475 - accuracy: 0.8556 - precision: 0.8189 - recall: 0.9101\n",
            "Epoch 36/100\n",
            "46/46 [==============================] - 0s 3ms/step - loss: 0.3441 - accuracy: 0.8576 - precision: 0.8171 - recall: 0.9192\n",
            "Epoch 37/100\n",
            "46/46 [==============================] - 0s 3ms/step - loss: 0.3434 - accuracy: 0.8552 - precision: 0.8163 - recall: 0.9121\n",
            "Epoch 38/100\n",
            "46/46 [==============================] - 0s 3ms/step - loss: 0.3430 - accuracy: 0.8543 - precision: 0.8187 - recall: 0.9091\n",
            "Epoch 39/100\n",
            "46/46 [==============================] - 0s 3ms/step - loss: 0.3429 - accuracy: 0.8539 - precision: 0.8163 - recall: 0.9128\n",
            "Epoch 40/100\n",
            "46/46 [==============================] - 0s 4ms/step - loss: 0.3478 - accuracy: 0.8530 - precision: 0.8158 - recall: 0.9107\n",
            "Epoch 41/100\n",
            "46/46 [==============================] - 0s 3ms/step - loss: 0.3344 - accuracy: 0.8576 - precision: 0.8214 - recall: 0.9132\n",
            "Epoch 42/100\n",
            "46/46 [==============================] - 0s 3ms/step - loss: 0.3400 - accuracy: 0.8583 - precision: 0.8253 - recall: 0.9083\n",
            "Epoch 43/100\n",
            "46/46 [==============================] - 0s 3ms/step - loss: 0.3398 - accuracy: 0.8585 - precision: 0.8240 - recall: 0.9090\n",
            "Epoch 44/100\n",
            "46/46 [==============================] - 0s 3ms/step - loss: 0.3350 - accuracy: 0.8589 - precision: 0.8243 - recall: 0.9100\n",
            "Epoch 45/100\n",
            "46/46 [==============================] - 0s 3ms/step - loss: 0.3440 - accuracy: 0.8539 - precision: 0.8159 - recall: 0.9121\n",
            "Epoch 46/100\n",
            "46/46 [==============================] - 0s 3ms/step - loss: 0.3276 - accuracy: 0.8627 - precision: 0.8335 - recall: 0.9086\n",
            "Epoch 47/100\n",
            "46/46 [==============================] - 0s 4ms/step - loss: 0.3289 - accuracy: 0.8655 - precision: 0.8304 - recall: 0.9199\n",
            "Epoch 48/100\n",
            "46/46 [==============================] - 0s 3ms/step - loss: 0.3321 - accuracy: 0.8631 - precision: 0.8265 - recall: 0.9165\n",
            "Epoch 49/100\n",
            "46/46 [==============================] - 0s 3ms/step - loss: 0.3360 - accuracy: 0.8603 - precision: 0.8287 - recall: 0.9111\n",
            "Epoch 50/100\n",
            "46/46 [==============================] - 0s 3ms/step - loss: 0.3260 - accuracy: 0.8653 - precision: 0.8329 - recall: 0.9123\n",
            "Epoch 51/100\n",
            "46/46 [==============================] - 0s 3ms/step - loss: 0.3279 - accuracy: 0.8700 - precision: 0.8306 - recall: 0.9265\n",
            "Epoch 52/100\n",
            "46/46 [==============================] - 0s 3ms/step - loss: 0.3362 - accuracy: 0.8565 - precision: 0.8237 - recall: 0.9035\n",
            "Epoch 53/100\n",
            "46/46 [==============================] - 0s 3ms/step - loss: 0.3224 - accuracy: 0.8625 - precision: 0.8266 - recall: 0.9160\n",
            "Epoch 54/100\n",
            "46/46 [==============================] - 0s 3ms/step - loss: 0.3193 - accuracy: 0.8689 - precision: 0.8307 - recall: 0.9231\n",
            "Epoch 55/100\n",
            "46/46 [==============================] - 0s 3ms/step - loss: 0.3223 - accuracy: 0.8625 - precision: 0.8259 - recall: 0.9134\n",
            "Epoch 56/100\n",
            "46/46 [==============================] - 0s 3ms/step - loss: 0.3235 - accuracy: 0.8651 - precision: 0.8330 - recall: 0.9154\n",
            "Epoch 57/100\n",
            "46/46 [==============================] - 0s 3ms/step - loss: 0.3272 - accuracy: 0.8638 - precision: 0.8325 - recall: 0.9149\n",
            "Epoch 58/100\n",
            "46/46 [==============================] - 0s 3ms/step - loss: 0.3281 - accuracy: 0.8618 - precision: 0.8295 - recall: 0.9115\n",
            "Epoch 59/100\n",
            "46/46 [==============================] - 0s 4ms/step - loss: 0.3196 - accuracy: 0.8678 - precision: 0.8332 - recall: 0.9221\n",
            "Epoch 60/100\n",
            "46/46 [==============================] - 0s 4ms/step - loss: 0.3176 - accuracy: 0.8649 - precision: 0.8327 - recall: 0.9160\n",
            "Epoch 61/100\n",
            "46/46 [==============================] - 0s 3ms/step - loss: 0.3220 - accuracy: 0.8678 - precision: 0.8309 - recall: 0.9219\n",
            "Epoch 62/100\n",
            "46/46 [==============================] - 0s 3ms/step - loss: 0.3089 - accuracy: 0.8713 - precision: 0.8356 - recall: 0.9224\n",
            "Epoch 63/100\n",
            "46/46 [==============================] - 0s 3ms/step - loss: 0.3166 - accuracy: 0.8658 - precision: 0.8299 - recall: 0.9186\n",
            "Epoch 64/100\n",
            "46/46 [==============================] - 0s 3ms/step - loss: 0.3141 - accuracy: 0.8695 - precision: 0.8327 - recall: 0.9178\n",
            "Epoch 65/100\n",
            "46/46 [==============================] - 0s 3ms/step - loss: 0.3155 - accuracy: 0.8704 - precision: 0.8362 - recall: 0.9249\n",
            "Epoch 66/100\n",
            "46/46 [==============================] - 0s 4ms/step - loss: 0.3203 - accuracy: 0.8660 - precision: 0.8347 - recall: 0.9140\n",
            "Epoch 67/100\n",
            "46/46 [==============================] - 0s 3ms/step - loss: 0.3197 - accuracy: 0.8684 - precision: 0.8410 - recall: 0.9128\n",
            "Epoch 68/100\n",
            "46/46 [==============================] - 0s 3ms/step - loss: 0.3235 - accuracy: 0.8625 - precision: 0.8301 - recall: 0.9108\n",
            "Epoch 69/100\n",
            "46/46 [==============================] - 0s 3ms/step - loss: 0.3067 - accuracy: 0.8719 - precision: 0.8374 - recall: 0.9230\n",
            "Epoch 70/100\n",
            "46/46 [==============================] - 0s 3ms/step - loss: 0.2952 - accuracy: 0.8790 - precision: 0.8470 - recall: 0.9224\n",
            "Epoch 71/100\n",
            "46/46 [==============================] - 0s 3ms/step - loss: 0.3028 - accuracy: 0.8757 - precision: 0.8442 - recall: 0.9218\n",
            "Epoch 72/100\n",
            "46/46 [==============================] - 0s 3ms/step - loss: 0.2969 - accuracy: 0.8757 - precision: 0.8431 - recall: 0.9222\n",
            "Epoch 73/100\n",
            "46/46 [==============================] - 0s 3ms/step - loss: 0.3055 - accuracy: 0.8763 - precision: 0.8488 - recall: 0.9207\n",
            "Epoch 74/100\n",
            "46/46 [==============================] - 0s 3ms/step - loss: 0.2993 - accuracy: 0.8783 - precision: 0.8450 - recall: 0.9249\n",
            "Epoch 75/100\n",
            "46/46 [==============================] - 0s 3ms/step - loss: 0.3029 - accuracy: 0.8770 - precision: 0.8429 - recall: 0.9265\n",
            "Epoch 76/100\n",
            "46/46 [==============================] - 0s 3ms/step - loss: 0.3014 - accuracy: 0.8739 - precision: 0.8382 - recall: 0.9231\n",
            "Epoch 77/100\n",
            "46/46 [==============================] - 0s 3ms/step - loss: 0.3100 - accuracy: 0.8722 - precision: 0.8371 - recall: 0.9252\n",
            "Epoch 78/100\n",
            "46/46 [==============================] - 0s 3ms/step - loss: 0.2970 - accuracy: 0.8801 - precision: 0.8449 - recall: 0.9271\n",
            "Epoch 79/100\n",
            "46/46 [==============================] - 0s 3ms/step - loss: 0.2974 - accuracy: 0.8772 - precision: 0.8460 - recall: 0.9264\n",
            "Epoch 80/100\n",
            "46/46 [==============================] - 0s 3ms/step - loss: 0.2833 - accuracy: 0.8819 - precision: 0.8478 - recall: 0.9272\n",
            "Epoch 81/100\n",
            "46/46 [==============================] - 0s 3ms/step - loss: 0.3030 - accuracy: 0.8717 - precision: 0.8410 - recall: 0.9212\n",
            "Epoch 82/100\n",
            "46/46 [==============================] - 0s 3ms/step - loss: 0.3066 - accuracy: 0.8713 - precision: 0.8363 - recall: 0.9240\n",
            "Epoch 83/100\n",
            "46/46 [==============================] - 0s 3ms/step - loss: 0.2897 - accuracy: 0.8849 - precision: 0.8529 - recall: 0.9298\n",
            "Epoch 84/100\n",
            "46/46 [==============================] - 0s 3ms/step - loss: 0.2923 - accuracy: 0.8797 - precision: 0.8521 - recall: 0.9171\n",
            "Epoch 85/100\n",
            "46/46 [==============================] - 0s 3ms/step - loss: 0.2966 - accuracy: 0.8792 - precision: 0.8440 - recall: 0.9269\n",
            "Epoch 86/100\n",
            "46/46 [==============================] - 0s 4ms/step - loss: 0.2981 - accuracy: 0.8772 - precision: 0.8480 - recall: 0.9211\n",
            "Epoch 87/100\n",
            "46/46 [==============================] - 0s 4ms/step - loss: 0.2958 - accuracy: 0.8819 - precision: 0.8495 - recall: 0.9275\n",
            "Epoch 88/100\n",
            "46/46 [==============================] - 0s 4ms/step - loss: 0.3001 - accuracy: 0.8737 - precision: 0.8413 - recall: 0.9229\n",
            "Epoch 89/100\n",
            "46/46 [==============================] - 0s 3ms/step - loss: 0.2903 - accuracy: 0.8805 - precision: 0.8503 - recall: 0.9183\n",
            "Epoch 90/100\n",
            "46/46 [==============================] - 0s 3ms/step - loss: 0.2959 - accuracy: 0.8792 - precision: 0.8403 - recall: 0.9372\n",
            "Epoch 91/100\n",
            "46/46 [==============================] - 0s 3ms/step - loss: 0.2917 - accuracy: 0.8788 - precision: 0.8439 - recall: 0.9341\n",
            "Epoch 92/100\n",
            "46/46 [==============================] - 0s 4ms/step - loss: 0.2945 - accuracy: 0.8801 - precision: 0.8539 - recall: 0.9198\n",
            "Epoch 93/100\n",
            "46/46 [==============================] - 0s 3ms/step - loss: 0.2927 - accuracy: 0.8739 - precision: 0.8414 - recall: 0.9214\n",
            "Epoch 94/100\n",
            "46/46 [==============================] - 0s 3ms/step - loss: 0.2958 - accuracy: 0.8750 - precision: 0.8466 - recall: 0.9182\n",
            "Epoch 95/100\n",
            "46/46 [==============================] - 0s 3ms/step - loss: 0.2875 - accuracy: 0.8786 - precision: 0.8391 - recall: 0.9293\n",
            "Epoch 96/100\n",
            "46/46 [==============================] - 0s 4ms/step - loss: 0.2914 - accuracy: 0.8799 - precision: 0.8466 - recall: 0.9267\n",
            "Epoch 97/100\n",
            "46/46 [==============================] - 0s 3ms/step - loss: 0.2863 - accuracy: 0.8841 - precision: 0.8517 - recall: 0.9305\n",
            "Epoch 98/100\n",
            "46/46 [==============================] - 0s 4ms/step - loss: 0.2854 - accuracy: 0.8819 - precision: 0.8529 - recall: 0.9256\n",
            "Epoch 99/100\n",
            "46/46 [==============================] - 0s 4ms/step - loss: 0.2941 - accuracy: 0.8781 - precision: 0.8476 - recall: 0.9240\n",
            "Epoch 100/100\n",
            "46/46 [==============================] - 0s 3ms/step - loss: 0.2886 - accuracy: 0.8805 - precision: 0.8469 - recall: 0.9271\n",
            "23/23 [==============================] - 0s 2ms/step - loss: 0.2985 - accuracy: 0.8823 - precision: 0.8430 - recall: 0.9488\n",
            "[CV 2/3] END ........batch_size=100, epochs=100;, score=0.882 total time=  18.2s\n",
            "Model: \"sequential_16\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " dense_96 (Dense)            (None, 32)                352       \n",
            "                                                                 \n",
            " batch_normalization_60 (Bat  (None, 32)               128       \n",
            " chNormalization)                                                \n",
            "                                                                 \n",
            " dense_97 (Dense)            (None, 16)                528       \n",
            "                                                                 \n",
            " batch_normalization_61 (Bat  (None, 16)               64        \n",
            " chNormalization)                                                \n",
            "                                                                 \n",
            " dense_98 (Dense)            (None, 8)                 136       \n",
            "                                                                 \n",
            " batch_normalization_62 (Bat  (None, 8)                32        \n",
            " chNormalization)                                                \n",
            "                                                                 \n",
            " dense_99 (Dense)            (None, 4)                 36        \n",
            "                                                                 \n",
            " batch_normalization_63 (Bat  (None, 4)                16        \n",
            " chNormalization)                                                \n",
            "                                                                 \n",
            " dense_100 (Dense)           (None, 2)                 10        \n",
            "                                                                 \n",
            " batch_normalization_64 (Bat  (None, 2)                8         \n",
            " chNormalization)                                                \n",
            "                                                                 \n",
            " dense_101 (Dense)           (None, 1)                 3         \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 1,313\n",
            "Trainable params: 1,189\n",
            "Non-trainable params: 124\n",
            "_________________________________________________________________\n",
            "None\n",
            "Epoch 1/100\n",
            "46/46 [==============================] - 2s 3ms/step - loss: 0.6255 - accuracy: 0.6341 - precision: 0.6321 - recall: 0.6647\n",
            "Epoch 2/100\n",
            "46/46 [==============================] - 0s 3ms/step - loss: 0.5704 - accuracy: 0.7218 - precision: 0.7311 - recall: 0.7207\n",
            "Epoch 3/100\n",
            "46/46 [==============================] - 0s 3ms/step - loss: 0.5352 - accuracy: 0.7536 - precision: 0.7589 - recall: 0.7584\n",
            "Epoch 4/100\n",
            "46/46 [==============================] - 0s 4ms/step - loss: 0.5083 - accuracy: 0.7668 - precision: 0.7747 - recall: 0.7637\n",
            "Epoch 5/100\n",
            "46/46 [==============================] - 0s 3ms/step - loss: 0.4881 - accuracy: 0.7840 - precision: 0.7752 - recall: 0.8109\n",
            "Epoch 6/100\n",
            "46/46 [==============================] - 0s 4ms/step - loss: 0.4667 - accuracy: 0.7950 - precision: 0.7914 - recall: 0.8130\n",
            "Epoch 7/100\n",
            "46/46 [==============================] - 0s 3ms/step - loss: 0.4534 - accuracy: 0.8003 - precision: 0.7848 - recall: 0.8383\n",
            "Epoch 8/100\n",
            "46/46 [==============================] - 0s 4ms/step - loss: 0.4399 - accuracy: 0.8041 - precision: 0.7817 - recall: 0.8487\n",
            "Epoch 9/100\n",
            "46/46 [==============================] - 0s 4ms/step - loss: 0.4257 - accuracy: 0.8201 - precision: 0.7941 - recall: 0.8695\n",
            "Epoch 10/100\n",
            "46/46 [==============================] - 0s 3ms/step - loss: 0.4210 - accuracy: 0.8177 - precision: 0.7854 - recall: 0.8816\n",
            "Epoch 11/100\n",
            "46/46 [==============================] - 0s 3ms/step - loss: 0.4188 - accuracy: 0.8146 - precision: 0.7882 - recall: 0.8704\n",
            "Epoch 12/100\n",
            "46/46 [==============================] - 0s 3ms/step - loss: 0.4125 - accuracy: 0.8179 - precision: 0.7922 - recall: 0.8741\n",
            "Epoch 13/100\n",
            "46/46 [==============================] - 0s 3ms/step - loss: 0.4005 - accuracy: 0.8268 - precision: 0.7975 - recall: 0.8880\n",
            "Epoch 14/100\n",
            "46/46 [==============================] - 0s 4ms/step - loss: 0.4004 - accuracy: 0.8235 - precision: 0.7952 - recall: 0.8807\n",
            "Epoch 15/100\n",
            "46/46 [==============================] - 0s 4ms/step - loss: 0.3918 - accuracy: 0.8274 - precision: 0.7954 - recall: 0.8883\n",
            "Epoch 16/100\n",
            "46/46 [==============================] - 0s 3ms/step - loss: 0.3887 - accuracy: 0.8268 - precision: 0.7941 - recall: 0.8918\n",
            "Epoch 17/100\n",
            "46/46 [==============================] - 0s 3ms/step - loss: 0.3799 - accuracy: 0.8327 - precision: 0.7994 - recall: 0.8941\n",
            "Epoch 18/100\n",
            "46/46 [==============================] - 0s 3ms/step - loss: 0.3827 - accuracy: 0.8314 - precision: 0.8012 - recall: 0.8907\n",
            "Epoch 19/100\n",
            "46/46 [==============================] - 0s 3ms/step - loss: 0.3765 - accuracy: 0.8367 - precision: 0.8077 - recall: 0.8950\n",
            "Epoch 20/100\n",
            "46/46 [==============================] - 0s 3ms/step - loss: 0.3756 - accuracy: 0.8347 - precision: 0.8053 - recall: 0.8886\n",
            "Epoch 21/100\n",
            "46/46 [==============================] - 0s 4ms/step - loss: 0.3766 - accuracy: 0.8349 - precision: 0.8012 - recall: 0.8971\n",
            "Epoch 22/100\n",
            "46/46 [==============================] - 0s 3ms/step - loss: 0.3648 - accuracy: 0.8400 - precision: 0.8052 - recall: 0.9020\n",
            "Epoch 23/100\n",
            "46/46 [==============================] - 0s 3ms/step - loss: 0.3690 - accuracy: 0.8358 - precision: 0.8012 - recall: 0.8950\n",
            "Epoch 24/100\n",
            "46/46 [==============================] - 0s 3ms/step - loss: 0.3648 - accuracy: 0.8395 - precision: 0.8034 - recall: 0.9052\n",
            "Epoch 25/100\n",
            "46/46 [==============================] - 0s 3ms/step - loss: 0.3576 - accuracy: 0.8453 - precision: 0.8122 - recall: 0.9014\n",
            "Epoch 26/100\n",
            "46/46 [==============================] - 0s 3ms/step - loss: 0.3585 - accuracy: 0.8453 - precision: 0.8127 - recall: 0.9032\n",
            "Epoch 27/100\n",
            "46/46 [==============================] - 0s 4ms/step - loss: 0.3534 - accuracy: 0.8475 - precision: 0.8195 - recall: 0.9015\n",
            "Epoch 28/100\n",
            "46/46 [==============================] - 0s 3ms/step - loss: 0.3622 - accuracy: 0.8415 - precision: 0.8155 - recall: 0.8945\n",
            "Epoch 29/100\n",
            "46/46 [==============================] - 0s 4ms/step - loss: 0.3598 - accuracy: 0.8411 - precision: 0.8128 - recall: 0.8936\n",
            "Epoch 30/100\n",
            "46/46 [==============================] - 0s 3ms/step - loss: 0.3568 - accuracy: 0.8490 - precision: 0.8194 - recall: 0.9011\n",
            "Epoch 31/100\n",
            "46/46 [==============================] - 0s 3ms/step - loss: 0.3550 - accuracy: 0.8435 - precision: 0.8144 - recall: 0.9003\n",
            "Epoch 32/100\n",
            "46/46 [==============================] - 0s 4ms/step - loss: 0.3501 - accuracy: 0.8451 - precision: 0.8159 - recall: 0.9012\n",
            "Epoch 33/100\n",
            "46/46 [==============================] - 0s 4ms/step - loss: 0.3506 - accuracy: 0.8466 - precision: 0.8197 - recall: 0.8978\n",
            "Epoch 34/100\n",
            "46/46 [==============================] - 0s 3ms/step - loss: 0.3486 - accuracy: 0.8475 - precision: 0.8190 - recall: 0.9021\n",
            "Epoch 35/100\n",
            "46/46 [==============================] - 0s 4ms/step - loss: 0.3414 - accuracy: 0.8514 - precision: 0.8224 - recall: 0.9043\n",
            "Epoch 36/100\n",
            "46/46 [==============================] - 0s 3ms/step - loss: 0.3426 - accuracy: 0.8530 - precision: 0.8255 - recall: 0.9011\n",
            "Epoch 37/100\n",
            "46/46 [==============================] - 0s 4ms/step - loss: 0.3370 - accuracy: 0.8581 - precision: 0.8276 - recall: 0.9127\n",
            "Epoch 38/100\n",
            "46/46 [==============================] - 0s 3ms/step - loss: 0.3374 - accuracy: 0.8506 - precision: 0.8183 - recall: 0.9066\n",
            "Epoch 39/100\n",
            "46/46 [==============================] - 0s 4ms/step - loss: 0.3362 - accuracy: 0.8534 - precision: 0.8232 - recall: 0.9076\n",
            "Epoch 40/100\n",
            "46/46 [==============================] - 0s 3ms/step - loss: 0.3372 - accuracy: 0.8525 - precision: 0.8223 - recall: 0.9078\n",
            "Epoch 41/100\n",
            "46/46 [==============================] - 0s 3ms/step - loss: 0.3332 - accuracy: 0.8581 - precision: 0.8272 - recall: 0.9088\n",
            "Epoch 42/100\n",
            "46/46 [==============================] - 0s 4ms/step - loss: 0.3354 - accuracy: 0.8570 - precision: 0.8242 - recall: 0.9153\n",
            "Epoch 43/100\n",
            "46/46 [==============================] - 0s 3ms/step - loss: 0.3368 - accuracy: 0.8534 - precision: 0.8305 - recall: 0.8962\n",
            "Epoch 44/100\n",
            "46/46 [==============================] - 0s 4ms/step - loss: 0.3431 - accuracy: 0.8519 - precision: 0.8229 - recall: 0.9053\n",
            "Epoch 45/100\n",
            "46/46 [==============================] - 0s 4ms/step - loss: 0.3317 - accuracy: 0.8563 - precision: 0.8279 - recall: 0.9104\n",
            "Epoch 46/100\n",
            "46/46 [==============================] - 0s 4ms/step - loss: 0.3352 - accuracy: 0.8539 - precision: 0.8269 - recall: 0.9001\n",
            "Epoch 47/100\n",
            "46/46 [==============================] - 0s 4ms/step - loss: 0.3334 - accuracy: 0.8547 - precision: 0.8279 - recall: 0.9047\n",
            "Epoch 48/100\n",
            "46/46 [==============================] - 0s 3ms/step - loss: 0.3301 - accuracy: 0.8607 - precision: 0.8289 - recall: 0.9121\n",
            "Epoch 49/100\n",
            "46/46 [==============================] - 0s 3ms/step - loss: 0.3339 - accuracy: 0.8603 - precision: 0.8308 - recall: 0.9127\n",
            "Epoch 50/100\n",
            "46/46 [==============================] - 0s 3ms/step - loss: 0.3228 - accuracy: 0.8607 - precision: 0.8363 - recall: 0.9050\n",
            "Epoch 51/100\n",
            "46/46 [==============================] - 0s 4ms/step - loss: 0.3259 - accuracy: 0.8605 - precision: 0.8276 - recall: 0.9139\n",
            "Epoch 52/100\n",
            "46/46 [==============================] - 0s 4ms/step - loss: 0.3217 - accuracy: 0.8600 - precision: 0.8365 - recall: 0.9023\n",
            "Epoch 53/100\n",
            "46/46 [==============================] - 0s 3ms/step - loss: 0.3258 - accuracy: 0.8556 - precision: 0.8253 - recall: 0.9101\n",
            "Epoch 54/100\n",
            "46/46 [==============================] - 0s 3ms/step - loss: 0.3172 - accuracy: 0.8669 - precision: 0.8375 - recall: 0.9154\n",
            "Epoch 55/100\n",
            "46/46 [==============================] - 0s 4ms/step - loss: 0.3235 - accuracy: 0.8589 - precision: 0.8326 - recall: 0.9042\n",
            "Epoch 56/100\n",
            "46/46 [==============================] - 0s 4ms/step - loss: 0.3201 - accuracy: 0.8671 - precision: 0.8388 - recall: 0.9165\n",
            "Epoch 57/100\n",
            "46/46 [==============================] - 0s 3ms/step - loss: 0.3184 - accuracy: 0.8658 - precision: 0.8378 - recall: 0.9146\n",
            "Epoch 58/100\n",
            "46/46 [==============================] - 0s 3ms/step - loss: 0.3263 - accuracy: 0.8561 - precision: 0.8304 - recall: 0.9030\n",
            "Epoch 59/100\n",
            "46/46 [==============================] - 0s 3ms/step - loss: 0.3341 - accuracy: 0.8570 - precision: 0.8229 - recall: 0.9145\n",
            "Epoch 60/100\n",
            "46/46 [==============================] - 0s 3ms/step - loss: 0.3105 - accuracy: 0.8689 - precision: 0.8390 - recall: 0.9137\n",
            "Epoch 61/100\n",
            "46/46 [==============================] - 0s 4ms/step - loss: 0.3128 - accuracy: 0.8691 - precision: 0.8370 - recall: 0.9239\n",
            "Epoch 62/100\n",
            "46/46 [==============================] - 0s 3ms/step - loss: 0.3139 - accuracy: 0.8631 - precision: 0.8358 - recall: 0.9126\n",
            "Epoch 63/100\n",
            "46/46 [==============================] - 0s 4ms/step - loss: 0.3114 - accuracy: 0.8655 - precision: 0.8392 - recall: 0.9088\n",
            "Epoch 64/100\n",
            "46/46 [==============================] - 0s 3ms/step - loss: 0.3166 - accuracy: 0.8629 - precision: 0.8367 - recall: 0.9065\n",
            "Epoch 65/100\n",
            "46/46 [==============================] - 0s 3ms/step - loss: 0.3165 - accuracy: 0.8662 - precision: 0.8403 - recall: 0.9126\n",
            "Epoch 66/100\n",
            "46/46 [==============================] - 0s 3ms/step - loss: 0.3211 - accuracy: 0.8633 - precision: 0.8359 - recall: 0.9106\n",
            "Epoch 67/100\n",
            "46/46 [==============================] - 0s 4ms/step - loss: 0.3008 - accuracy: 0.8744 - precision: 0.8442 - recall: 0.9212\n",
            "Epoch 68/100\n",
            "46/46 [==============================] - 0s 4ms/step - loss: 0.3207 - accuracy: 0.8629 - precision: 0.8371 - recall: 0.9088\n",
            "Epoch 69/100\n",
            "46/46 [==============================] - 0s 3ms/step - loss: 0.3075 - accuracy: 0.8691 - precision: 0.8403 - recall: 0.9207\n",
            "Epoch 70/100\n",
            "46/46 [==============================] - 0s 4ms/step - loss: 0.3097 - accuracy: 0.8678 - precision: 0.8368 - recall: 0.9186\n",
            "Epoch 71/100\n",
            "46/46 [==============================] - 0s 4ms/step - loss: 0.3085 - accuracy: 0.8686 - precision: 0.8428 - recall: 0.9136\n",
            "Epoch 72/100\n",
            "46/46 [==============================] - 0s 3ms/step - loss: 0.3037 - accuracy: 0.8759 - precision: 0.8503 - recall: 0.9173\n",
            "Epoch 73/100\n",
            "46/46 [==============================] - 0s 3ms/step - loss: 0.3064 - accuracy: 0.8700 - precision: 0.8476 - recall: 0.9118\n",
            "Epoch 74/100\n",
            "46/46 [==============================] - 0s 4ms/step - loss: 0.3162 - accuracy: 0.8622 - precision: 0.8409 - recall: 0.9052\n",
            "Epoch 75/100\n",
            "46/46 [==============================] - 0s 3ms/step - loss: 0.3036 - accuracy: 0.8706 - precision: 0.8478 - recall: 0.9114\n",
            "Epoch 76/100\n",
            "46/46 [==============================] - 0s 3ms/step - loss: 0.3008 - accuracy: 0.8730 - precision: 0.8460 - recall: 0.9191\n",
            "Epoch 77/100\n",
            "46/46 [==============================] - 0s 3ms/step - loss: 0.3081 - accuracy: 0.8711 - precision: 0.8446 - recall: 0.9138\n",
            "Epoch 78/100\n",
            "46/46 [==============================] - 0s 3ms/step - loss: 0.3021 - accuracy: 0.8717 - precision: 0.8397 - recall: 0.9187\n",
            "Epoch 79/100\n",
            "46/46 [==============================] - 0s 3ms/step - loss: 0.3179 - accuracy: 0.8607 - precision: 0.8389 - recall: 0.8996\n",
            "Epoch 80/100\n",
            "46/46 [==============================] - 0s 3ms/step - loss: 0.3036 - accuracy: 0.8671 - precision: 0.8394 - recall: 0.9144\n",
            "Epoch 81/100\n",
            "46/46 [==============================] - 0s 3ms/step - loss: 0.3069 - accuracy: 0.8695 - precision: 0.8467 - recall: 0.9099\n",
            "Epoch 82/100\n",
            "46/46 [==============================] - 0s 3ms/step - loss: 0.3087 - accuracy: 0.8724 - precision: 0.8435 - recall: 0.9184\n",
            "Epoch 83/100\n",
            "46/46 [==============================] - 0s 3ms/step - loss: 0.2951 - accuracy: 0.8781 - precision: 0.8579 - recall: 0.9124\n",
            "Epoch 84/100\n",
            "46/46 [==============================] - 0s 3ms/step - loss: 0.2958 - accuracy: 0.8763 - precision: 0.8501 - recall: 0.9200\n",
            "Epoch 85/100\n",
            "46/46 [==============================] - 0s 3ms/step - loss: 0.2945 - accuracy: 0.8761 - precision: 0.8486 - recall: 0.9168\n",
            "Epoch 86/100\n",
            "46/46 [==============================] - 0s 3ms/step - loss: 0.2968 - accuracy: 0.8722 - precision: 0.8485 - recall: 0.9124\n",
            "Epoch 87/100\n",
            "46/46 [==============================] - 0s 3ms/step - loss: 0.2977 - accuracy: 0.8761 - precision: 0.8520 - recall: 0.9159\n",
            "Epoch 88/100\n",
            "46/46 [==============================] - 0s 3ms/step - loss: 0.2945 - accuracy: 0.8777 - precision: 0.8511 - recall: 0.9173\n",
            "Epoch 89/100\n",
            "46/46 [==============================] - 0s 3ms/step - loss: 0.2949 - accuracy: 0.8755 - precision: 0.8512 - recall: 0.9132\n",
            "Epoch 90/100\n",
            "46/46 [==============================] - 0s 3ms/step - loss: 0.3030 - accuracy: 0.8697 - precision: 0.8341 - recall: 0.9264\n",
            "Epoch 91/100\n",
            "46/46 [==============================] - 0s 3ms/step - loss: 0.2903 - accuracy: 0.8823 - precision: 0.8528 - recall: 0.9259\n",
            "Epoch 92/100\n",
            "46/46 [==============================] - 0s 4ms/step - loss: 0.2910 - accuracy: 0.8775 - precision: 0.8491 - recall: 0.9209\n",
            "Epoch 93/100\n",
            "46/46 [==============================] - 0s 4ms/step - loss: 0.2972 - accuracy: 0.8715 - precision: 0.8518 - recall: 0.9095\n",
            "Epoch 94/100\n",
            "46/46 [==============================] - 0s 3ms/step - loss: 0.2919 - accuracy: 0.8772 - precision: 0.8527 - recall: 0.9198\n",
            "Epoch 95/100\n",
            "46/46 [==============================] - 0s 3ms/step - loss: 0.2873 - accuracy: 0.8790 - precision: 0.8521 - recall: 0.9250\n",
            "Epoch 96/100\n",
            "46/46 [==============================] - 0s 3ms/step - loss: 0.2967 - accuracy: 0.8726 - precision: 0.8462 - recall: 0.9142\n",
            "Epoch 97/100\n",
            "46/46 [==============================] - 0s 3ms/step - loss: 0.2941 - accuracy: 0.8781 - precision: 0.8527 - recall: 0.9220\n",
            "Epoch 98/100\n",
            "46/46 [==============================] - 0s 3ms/step - loss: 0.2953 - accuracy: 0.8766 - precision: 0.8500 - recall: 0.9194\n",
            "Epoch 99/100\n",
            "46/46 [==============================] - 0s 4ms/step - loss: 0.2956 - accuracy: 0.8726 - precision: 0.8465 - recall: 0.9159\n",
            "Epoch 100/100\n",
            "46/46 [==============================] - 0s 4ms/step - loss: 0.2927 - accuracy: 0.8752 - precision: 0.8499 - recall: 0.9122\n",
            "23/23 [==============================] - 0s 2ms/step - loss: 0.3585 - accuracy: 0.8532 - precision: 0.8008 - recall: 0.9373\n",
            "[CV 3/3] END ........batch_size=100, epochs=100;, score=0.853 total time=  18.9s\n",
            "Model: \"sequential_17\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " dense_102 (Dense)           (None, 32)                352       \n",
            "                                                                 \n",
            " batch_normalization_65 (Bat  (None, 32)               128       \n",
            " chNormalization)                                                \n",
            "                                                                 \n",
            " dense_103 (Dense)           (None, 16)                528       \n",
            "                                                                 \n",
            " batch_normalization_66 (Bat  (None, 16)               64        \n",
            " chNormalization)                                                \n",
            "                                                                 \n",
            " dense_104 (Dense)           (None, 8)                 136       \n",
            "                                                                 \n",
            " batch_normalization_67 (Bat  (None, 8)                32        \n",
            " chNormalization)                                                \n",
            "                                                                 \n",
            " dense_105 (Dense)           (None, 4)                 36        \n",
            "                                                                 \n",
            " batch_normalization_68 (Bat  (None, 4)                16        \n",
            " chNormalization)                                                \n",
            "                                                                 \n",
            " dense_106 (Dense)           (None, 2)                 10        \n",
            "                                                                 \n",
            " batch_normalization_69 (Bat  (None, 2)                8         \n",
            " chNormalization)                                                \n",
            "                                                                 \n",
            " dense_107 (Dense)           (None, 1)                 3         \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 1,313\n",
            "Trainable params: 1,189\n",
            "Non-trainable params: 124\n",
            "_________________________________________________________________\n",
            "None\n",
            "Epoch 1/100\n",
            "137/137 [==============================] - 2s 3ms/step - loss: 0.6434 - accuracy: 0.6425 - precision: 0.6918 - recall: 0.5239\n",
            "Epoch 2/100\n",
            "137/137 [==============================] - 0s 3ms/step - loss: 0.5527 - accuracy: 0.7428 - precision: 0.7369 - recall: 0.7602\n",
            "Epoch 3/100\n",
            "137/137 [==============================] - 0s 3ms/step - loss: 0.5053 - accuracy: 0.7780 - precision: 0.7610 - recall: 0.8126\n",
            "Epoch 4/100\n",
            "137/137 [==============================] - 0s 3ms/step - loss: 0.4778 - accuracy: 0.7859 - precision: 0.7636 - recall: 0.8394\n",
            "Epoch 5/100\n",
            "137/137 [==============================] - 0s 3ms/step - loss: 0.4553 - accuracy: 0.7957 - precision: 0.7638 - recall: 0.8609\n",
            "Epoch 6/100\n",
            "137/137 [==============================] - 0s 3ms/step - loss: 0.4545 - accuracy: 0.7971 - precision: 0.7680 - recall: 0.8591\n",
            "Epoch 7/100\n",
            "137/137 [==============================] - 0s 3ms/step - loss: 0.4475 - accuracy: 0.8016 - precision: 0.7684 - recall: 0.8534\n",
            "Epoch 8/100\n",
            "137/137 [==============================] - 0s 3ms/step - loss: 0.4453 - accuracy: 0.7975 - precision: 0.7659 - recall: 0.8642\n",
            "Epoch 9/100\n",
            "137/137 [==============================] - 0s 4ms/step - loss: 0.4324 - accuracy: 0.8068 - precision: 0.7795 - recall: 0.8632\n",
            "Epoch 10/100\n",
            "137/137 [==============================] - 0s 3ms/step - loss: 0.4277 - accuracy: 0.8115 - precision: 0.7789 - recall: 0.8742\n",
            "Epoch 11/100\n",
            "137/137 [==============================] - 0s 3ms/step - loss: 0.4245 - accuracy: 0.8081 - precision: 0.7825 - recall: 0.8661\n",
            "Epoch 12/100\n",
            "137/137 [==============================] - 1s 4ms/step - loss: 0.4204 - accuracy: 0.8106 - precision: 0.7792 - recall: 0.8799\n",
            "Epoch 13/100\n",
            "137/137 [==============================] - 0s 3ms/step - loss: 0.4144 - accuracy: 0.8148 - precision: 0.7823 - recall: 0.8739\n",
            "Epoch 14/100\n",
            "137/137 [==============================] - 0s 3ms/step - loss: 0.4108 - accuracy: 0.8154 - precision: 0.7840 - recall: 0.8780\n",
            "Epoch 15/100\n",
            "137/137 [==============================] - 0s 3ms/step - loss: 0.4093 - accuracy: 0.8166 - precision: 0.7864 - recall: 0.8725\n",
            "Epoch 16/100\n",
            "137/137 [==============================] - 0s 4ms/step - loss: 0.4069 - accuracy: 0.8168 - precision: 0.7859 - recall: 0.8790\n",
            "Epoch 17/100\n",
            "137/137 [==============================] - 0s 3ms/step - loss: 0.4022 - accuracy: 0.8212 - precision: 0.7927 - recall: 0.8784\n",
            "Epoch 18/100\n",
            "137/137 [==============================] - 0s 3ms/step - loss: 0.4011 - accuracy: 0.8150 - precision: 0.7833 - recall: 0.8746\n",
            "Epoch 19/100\n",
            "137/137 [==============================] - 0s 3ms/step - loss: 0.4095 - accuracy: 0.8165 - precision: 0.7861 - recall: 0.8723\n",
            "Epoch 20/100\n",
            "137/137 [==============================] - 0s 3ms/step - loss: 0.4016 - accuracy: 0.8247 - precision: 0.7935 - recall: 0.8801\n",
            "Epoch 21/100\n",
            "137/137 [==============================] - 0s 3ms/step - loss: 0.4019 - accuracy: 0.8201 - precision: 0.7878 - recall: 0.8846\n",
            "Epoch 22/100\n",
            "137/137 [==============================] - 0s 3ms/step - loss: 0.3898 - accuracy: 0.8307 - precision: 0.7930 - recall: 0.8990\n",
            "Epoch 23/100\n",
            "137/137 [==============================] - 0s 3ms/step - loss: 0.3951 - accuracy: 0.8225 - precision: 0.7834 - recall: 0.8850\n",
            "Epoch 24/100\n",
            "137/137 [==============================] - 0s 4ms/step - loss: 0.4020 - accuracy: 0.8141 - precision: 0.7879 - recall: 0.8708\n",
            "Epoch 25/100\n",
            "137/137 [==============================] - 0s 3ms/step - loss: 0.3871 - accuracy: 0.8309 - precision: 0.7954 - recall: 0.8996\n",
            "Epoch 26/100\n",
            "137/137 [==============================] - 0s 3ms/step - loss: 0.3844 - accuracy: 0.8323 - precision: 0.8001 - recall: 0.8939\n",
            "Epoch 27/100\n",
            "137/137 [==============================] - 0s 3ms/step - loss: 0.3835 - accuracy: 0.8335 - precision: 0.7922 - recall: 0.8944\n",
            "Epoch 28/100\n",
            "137/137 [==============================] - 0s 3ms/step - loss: 0.3848 - accuracy: 0.8282 - precision: 0.7987 - recall: 0.8826\n",
            "Epoch 29/100\n",
            "137/137 [==============================] - 0s 3ms/step - loss: 0.3808 - accuracy: 0.8328 - precision: 0.8016 - recall: 0.8907\n",
            "Epoch 30/100\n",
            "137/137 [==============================] - 0s 3ms/step - loss: 0.3803 - accuracy: 0.8335 - precision: 0.7983 - recall: 0.9021\n",
            "Epoch 31/100\n",
            "137/137 [==============================] - 0s 3ms/step - loss: 0.3795 - accuracy: 0.8316 - precision: 0.8023 - recall: 0.8867\n",
            "Epoch 32/100\n",
            "137/137 [==============================] - 0s 3ms/step - loss: 0.3764 - accuracy: 0.8354 - precision: 0.7915 - recall: 0.9010\n",
            "Epoch 33/100\n",
            "137/137 [==============================] - 0s 3ms/step - loss: 0.3755 - accuracy: 0.8351 - precision: 0.8026 - recall: 0.8970\n",
            "Epoch 34/100\n",
            "137/137 [==============================] - 0s 3ms/step - loss: 0.3720 - accuracy: 0.8334 - precision: 0.8024 - recall: 0.8957\n",
            "Epoch 35/100\n",
            "137/137 [==============================] - 0s 3ms/step - loss: 0.3654 - accuracy: 0.8431 - precision: 0.8050 - recall: 0.9080\n",
            "Epoch 36/100\n",
            "137/137 [==============================] - 0s 3ms/step - loss: 0.3709 - accuracy: 0.8375 - precision: 0.8064 - recall: 0.8905\n",
            "Epoch 37/100\n",
            "137/137 [==============================] - 0s 3ms/step - loss: 0.3635 - accuracy: 0.8379 - precision: 0.8061 - recall: 0.8876\n",
            "Epoch 38/100\n",
            "137/137 [==============================] - 0s 3ms/step - loss: 0.3622 - accuracy: 0.8403 - precision: 0.8049 - recall: 0.9044\n",
            "Epoch 39/100\n",
            "137/137 [==============================] - 1s 4ms/step - loss: 0.3645 - accuracy: 0.8392 - precision: 0.8038 - recall: 0.9000\n",
            "Epoch 40/100\n",
            "137/137 [==============================] - 1s 4ms/step - loss: 0.3662 - accuracy: 0.8325 - precision: 0.8010 - recall: 0.8953\n",
            "Epoch 41/100\n",
            "137/137 [==============================] - 0s 3ms/step - loss: 0.3572 - accuracy: 0.8444 - precision: 0.8159 - recall: 0.8969\n",
            "Epoch 42/100\n",
            "137/137 [==============================] - 0s 3ms/step - loss: 0.3650 - accuracy: 0.8398 - precision: 0.8078 - recall: 0.8964\n",
            "Epoch 43/100\n",
            "137/137 [==============================] - 0s 3ms/step - loss: 0.3582 - accuracy: 0.8454 - precision: 0.8092 - recall: 0.9048\n",
            "Epoch 44/100\n",
            "137/137 [==============================] - 0s 3ms/step - loss: 0.3562 - accuracy: 0.8442 - precision: 0.8080 - recall: 0.9060\n",
            "Epoch 45/100\n",
            "137/137 [==============================] - 0s 3ms/step - loss: 0.3511 - accuracy: 0.8476 - precision: 0.8151 - recall: 0.8929\n",
            "Epoch 46/100\n",
            "137/137 [==============================] - 0s 3ms/step - loss: 0.3521 - accuracy: 0.8451 - precision: 0.8143 - recall: 0.9002\n",
            "Epoch 47/100\n",
            "137/137 [==============================] - 0s 3ms/step - loss: 0.3462 - accuracy: 0.8503 - precision: 0.8175 - recall: 0.9113\n",
            "Epoch 48/100\n",
            "137/137 [==============================] - 0s 4ms/step - loss: 0.3434 - accuracy: 0.8538 - precision: 0.8225 - recall: 0.9119\n",
            "Epoch 49/100\n",
            "137/137 [==============================] - 0s 3ms/step - loss: 0.3384 - accuracy: 0.8533 - precision: 0.8184 - recall: 0.9017\n",
            "Epoch 50/100\n",
            "137/137 [==============================] - 0s 3ms/step - loss: 0.3513 - accuracy: 0.8451 - precision: 0.8179 - recall: 0.8971\n",
            "Epoch 51/100\n",
            "137/137 [==============================] - 0s 3ms/step - loss: 0.3454 - accuracy: 0.8503 - precision: 0.8164 - recall: 0.9104\n",
            "Epoch 52/100\n",
            "137/137 [==============================] - 0s 3ms/step - loss: 0.3378 - accuracy: 0.8510 - precision: 0.8165 - recall: 0.9073\n",
            "Epoch 53/100\n",
            "137/137 [==============================] - 0s 3ms/step - loss: 0.3540 - accuracy: 0.8434 - precision: 0.8072 - recall: 0.8952\n",
            "Epoch 54/100\n",
            "137/137 [==============================] - 0s 3ms/step - loss: 0.3449 - accuracy: 0.8467 - precision: 0.8203 - recall: 0.8942\n",
            "Epoch 55/100\n",
            "137/137 [==============================] - 0s 4ms/step - loss: 0.3448 - accuracy: 0.8473 - precision: 0.8134 - recall: 0.9049\n",
            "Epoch 56/100\n",
            "137/137 [==============================] - 0s 3ms/step - loss: 0.3419 - accuracy: 0.8504 - precision: 0.8180 - recall: 0.9084\n",
            "Epoch 57/100\n",
            "137/137 [==============================] - 0s 3ms/step - loss: 0.3351 - accuracy: 0.8516 - precision: 0.8247 - recall: 0.9013\n",
            "Epoch 58/100\n",
            "137/137 [==============================] - 0s 3ms/step - loss: 0.3292 - accuracy: 0.8608 - precision: 0.8282 - recall: 0.9190\n",
            "Epoch 59/100\n",
            "137/137 [==============================] - 1s 4ms/step - loss: 0.3350 - accuracy: 0.8558 - precision: 0.8218 - recall: 0.9131\n",
            "Epoch 60/100\n",
            "137/137 [==============================] - 0s 4ms/step - loss: 0.3306 - accuracy: 0.8528 - precision: 0.8214 - recall: 0.9064\n",
            "Epoch 61/100\n",
            "137/137 [==============================] - 0s 3ms/step - loss: 0.3302 - accuracy: 0.8579 - precision: 0.8270 - recall: 0.9134\n",
            "Epoch 62/100\n",
            "137/137 [==============================] - 0s 4ms/step - loss: 0.3282 - accuracy: 0.8623 - precision: 0.8237 - recall: 0.9216\n",
            "Epoch 63/100\n",
            "137/137 [==============================] - 0s 3ms/step - loss: 0.3291 - accuracy: 0.8582 - precision: 0.8294 - recall: 0.9102\n",
            "Epoch 64/100\n",
            "137/137 [==============================] - 0s 4ms/step - loss: 0.3382 - accuracy: 0.8566 - precision: 0.8238 - recall: 0.9114\n",
            "Epoch 65/100\n",
            "137/137 [==============================] - 0s 3ms/step - loss: 0.3406 - accuracy: 0.8532 - precision: 0.8145 - recall: 0.9136\n",
            "Epoch 66/100\n",
            "137/137 [==============================] - 0s 4ms/step - loss: 0.3367 - accuracy: 0.8541 - precision: 0.8228 - recall: 0.9051\n",
            "Epoch 67/100\n",
            "137/137 [==============================] - 0s 3ms/step - loss: 0.3416 - accuracy: 0.8498 - precision: 0.8204 - recall: 0.9060\n",
            "Epoch 68/100\n",
            "137/137 [==============================] - 0s 3ms/step - loss: 0.3304 - accuracy: 0.8564 - precision: 0.8244 - recall: 0.9132\n",
            "Epoch 69/100\n",
            "137/137 [==============================] - 0s 3ms/step - loss: 0.3241 - accuracy: 0.8610 - precision: 0.8288 - recall: 0.9170\n",
            "Epoch 70/100\n",
            "137/137 [==============================] - 0s 3ms/step - loss: 0.3302 - accuracy: 0.8582 - precision: 0.8211 - recall: 0.9069\n",
            "Epoch 71/100\n",
            "137/137 [==============================] - 0s 3ms/step - loss: 0.3273 - accuracy: 0.8635 - precision: 0.8339 - recall: 0.9120\n",
            "Epoch 72/100\n",
            "137/137 [==============================] - 0s 3ms/step - loss: 0.3211 - accuracy: 0.8613 - precision: 0.8286 - recall: 0.9155\n",
            "Epoch 73/100\n",
            "137/137 [==============================] - 0s 4ms/step - loss: 0.3236 - accuracy: 0.8613 - precision: 0.8258 - recall: 0.9202\n",
            "Epoch 74/100\n",
            "137/137 [==============================] - 0s 3ms/step - loss: 0.3333 - accuracy: 0.8547 - precision: 0.8246 - recall: 0.9055\n",
            "Epoch 75/100\n",
            "137/137 [==============================] - 0s 3ms/step - loss: 0.3231 - accuracy: 0.8598 - precision: 0.8245 - recall: 0.9155\n",
            "Epoch 76/100\n",
            "137/137 [==============================] - 0s 3ms/step - loss: 0.3178 - accuracy: 0.8660 - precision: 0.8341 - recall: 0.9185\n",
            "Epoch 77/100\n",
            "137/137 [==============================] - 0s 3ms/step - loss: 0.3463 - accuracy: 0.8482 - precision: 0.8191 - recall: 0.9021\n",
            "Epoch 78/100\n",
            "137/137 [==============================] - 0s 3ms/step - loss: 0.3365 - accuracy: 0.8513 - precision: 0.8203 - recall: 0.9080\n",
            "Epoch 79/100\n",
            "137/137 [==============================] - 0s 3ms/step - loss: 0.3300 - accuracy: 0.8570 - precision: 0.8250 - recall: 0.9161\n",
            "Epoch 80/100\n",
            "137/137 [==============================] - 0s 3ms/step - loss: 0.3249 - accuracy: 0.8608 - precision: 0.8307 - recall: 0.9152\n",
            "Epoch 81/100\n",
            "137/137 [==============================] - 0s 3ms/step - loss: 0.3230 - accuracy: 0.8591 - precision: 0.8252 - recall: 0.9125\n",
            "Epoch 82/100\n",
            "137/137 [==============================] - 0s 3ms/step - loss: 0.3148 - accuracy: 0.8685 - precision: 0.8319 - recall: 0.9249\n",
            "Epoch 83/100\n",
            "137/137 [==============================] - 1s 4ms/step - loss: 0.3181 - accuracy: 0.8580 - precision: 0.8291 - recall: 0.9112\n",
            "Epoch 84/100\n",
            "137/137 [==============================] - 0s 3ms/step - loss: 0.3089 - accuracy: 0.8701 - precision: 0.8348 - recall: 0.9226\n",
            "Epoch 85/100\n",
            "137/137 [==============================] - 1s 4ms/step - loss: 0.3247 - accuracy: 0.8594 - precision: 0.8300 - recall: 0.9096\n",
            "Epoch 86/100\n",
            "137/137 [==============================] - 0s 4ms/step - loss: 0.3175 - accuracy: 0.8600 - precision: 0.8309 - recall: 0.9095\n",
            "Epoch 87/100\n",
            "137/137 [==============================] - 0s 3ms/step - loss: 0.3146 - accuracy: 0.8661 - precision: 0.8331 - recall: 0.9178\n",
            "Epoch 88/100\n",
            "137/137 [==============================] - 0s 4ms/step - loss: 0.3156 - accuracy: 0.8664 - precision: 0.8272 - recall: 0.9229\n",
            "Epoch 89/100\n",
            "137/137 [==============================] - 0s 3ms/step - loss: 0.3155 - accuracy: 0.8667 - precision: 0.8393 - recall: 0.9151\n",
            "Epoch 90/100\n",
            "137/137 [==============================] - 1s 4ms/step - loss: 0.3156 - accuracy: 0.8658 - precision: 0.8378 - recall: 0.9110\n",
            "Epoch 91/100\n",
            "137/137 [==============================] - 0s 3ms/step - loss: 0.3131 - accuracy: 0.8664 - precision: 0.8315 - recall: 0.9212\n",
            "Epoch 92/100\n",
            "137/137 [==============================] - 0s 3ms/step - loss: 0.3116 - accuracy: 0.8683 - precision: 0.8424 - recall: 0.9163\n",
            "Epoch 93/100\n",
            "137/137 [==============================] - 0s 4ms/step - loss: 0.3186 - accuracy: 0.8639 - precision: 0.8330 - recall: 0.9107\n",
            "Epoch 94/100\n",
            "137/137 [==============================] - 0s 3ms/step - loss: 0.3167 - accuracy: 0.8589 - precision: 0.8306 - recall: 0.9089\n",
            "Epoch 95/100\n",
            "137/137 [==============================] - 0s 3ms/step - loss: 0.3076 - accuracy: 0.8701 - precision: 0.8327 - recall: 0.9172\n",
            "Epoch 96/100\n",
            "137/137 [==============================] - 0s 3ms/step - loss: 0.3105 - accuracy: 0.8660 - precision: 0.8373 - recall: 0.9050\n",
            "Epoch 97/100\n",
            "137/137 [==============================] - 0s 3ms/step - loss: 0.3078 - accuracy: 0.8698 - precision: 0.8297 - recall: 0.9222\n",
            "Epoch 98/100\n",
            "137/137 [==============================] - 0s 3ms/step - loss: 0.3093 - accuracy: 0.8694 - precision: 0.8349 - recall: 0.9203\n",
            "Epoch 99/100\n",
            "137/137 [==============================] - 0s 3ms/step - loss: 0.3038 - accuracy: 0.8705 - precision: 0.8386 - recall: 0.9232\n",
            "Epoch 100/100\n",
            "137/137 [==============================] - 0s 3ms/step - loss: 0.3107 - accuracy: 0.8672 - precision: 0.8327 - recall: 0.9205\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "GridSearchCV(cv=3,\n",
              "             estimator=<keras.wrappers.scikit_learn.KerasClassifier object at 0x7f2fd7e37310>,\n",
              "             param_grid={'batch_size': [50, 100], 'epochs': [50, 100]},\n",
              "             verbose=3)"
            ]
          },
          "metadata": {},
          "execution_count": 30
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5oMDOqprkS8S"
      },
      "source": [
        "Find the best parameters according to the Grid Search you have done, and the accuracy for the best model."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fom1lr4pMku4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b91ecedd-e4b7-4955-c20a-87c225e8bea5"
      },
      "source": [
        "#Test Your Zaka\n",
        "# summarize\n",
        "\n",
        "print('Best Accuracy score: %.3f' % DL_search.best_score_)\n",
        "print('Config: %s' % DL_search.best_params_)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Best Accuracy score: 0.874\n",
            "Config: {'batch_size': 50, 'epochs': 100}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZqKDaC5aUQtc"
      },
      "source": [
        "Fit the model on the best hyperparameters we obtained."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IiauYJNtQXLB",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "42abe23d-5882-4d7d-8aa1-4b39403c2716"
      },
      "source": [
        "#Test Your Zaka\n",
        "model4.fit(x_train2 , y_train2, batch_size=50, epochs=100)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential_18\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " dense_108 (Dense)           (None, 32)                352       \n",
            "                                                                 \n",
            " batch_normalization_70 (Bat  (None, 32)               128       \n",
            " chNormalization)                                                \n",
            "                                                                 \n",
            " dense_109 (Dense)           (None, 16)                528       \n",
            "                                                                 \n",
            " batch_normalization_71 (Bat  (None, 16)               64        \n",
            " chNormalization)                                                \n",
            "                                                                 \n",
            " dense_110 (Dense)           (None, 8)                 136       \n",
            "                                                                 \n",
            " batch_normalization_72 (Bat  (None, 8)                32        \n",
            " chNormalization)                                                \n",
            "                                                                 \n",
            " dense_111 (Dense)           (None, 4)                 36        \n",
            "                                                                 \n",
            " batch_normalization_73 (Bat  (None, 4)                16        \n",
            " chNormalization)                                                \n",
            "                                                                 \n",
            " dense_112 (Dense)           (None, 2)                 10        \n",
            "                                                                 \n",
            " batch_normalization_74 (Bat  (None, 2)                8         \n",
            " chNormalization)                                                \n",
            "                                                                 \n",
            " dense_113 (Dense)           (None, 1)                 3         \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 1,313\n",
            "Trainable params: 1,189\n",
            "Non-trainable params: 124\n",
            "_________________________________________________________________\n",
            "None\n",
            "Epoch 1/100\n",
            "137/137 [==============================] - 2s 2ms/step - loss: 0.6144 - accuracy: 0.6423 - precision: 0.6499 - recall: 0.6061\n",
            "Epoch 2/100\n",
            "137/137 [==============================] - 0s 3ms/step - loss: 0.5103 - accuracy: 0.7684 - precision: 0.7262 - recall: 0.8652\n",
            "Epoch 3/100\n",
            "137/137 [==============================] - 0s 3ms/step - loss: 0.4710 - accuracy: 0.7860 - precision: 0.7542 - recall: 0.8504\n",
            "Epoch 4/100\n",
            "137/137 [==============================] - 0s 2ms/step - loss: 0.4480 - accuracy: 0.7927 - precision: 0.7572 - recall: 0.8774\n",
            "Epoch 5/100\n",
            "137/137 [==============================] - 0s 2ms/step - loss: 0.4273 - accuracy: 0.8025 - precision: 0.7687 - recall: 0.8727\n",
            "Epoch 6/100\n",
            "137/137 [==============================] - 0s 3ms/step - loss: 0.4253 - accuracy: 0.8025 - precision: 0.7635 - recall: 0.8766\n",
            "Epoch 7/100\n",
            "137/137 [==============================] - 0s 3ms/step - loss: 0.4054 - accuracy: 0.8134 - precision: 0.7827 - recall: 0.8802\n",
            "Epoch 8/100\n",
            "137/137 [==============================] - 0s 2ms/step - loss: 0.4049 - accuracy: 0.8112 - precision: 0.7801 - recall: 0.8751\n",
            "Epoch 9/100\n",
            "137/137 [==============================] - 0s 2ms/step - loss: 0.3975 - accuracy: 0.8166 - precision: 0.7876 - recall: 0.8729\n",
            "Epoch 10/100\n",
            "137/137 [==============================] - 0s 2ms/step - loss: 0.3977 - accuracy: 0.8153 - precision: 0.7863 - recall: 0.8803\n",
            "Epoch 11/100\n",
            "137/137 [==============================] - 0s 3ms/step - loss: 0.3927 - accuracy: 0.8226 - precision: 0.7871 - recall: 0.8843\n",
            "Epoch 12/100\n",
            "137/137 [==============================] - 0s 2ms/step - loss: 0.3874 - accuracy: 0.8207 - precision: 0.7797 - recall: 0.8862\n",
            "Epoch 13/100\n",
            "137/137 [==============================] - 0s 2ms/step - loss: 0.3794 - accuracy: 0.8282 - precision: 0.7989 - recall: 0.8877\n",
            "Epoch 14/100\n",
            "137/137 [==============================] - 0s 2ms/step - loss: 0.3822 - accuracy: 0.8278 - precision: 0.7900 - recall: 0.9004\n",
            "Epoch 15/100\n",
            "137/137 [==============================] - 0s 2ms/step - loss: 0.3811 - accuracy: 0.8266 - precision: 0.7878 - recall: 0.8964\n",
            "Epoch 16/100\n",
            "137/137 [==============================] - 0s 3ms/step - loss: 0.3738 - accuracy: 0.8319 - precision: 0.7954 - recall: 0.8946\n",
            "Epoch 17/100\n",
            "137/137 [==============================] - 0s 2ms/step - loss: 0.3728 - accuracy: 0.8314 - precision: 0.7945 - recall: 0.9017\n",
            "Epoch 18/100\n",
            "137/137 [==============================] - 0s 3ms/step - loss: 0.3807 - accuracy: 0.8275 - precision: 0.7870 - recall: 0.8985\n",
            "Epoch 19/100\n",
            "137/137 [==============================] - 0s 3ms/step - loss: 0.3726 - accuracy: 0.8359 - precision: 0.7960 - recall: 0.9104\n",
            "Epoch 20/100\n",
            "137/137 [==============================] - 0s 3ms/step - loss: 0.3666 - accuracy: 0.8348 - precision: 0.7987 - recall: 0.9060\n",
            "Epoch 21/100\n",
            "137/137 [==============================] - 0s 3ms/step - loss: 0.3845 - accuracy: 0.8234 - precision: 0.7790 - recall: 0.8938\n",
            "Epoch 22/100\n",
            "137/137 [==============================] - 0s 3ms/step - loss: 0.3701 - accuracy: 0.8326 - precision: 0.7933 - recall: 0.9095\n",
            "Epoch 23/100\n",
            "137/137 [==============================] - 0s 2ms/step - loss: 0.3633 - accuracy: 0.8360 - precision: 0.7991 - recall: 0.9091\n",
            "Epoch 24/100\n",
            "137/137 [==============================] - 0s 3ms/step - loss: 0.3541 - accuracy: 0.8466 - precision: 0.8070 - recall: 0.9147\n",
            "Epoch 25/100\n",
            "137/137 [==============================] - 0s 3ms/step - loss: 0.3576 - accuracy: 0.8417 - precision: 0.8054 - recall: 0.9068\n",
            "Epoch 26/100\n",
            "137/137 [==============================] - 0s 3ms/step - loss: 0.3504 - accuracy: 0.8472 - precision: 0.8111 - recall: 0.9119\n",
            "Epoch 27/100\n",
            "137/137 [==============================] - 0s 2ms/step - loss: 0.3477 - accuracy: 0.8432 - precision: 0.8061 - recall: 0.9048\n",
            "Epoch 28/100\n",
            "137/137 [==============================] - 0s 2ms/step - loss: 0.3521 - accuracy: 0.8432 - precision: 0.8085 - recall: 0.9063\n",
            "Epoch 29/100\n",
            "137/137 [==============================] - 0s 3ms/step - loss: 0.3573 - accuracy: 0.8392 - precision: 0.8079 - recall: 0.9018\n",
            "Epoch 30/100\n",
            "137/137 [==============================] - 0s 2ms/step - loss: 0.3458 - accuracy: 0.8469 - precision: 0.8097 - recall: 0.9153\n",
            "Epoch 31/100\n",
            "137/137 [==============================] - 0s 2ms/step - loss: 0.3398 - accuracy: 0.8491 - precision: 0.8145 - recall: 0.9085\n",
            "Epoch 32/100\n",
            "137/137 [==============================] - 0s 2ms/step - loss: 0.3471 - accuracy: 0.8466 - precision: 0.8116 - recall: 0.9118\n",
            "Epoch 33/100\n",
            "137/137 [==============================] - 0s 2ms/step - loss: 0.3348 - accuracy: 0.8588 - precision: 0.8226 - recall: 0.9201\n",
            "Epoch 34/100\n",
            "137/137 [==============================] - 0s 3ms/step - loss: 0.3419 - accuracy: 0.8503 - precision: 0.8166 - recall: 0.9136\n",
            "Epoch 35/100\n",
            "137/137 [==============================] - 0s 2ms/step - loss: 0.3381 - accuracy: 0.8533 - precision: 0.8133 - recall: 0.9230\n",
            "Epoch 36/100\n",
            "137/137 [==============================] - 0s 3ms/step - loss: 0.3394 - accuracy: 0.8523 - precision: 0.8147 - recall: 0.9145\n",
            "Epoch 37/100\n",
            "137/137 [==============================] - 0s 2ms/step - loss: 0.3651 - accuracy: 0.8350 - precision: 0.7976 - recall: 0.9038\n",
            "Epoch 38/100\n",
            "137/137 [==============================] - 0s 3ms/step - loss: 0.3526 - accuracy: 0.8436 - precision: 0.8015 - recall: 0.9133\n",
            "Epoch 39/100\n",
            "137/137 [==============================] - 0s 2ms/step - loss: 0.3416 - accuracy: 0.8566 - precision: 0.8155 - recall: 0.9138\n",
            "Epoch 40/100\n",
            "137/137 [==============================] - 0s 3ms/step - loss: 0.3324 - accuracy: 0.8561 - precision: 0.8192 - recall: 0.9228\n",
            "Epoch 41/100\n",
            "137/137 [==============================] - 0s 2ms/step - loss: 0.3364 - accuracy: 0.8532 - precision: 0.8210 - recall: 0.9129\n",
            "Epoch 42/100\n",
            "137/137 [==============================] - 0s 2ms/step - loss: 0.3239 - accuracy: 0.8592 - precision: 0.8227 - recall: 0.9222\n",
            "Epoch 43/100\n",
            "137/137 [==============================] - 0s 3ms/step - loss: 0.3314 - accuracy: 0.8566 - precision: 0.8189 - recall: 0.9192\n",
            "Epoch 44/100\n",
            "137/137 [==============================] - 0s 3ms/step - loss: 0.3279 - accuracy: 0.8607 - precision: 0.8246 - recall: 0.9180\n",
            "Epoch 45/100\n",
            "137/137 [==============================] - 0s 3ms/step - loss: 0.3432 - accuracy: 0.8504 - precision: 0.8171 - recall: 0.9121\n",
            "Epoch 46/100\n",
            "137/137 [==============================] - 0s 3ms/step - loss: 0.3353 - accuracy: 0.8533 - precision: 0.8124 - recall: 0.9225\n",
            "Epoch 47/100\n",
            "137/137 [==============================] - 0s 2ms/step - loss: 0.3239 - accuracy: 0.8580 - precision: 0.8195 - recall: 0.9177\n",
            "Epoch 48/100\n",
            "137/137 [==============================] - 0s 2ms/step - loss: 0.3442 - accuracy: 0.8486 - precision: 0.8083 - recall: 0.9155\n",
            "Epoch 49/100\n",
            "137/137 [==============================] - 0s 3ms/step - loss: 0.3248 - accuracy: 0.8580 - precision: 0.8248 - recall: 0.9152\n",
            "Epoch 50/100\n",
            "137/137 [==============================] - 0s 3ms/step - loss: 0.3213 - accuracy: 0.8627 - precision: 0.8236 - recall: 0.9136\n",
            "Epoch 51/100\n",
            "137/137 [==============================] - 0s 3ms/step - loss: 0.3201 - accuracy: 0.8616 - precision: 0.8321 - recall: 0.9163\n",
            "Epoch 52/100\n",
            "137/137 [==============================] - 0s 3ms/step - loss: 0.3266 - accuracy: 0.8570 - precision: 0.8266 - recall: 0.9159\n",
            "Epoch 53/100\n",
            "137/137 [==============================] - 0s 3ms/step - loss: 0.3230 - accuracy: 0.8566 - precision: 0.8272 - recall: 0.9085\n",
            "Epoch 54/100\n",
            "137/137 [==============================] - 0s 3ms/step - loss: 0.3245 - accuracy: 0.8616 - precision: 0.8242 - recall: 0.9215\n",
            "Epoch 55/100\n",
            "137/137 [==============================] - 0s 3ms/step - loss: 0.3211 - accuracy: 0.8611 - precision: 0.8254 - recall: 0.9252\n",
            "Epoch 56/100\n",
            "137/137 [==============================] - 0s 2ms/step - loss: 0.3094 - accuracy: 0.8669 - precision: 0.8300 - recall: 0.9246\n",
            "Epoch 57/100\n",
            "137/137 [==============================] - 0s 3ms/step - loss: 0.3222 - accuracy: 0.8632 - precision: 0.8244 - recall: 0.9158\n",
            "Epoch 58/100\n",
            "137/137 [==============================] - 0s 3ms/step - loss: 0.3180 - accuracy: 0.8639 - precision: 0.8238 - recall: 0.9254\n",
            "Epoch 59/100\n",
            "137/137 [==============================] - 0s 3ms/step - loss: 0.3272 - accuracy: 0.8586 - precision: 0.8266 - recall: 0.9128\n",
            "Epoch 60/100\n",
            "137/137 [==============================] - 0s 3ms/step - loss: 0.3141 - accuracy: 0.8663 - precision: 0.8393 - recall: 0.9136\n",
            "Epoch 61/100\n",
            "137/137 [==============================] - 0s 2ms/step - loss: 0.3235 - accuracy: 0.8617 - precision: 0.8280 - recall: 0.9229\n",
            "Epoch 62/100\n",
            "137/137 [==============================] - 0s 3ms/step - loss: 0.3135 - accuracy: 0.8669 - precision: 0.8382 - recall: 0.9185\n",
            "Epoch 63/100\n",
            "137/137 [==============================] - 0s 3ms/step - loss: 0.3164 - accuracy: 0.8664 - precision: 0.8363 - recall: 0.9221\n",
            "Epoch 64/100\n",
            "137/137 [==============================] - 0s 2ms/step - loss: 0.3120 - accuracy: 0.8701 - precision: 0.8437 - recall: 0.9156\n",
            "Epoch 65/100\n",
            "137/137 [==============================] - 0s 3ms/step - loss: 0.3147 - accuracy: 0.8683 - precision: 0.8321 - recall: 0.9287\n",
            "Epoch 66/100\n",
            "137/137 [==============================] - 0s 2ms/step - loss: 0.3062 - accuracy: 0.8701 - precision: 0.8402 - recall: 0.9127\n",
            "Epoch 67/100\n",
            "137/137 [==============================] - 0s 2ms/step - loss: 0.3138 - accuracy: 0.8625 - precision: 0.8329 - recall: 0.9165\n",
            "Epoch 68/100\n",
            "137/137 [==============================] - 0s 3ms/step - loss: 0.3063 - accuracy: 0.8692 - precision: 0.8380 - recall: 0.9170\n",
            "Epoch 69/100\n",
            "137/137 [==============================] - 0s 3ms/step - loss: 0.3092 - accuracy: 0.8698 - precision: 0.8409 - recall: 0.9187\n",
            "Epoch 70/100\n",
            "137/137 [==============================] - 0s 3ms/step - loss: 0.3128 - accuracy: 0.8675 - precision: 0.8366 - recall: 0.9205\n",
            "Epoch 71/100\n",
            "137/137 [==============================] - 0s 3ms/step - loss: 0.3071 - accuracy: 0.8686 - precision: 0.8344 - recall: 0.9215\n",
            "Epoch 72/100\n",
            "137/137 [==============================] - 0s 2ms/step - loss: 0.3136 - accuracy: 0.8641 - precision: 0.8318 - recall: 0.9179\n",
            "Epoch 73/100\n",
            "137/137 [==============================] - 0s 2ms/step - loss: 0.3129 - accuracy: 0.8644 - precision: 0.8316 - recall: 0.9196\n",
            "Epoch 74/100\n",
            "137/137 [==============================] - 0s 3ms/step - loss: 0.3084 - accuracy: 0.8648 - precision: 0.8383 - recall: 0.9112\n",
            "Epoch 75/100\n",
            "137/137 [==============================] - 0s 3ms/step - loss: 0.3033 - accuracy: 0.8714 - precision: 0.8383 - recall: 0.9215\n",
            "Epoch 76/100\n",
            "137/137 [==============================] - 0s 3ms/step - loss: 0.3030 - accuracy: 0.8714 - precision: 0.8419 - recall: 0.9252\n",
            "Epoch 77/100\n",
            "137/137 [==============================] - 0s 3ms/step - loss: 0.3017 - accuracy: 0.8694 - precision: 0.8387 - recall: 0.9219\n",
            "Epoch 78/100\n",
            "137/137 [==============================] - 0s 3ms/step - loss: 0.3023 - accuracy: 0.8722 - precision: 0.8450 - recall: 0.9188\n",
            "Epoch 79/100\n",
            "137/137 [==============================] - 0s 3ms/step - loss: 0.2958 - accuracy: 0.8757 - precision: 0.8483 - recall: 0.9232\n",
            "Epoch 80/100\n",
            "137/137 [==============================] - 0s 3ms/step - loss: 0.2953 - accuracy: 0.8754 - precision: 0.8499 - recall: 0.9193\n",
            "Epoch 81/100\n",
            "137/137 [==============================] - 0s 3ms/step - loss: 0.3022 - accuracy: 0.8732 - precision: 0.8434 - recall: 0.9232\n",
            "Epoch 82/100\n",
            "137/137 [==============================] - 0s 3ms/step - loss: 0.2971 - accuracy: 0.8776 - precision: 0.8520 - recall: 0.9183\n",
            "Epoch 83/100\n",
            "137/137 [==============================] - 0s 3ms/step - loss: 0.3017 - accuracy: 0.8766 - precision: 0.8445 - recall: 0.9272\n",
            "Epoch 84/100\n",
            "137/137 [==============================] - 0s 3ms/step - loss: 0.3070 - accuracy: 0.8666 - precision: 0.8421 - recall: 0.9126\n",
            "Epoch 85/100\n",
            "137/137 [==============================] - 0s 3ms/step - loss: 0.2929 - accuracy: 0.8757 - precision: 0.8491 - recall: 0.9232\n",
            "Epoch 86/100\n",
            "137/137 [==============================] - 0s 3ms/step - loss: 0.2949 - accuracy: 0.8735 - precision: 0.8493 - recall: 0.9142\n",
            "Epoch 87/100\n",
            "137/137 [==============================] - 0s 3ms/step - loss: 0.2943 - accuracy: 0.8745 - precision: 0.8490 - recall: 0.9206\n",
            "Epoch 88/100\n",
            "137/137 [==============================] - 0s 3ms/step - loss: 0.2968 - accuracy: 0.8779 - precision: 0.8497 - recall: 0.9239\n",
            "Epoch 89/100\n",
            "137/137 [==============================] - 0s 3ms/step - loss: 0.2861 - accuracy: 0.8841 - precision: 0.8600 - recall: 0.9221\n",
            "Epoch 90/100\n",
            "137/137 [==============================] - 0s 3ms/step - loss: 0.2993 - accuracy: 0.8726 - precision: 0.8514 - recall: 0.9141\n",
            "Epoch 91/100\n",
            "137/137 [==============================] - 0s 3ms/step - loss: 0.2865 - accuracy: 0.8801 - precision: 0.8562 - recall: 0.9189\n",
            "Epoch 92/100\n",
            "137/137 [==============================] - 0s 3ms/step - loss: 0.3040 - accuracy: 0.8714 - precision: 0.8475 - recall: 0.9125\n",
            "Epoch 93/100\n",
            "137/137 [==============================] - 0s 2ms/step - loss: 0.2951 - accuracy: 0.8786 - precision: 0.8457 - recall: 0.9306\n",
            "Epoch 94/100\n",
            "137/137 [==============================] - 0s 3ms/step - loss: 0.2886 - accuracy: 0.8755 - precision: 0.8539 - recall: 0.9117\n",
            "Epoch 95/100\n",
            "137/137 [==============================] - 0s 2ms/step - loss: 0.2856 - accuracy: 0.8816 - precision: 0.8532 - recall: 0.9311\n",
            "Epoch 96/100\n",
            "137/137 [==============================] - 0s 3ms/step - loss: 0.2813 - accuracy: 0.8817 - precision: 0.8591 - recall: 0.9203\n",
            "Epoch 97/100\n",
            "137/137 [==============================] - 0s 3ms/step - loss: 0.3064 - accuracy: 0.8752 - precision: 0.8500 - recall: 0.9170\n",
            "Epoch 98/100\n",
            "137/137 [==============================] - 0s 3ms/step - loss: 0.2951 - accuracy: 0.8791 - precision: 0.8433 - recall: 0.9315\n",
            "Epoch 99/100\n",
            "137/137 [==============================] - 0s 2ms/step - loss: 0.2981 - accuracy: 0.8764 - precision: 0.8440 - recall: 0.9147\n",
            "Epoch 100/100\n",
            "137/137 [==============================] - 0s 3ms/step - loss: 0.3179 - accuracy: 0.8666 - precision: 0.8333 - recall: 0.9081\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7f2fd8944610>"
            ]
          },
          "metadata": {},
          "execution_count": 32
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import accuracy_score, precision_score, recall_score\n",
        "\n",
        "pred_4=model4.predict(x_test2)\n",
        "\n",
        "accuracy_model4 = accuracy_score(y_test2, pred_4)\n",
        "precision_model4 = precision_score(y_test2, pred_4)\n",
        "recall_model4 = recall_score(y_test2, pred_4)\n",
        "\n",
        "print(\"Accuracy:\", accuracy_model4,\n",
        "      \"\\nPrecision:\",precision_model4,\n",
        "      \"\\nRecall:\", recall_model4)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "q4zDG9XJeSct",
        "outputId": "96e14bc2-d6b0-433a-8e47-cdfbcbb7364d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: 0.8909838875557079 \n",
            "Precision: 0.8411728772144166 \n",
            "Recall: 0.9595818815331011\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nJ59ggoxkniT"
      },
      "source": [
        "\n",
        "\n",
        "*  model1_performance:Accuracy: 95.24%-Precision: 0%- Recall: 0% \n",
        "*  model2_performance:Accuracy: 81.49%-Precision: 78.09%- Recall: 87.59%\n",
        "*  model3_performance:Accuracy: 84.50%-Precision: 80.74%- Recall: 90.94%\n",
        "*  model4_performance:Accuracy: 89.09%-Precision: 84.11%-Recall:95.9\n",
        "\n",
        "**Last model has the best performance as it showed highest scores in case of precision and recall**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bKNwy0G9U9xo"
      },
      "source": [
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}